{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "406e4345-3bdb-45aa-bd69-da2cb12f4332",
   "metadata": {},
   "source": [
    "For details, see the original notebook on `LoRA_for_token_classifcation` [[link](https://github.com/matthiasdroth/Compute-Optimal_LoRA-Adapters_for_Language_Models/blob/main/LoRA_for_token_classification.ipynb)].\n",
    "\n",
    "**Done**:\n",
    "- change model to `FacebookAI/roberta-large`\n",
    "- change dataset to `DFKI-SLT/few-nerd`\n",
    "- collect all splits in one huge dataset\n",
    "- tokenize and align labels\n",
    "- split dataset into train, valid, test, and dev splits\n",
    "\n",
    "**ToDo**:\n",
    "- define data_collators\n",
    "- configure LoRA adapter\n",
    "- define LoRA model\n",
    "- run training via trainer\n",
    "- change training to `accelerate`\n",
    "- count FLOPs via `einops` in training loop\n",
    "- add logic to find maximum batch size\n",
    "- add basic sweep and log to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce964de-ab92-498e-9732-42ee6ddd263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "checkpoint = \"FacebookAI/roberta-large\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef6a17b2-b2c3-47cd-a7ad-e3c5ad61ca0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'words', 'ner_tags', 'fine_ner_tags'],\n",
       "    num_rows: 188239\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewnerd = load_dataset(\"DFKI-SLT/few-nerd\", \"supervised\")\n",
    "fewnerd_all = concatenate_datasets([fewnerd[\"train\"], fewnerd[\"validation\"], fewnerd[\"test\"]]).rename_column(\"tokens\", \"words\")\n",
    "fewnerd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f949db-fd27-47ca-87b3-fcf2b99d4e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 'O',\n",
       "  1: 'art',\n",
       "  2: 'building',\n",
       "  3: 'event',\n",
       "  4: 'location',\n",
       "  5: 'organization',\n",
       "  6: 'other',\n",
       "  7: 'person',\n",
       "  8: 'product'},\n",
       " {'O': 0,\n",
       "  'art': 1,\n",
       "  'building': 2,\n",
       "  'event': 3,\n",
       "  'location': 4,\n",
       "  'organization': 5,\n",
       "  'other': 6,\n",
       "  'person': 7,\n",
       "  'product': 8},\n",
       " ['O',\n",
       "  'art',\n",
       "  'building',\n",
       "  'event',\n",
       "  'location',\n",
       "  'organization',\n",
       "  'other',\n",
       "  'person',\n",
       "  'product'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = fewnerd_all.features[\"ner_tags\"].feature.names\n",
    "id2label = {k: v for k, v in enumerate(labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "id2label, label2id, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e323e6-101f-43ae-ad4c-8b9d7cac8a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'words', 'ner_tags', 'fine_ner_tags', 'tokens', 'word_ids', 'input_ids', 'attention_mask', 'matched_ner_tags'],\n",
       "    num_rows: 188239\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_and_align_ner_tags(instance_data, verbose=False):\n",
    "    \"\"\"verbosity = 0 (no logs), 1 (few logs), or 2 (all logs)\"\"\"\n",
    "    # tokenize \"words\" field and extract items\n",
    "    inputs = tokenizer(instance_data[\"words\"], is_split_into_words=True)\n",
    "    tokens = inputs.tokens()\n",
    "    word_ids = inputs.word_ids()\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    # declare variables to use in the loop below\n",
    "    labels = instance_data[\"ner_tags\"]\n",
    "    simple_word_ids = word_ids[1:-1]\n",
    "    simple_tokens = tokens[1:-1]\n",
    "    previous_word_id = False\n",
    "    previous_label = False\n",
    "    label_index = -1\n",
    "    match_labels = []\n",
    "    # loop start\n",
    "    for i in range(len(simple_word_ids)):\n",
    "        word_id = simple_word_ids[i]\n",
    "        if word_id==previous_word_id and type(word_id)==type(previous_word_id): # type(False) = bool (handle previous_word_id = False)\n",
    "            if verbose:\n",
    "                print(\"\\t word_id repeats\")\n",
    "            # add previous_label\n",
    "            match_labels.append(int(previous_label)) # int(False) = 0 (handle previous_word_id = False)\n",
    "        else:\n",
    "            # increment label_index and get label via label_index\n",
    "            label_index += 1\n",
    "            label = labels[label_index]\n",
    "            match_labels.append(label)\n",
    "        # update previous_word_id and previous_label\n",
    "        previous_word_id = word_id\n",
    "        previous_label = label\n",
    "        if verbose:\n",
    "            # logs\n",
    "            print(f\"i={i} \\t word_id={word_id} \\t label={label} \\t token={tokens[i+1]}\")\n",
    "    # loop end\n",
    "    match_labels = [-100] + match_labels + [-100]\n",
    "    return_items = {\n",
    "        \"tokens\": tokens,\n",
    "        \"word_ids\": word_ids,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"matched_ner_tags\": match_labels\n",
    "    }\n",
    "    return return_items\n",
    "\n",
    "fewnerd_all_mapped = fewnerd_all.map(tokenize_and_align_ner_tags)\n",
    "fewnerd_all_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "717deb2d-d6f2-4658-b968-4a4812d1c3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĠKnown (O)\n",
      "Ġlocally (O)\n",
      "Ġas (O)\n",
      "Ġ`` (O)\n",
      "ĠFair (product)\n",
      "bottom (product)\n",
      "ĠBob (product)\n",
      "s (product)\n",
      "Ġ`` (O)\n",
      "Ġit (O)\n",
      "Ġis (O)\n",
      "Ġnow (O)\n",
      "Ġpreserved (O)\n",
      "Ġat (O)\n",
      "Ġthe (O)\n",
      "ĠHenry (building)\n",
      "ĠFord (building)\n",
      "ĠMuseum (building)\n",
      "Ġin (O)\n",
      "ĠDear (location)\n",
      "born (location)\n",
      "Ġ, (O)\n",
      "ĠMichigan (location)\n",
      "Ġ. (O)\n"
     ]
    }
   ],
   "source": [
    "idx = 22\n",
    "tokens_idx = fewnerd_all_mapped[idx][\"tokens\"]\n",
    "ner_tags_idx = fewnerd_all_mapped[idx][\"matched_ner_tags\"]\n",
    "for i in range(len(tokens_idx)):\n",
    "    token_i = tokens_idx[i]\n",
    "    ner_tag_i = ner_tags_idx[i]\n",
    "    if ner_tag_i>-100:\n",
    "        print(f\"{token_i} ({id2label[ner_tag_i]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de7eef5d-d68c-47e1-82ca-c2186e59690e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['words', 'input_ids', 'attention_mask', 'matched_ner_tags'],\n",
       "        num_rows: 136002\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['words', 'input_ids', 'attention_mask', 'matched_ner_tags'],\n",
       "        num_rows: 24001\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['words', 'input_ids', 'attention_mask', 'matched_ner_tags'],\n",
       "        num_rows: 28236\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['words', 'input_ids', 'attention_mask', 'matched_ner_tags'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainvalid_test_split = fewnerd_all_mapped.train_test_split(test_size=0.15)\n",
    "trainvalid_split = trainvalid_test_split[\"train\"]\n",
    "test_split = trainvalid_test_split[\"test\"]\n",
    "train_valid_split = trainvalid_split.train_test_split(test_size=0.15)\n",
    "valid_split = train_valid_split[\"test\"]\n",
    "train_split = train_valid_split[\"train\"]\n",
    "dev_split = fewnerd_all_mapped.train_test_split(test_size=4)[\"test\"]\n",
    "fewnerd_dataset = DatasetDict({\n",
    "    \"train\": train_split,\n",
    "    \"valid\": valid_split,\n",
    "    \"test\": test_split,\n",
    "    \"dev\": dev_split\n",
    "}).remove_columns([\"id\", \"word_ids\", \"tokens\", \"ner_tags\", \"fine_ner_tags\"])\n",
    "fewnerd_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91418514-8ece-430e-9a40-2affb3d8a42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'art',\n",
       " 'building',\n",
       " 'event',\n",
       " 'location',\n",
       " 'organization',\n",
       " 'other',\n",
       " 'person',\n",
       " 'product']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = fewnerd_all.features[\"ner_tags\"].feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382b1f9b-1fbe-40c3-85e5-a9895929c88f",
   "metadata": {},
   "source": [
    "Load the [**fewnerd**](https://arxiv.org/pdf/2105.07464v6.pdf) dataset and read the according [**publication**](https://aclanthology.org/2021.acl-long.248/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbaa5e4-323f-4bc3-a033-441b265ca943",
   "metadata": {},
   "source": [
    "<font style=\"font-weight:300\">✔</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
