{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "406e4345-3bdb-45aa-bd69-da2cb12f4332",
   "metadata": {},
   "source": [
    "For details, see the original notebook on `LoRA_for_token_classifcation` [[link](https://github.com/matthiasdroth/Compute-Optimal_LoRA-Adapters_for_Language_Models/blob/main/LoRA_for_token_classification.ipynb)].\n",
    "\n",
    "ToDo:\n",
    "- change model to `FacebookAI/roberta-large`\n",
    "- change dataset to `DFKI-SLT/few-nerd`\n",
    "- change training to `accelerate`\n",
    "- count FLOPs via `einops` in training loop\n",
    "- add logic to find maximum batch size\n",
    "- add basic sweep and log to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce964de-ab92-498e-9732-42ee6ddd263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "checkpoint = \"FacebookAI/roberta-large\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef6a17b2-b2c3-47cd-a7ad-e3c5ad61ca0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'words', 'ner_tags', 'fine_ner_tags'],\n",
       "    num_rows: 188239\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect all splits in one huge dataset\n",
    "# tokenize and align labels\n",
    "# split dataset into train, valid, test, and dev splits\n",
    "# define data_collators\n",
    "# configure LoRA adapter\n",
    "# define LoRA model\n",
    "# run training via trainer\n",
    "# run training via accelerate\n",
    "fewnerd = load_dataset(\"DFKI-SLT/few-nerd\", \"supervised\")\n",
    "fewnerd_all = concatenate_datasets([fewnerd[\"train\"], fewnerd[\"validation\"], fewnerd[\"test\"]]).rename_column(\"tokens\", \"words\")\n",
    "fewnerd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0e323e6-101f-43ae-ad4c-8b9d7cac8a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_ner_tags(instance_data, verbose=False):\n",
    "    \"\"\"verbosity = 0 (no logs), 1 (few logs), or 2 (all logs)\"\"\"\n",
    "    # tokenize \"words\" field and extract items\n",
    "    inputs = tokenizer(instance_data[\"words\"], is_split_into_words=True)\n",
    "    tokens = inputs.tokens()\n",
    "    word_ids = inputs.word_ids()\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    # declare variables to use in the loop below\n",
    "    labels = instance_data[\"ner_tags\"]\n",
    "    simple_word_ids = word_ids[1:-1]\n",
    "    simple_tokens = tokens[1:-1]\n",
    "    previous_word_id = False\n",
    "    previous_label = False\n",
    "    label_index = -1\n",
    "    match_labels = []\n",
    "    # loop start\n",
    "    for i in range(len(simple_word_ids)):\n",
    "        word_id = simple_word_ids[i]\n",
    "        if word_id==previous_word_id and type(word_id)==type(previous_word_id): # type(False) = bool (handle previous_word_id = False)\n",
    "            if verbose:\n",
    "                print(\"\\t word_id repeats\")\n",
    "            # add previous_label\n",
    "            match_labels.append(int(previous_label)) # int(False) = 0 (handle previous_word_id = False)\n",
    "        else:\n",
    "            # increment label_index and get label via label_index\n",
    "            label_index += 1\n",
    "            label = labels[label_index]\n",
    "            match_labels.append(label)\n",
    "        # update previous_word_id and previous_label\n",
    "        previous_word_id = word_id\n",
    "        previous_label = label\n",
    "        if verbose:\n",
    "            # logs\n",
    "            print(f\"i={i} \\t word_id={word_id} \\t label={label} \\t token={tokens[i+1]}\")\n",
    "    # loop end\n",
    "    match_labels = [-100] + match_labels + [-100]\n",
    "    return_items = {\n",
    "        \"tokens\": tokens,\n",
    "        \"word_ids\": word_ids,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"matched_ner_tags\": match_labels\n",
    "    }\n",
    "    return return_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3db735a-c810-457f-94f4-29d68b787798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'words', 'ner_tags', 'fine_ner_tags', 'tokens', 'word_ids', 'input_ids', 'attention_mask', 'matched_ner_tags'],\n",
       "    num_rows: 188239\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewnerd_all_mapped = fewnerd_all.map(tokenize_and_align_ner_tags)\n",
    "fewnerd_all_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf76e3-b19d-4c9d-a515-33ead31ae36e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de7eef5d-d68c-47e1-82ca-c2186e59690e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'words', 'ner_tags', 'fine_ner_tags'],\n",
       "        num_rows: 136002\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['id', 'words', 'ner_tags', 'fine_ner_tags'],\n",
       "        num_rows: 24001\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'words', 'ner_tags', 'fine_ner_tags'],\n",
       "        num_rows: 28236\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['id', 'words', 'ner_tags', 'fine_ner_tags'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#fewnerd_all = fewnerd_all.rename_column(\"tokens\", \"text\")\n",
    "fewnerd_all = fewnerd_all.rename_column(\"tokens\", \"words\")\n",
    "dev_split = fewnerd_all.train_test_split(test_size=4)[\"test\"]\n",
    "trainvalid_test_split = fewnerd_all.train_test_split(test_size=0.15)\n",
    "trainvalid_split = trainvalid_test_split[\"train\"]\n",
    "test_split = trainvalid_test_split[\"test\"]\n",
    "train_valid_split = trainvalid_split.train_test_split(test_size=0.15)\n",
    "valid_split = train_valid_split[\"test\"]\n",
    "train_split = train_valid_split[\"train\"]\n",
    "fewnerd_dataset = DatasetDict({\n",
    "    \"train\": train_split,\n",
    "    \"valid\": valid_split,\n",
    "    \"test\": test_split,\n",
    "    \"dev\": dev_split\n",
    "})\n",
    "fewnerd_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1316786-d418-42df-bb32-a5432801202c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '22',\n",
       " 'text': ['Known',\n",
       "  'locally',\n",
       "  'as',\n",
       "  '``',\n",
       "  'Fairbottom',\n",
       "  'Bobs',\n",
       "  '``',\n",
       "  'it',\n",
       "  'is',\n",
       "  'now',\n",
       "  'preserved',\n",
       "  'at',\n",
       "  'the',\n",
       "  'Henry',\n",
       "  'Ford',\n",
       "  'Museum',\n",
       "  'in',\n",
       "  'Dearborn',\n",
       "  ',',\n",
       "  'Michigan',\n",
       "  '.'],\n",
       " 'ner_tags': [0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 4, 0, 4, 0],\n",
       " 'fine_ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  62,\n",
       "  62,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  0,\n",
       "  21,\n",
       "  0,\n",
       "  21,\n",
       "  0]}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewnerd_all[22] # 8, 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7334e0f1-3d2c-4344-a077-a25da647e71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Known',\n",
       " 'locally',\n",
       " 'as',\n",
       " '``',\n",
       " 'Fairbottom',\n",
       " 'Bobs',\n",
       " '``',\n",
       " 'it',\n",
       " 'is',\n",
       " 'now',\n",
       " 'preserved',\n",
       " 'at',\n",
       " 'the',\n",
       " 'Henry',\n",
       " 'Ford',\n",
       " 'Museum',\n",
       " 'in',\n",
       " 'Dearborn',\n",
       " ',',\n",
       " 'Michigan',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 22\n",
    "fewnerd_all[i][\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2107dc1b-8c12-47dd-a169-107e5662cba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 4, 0, 4, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewnerd_all[i][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff863ef-8423-498b-a782-5f288f991a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 21)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fewnerd_all[i][\"words\"]), len(fewnerd_all[i][\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91418514-8ece-430e-9a40-2affb3d8a42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'art',\n",
       " 'building',\n",
       " 'event',\n",
       " 'location',\n",
       " 'organization',\n",
       " 'other',\n",
       " 'person',\n",
       " 'product']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = fewnerd_all.features[\"ner_tags\"].feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9dc199d-530d-4c54-8e0e-f0d3cf0475f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known locally as `` Fairbottom Bobs    `` it is now preserved at the Henry    Ford     Museum   in Dearborn , Michigan . \n",
      "O     O       O  O  product    product O  O  O  O   O         O  O   building building building O  location O location O \n"
     ]
    }
   ],
   "source": [
    "i = 22\n",
    "words = fewnerd_all[i][\"words\"]\n",
    "labels = fewnerd_all[i][\"ner_tags\"]\n",
    "assert len(words)==len(labels)\n",
    "# https://github.com/matthiasdroth/Huggingface-course/blob/main/7.2-Token_classification.ipynb\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c4075b9-aae6-406a-87a1-f292b189a310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='FacebookAI/roberta-large', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "313490c8-a061-4d6a-aa94-1d80632612fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75492528-d3b0-4e85-8a93-df3b77552017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:\t0\tword:\tKnown\n",
      "label:\t0\tword:\tlocally\n",
      "label:\t0\tword:\tas\n",
      "label:\t0\tword:\t``\n",
      "label:\t8\tword:\tFairbottom\n",
      "label:\t8\tword:\tBobs\n",
      "label:\t0\tword:\t``\n",
      "label:\t0\tword:\tit\n",
      "label:\t0\tword:\tis\n",
      "label:\t0\tword:\tnow\n",
      "label:\t0\tword:\tpreserved\n",
      "label:\t0\tword:\tat\n",
      "label:\t0\tword:\tthe\n",
      "label:\t2\tword:\tHenry\n",
      "label:\t2\tword:\tFord\n",
      "label:\t2\tword:\tMuseum\n",
      "label:\t0\tword:\tin\n",
      "label:\t4\tword:\tDearborn\n",
      "label:\t0\tword:\t,\n",
      "label:\t4\tword:\tMichigan\n",
      "label:\t0\tword:\t.\n"
     ]
    }
   ],
   "source": [
    "i = 22\n",
    "words = fewnerd_all[i][\"words\"]\n",
    "labels = fewnerd_all[i][\"ner_tags\"]\n",
    "assert len(words)==len(labels)\n",
    "for j in range(len(words)):\n",
    "    print(f\"label:\\t{labels[j]}\\tword:\\t{words[j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4e0f9c4-0dd2-4c47-bb0c-2e6cd59e547c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 21)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words), len(labels) # tokenizing idx_tokens results in a longer list => adapt labels accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6af2fdce-3f84-459d-98af-bf5d78035947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 21)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 22\n",
    "inputs = tokenizer(fewnerd_all[i][\"words\"], is_split_into_words=True)\n",
    "tokens = inputs.tokens()\n",
    "word_ids = inputs.word_ids()\n",
    "input_ids = inputs.input_ids\n",
    "attention_mask = inputs.attention_mask\n",
    "assert len(tokens)==len(word_ids) and len(tokens)==len(input_ids) and len(tokens)==len(attention_mask)\n",
    "len(tokens), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27a2d606-7012-4a2e-94ec-ae08d1fcebb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '22',\n",
       " 'words': ['Known',\n",
       "  'locally',\n",
       "  'as',\n",
       "  '``',\n",
       "  'Fairbottom',\n",
       "  'Bobs',\n",
       "  '``',\n",
       "  'it',\n",
       "  'is',\n",
       "  'now',\n",
       "  'preserved',\n",
       "  'at',\n",
       "  'the',\n",
       "  'Henry',\n",
       "  'Ford',\n",
       "  'Museum',\n",
       "  'in',\n",
       "  'Dearborn',\n",
       "  ',',\n",
       "  'Michigan',\n",
       "  '.'],\n",
       " 'ner_tags': [0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 4, 0, 4, 0],\n",
       " 'fine_ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  62,\n",
       "  62,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  0,\n",
       "  21,\n",
       "  0,\n",
       "  21,\n",
       "  0]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewnerd_all[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9423a300-7dde-4d53-a2bc-7ffe2b7cc9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \t word_id=0 \t label=0 \t token=ĠTeams\n",
      "i=1 \t word_id=1 \t label=0 \t token=Ġincluded\n",
      "i=2 \t word_id=2 \t label=5 \t token=ĠSpecial\n",
      "i=3 \t word_id=3 \t label=5 \t token=ĠForces\n",
      "i=4 \t word_id=4 \t label=5 \t token=Ġsoldiers\n",
      "i=5 \t word_id=5 \t label=0 \t token=Ġ,\n",
      "i=6 \t word_id=6 \t label=7 \t token=ĠRichard\n",
      "i=7 \t word_id=7 \t label=7 \t token=ĠDun\n",
      "\t word_id repeats\n",
      "i=8 \t word_id=7 \t label=7 \t token=wood\n",
      "\t word_id repeats\n",
      "i=9 \t word_id=7 \t label=7 \t token=y\n",
      "i=10 \t word_id=8 \t label=0 \t token=Ġ(\n",
      "i=11 \t word_id=9 \t label=0 \t token=Ġprofessional\n",
      "i=12 \t word_id=10 \t label=0 \t token=Ġj\n",
      "\t word_id repeats\n",
      "i=13 \t word_id=10 \t label=0 \t token=ockey\n",
      "i=14 \t word_id=11 \t label=0 \t token=Ġ)\n",
      "i=15 \t word_id=12 \t label=0 \t token=Ġ,\n",
      "i=16 \t word_id=13 \t label=7 \t token=ĠSteven\n",
      "i=17 \t word_id=14 \t label=7 \t token=ĠEast\n",
      "i=18 \t word_id=15 \t label=0 \t token=Ġ,\n",
      "i=19 \t word_id=16 \t label=7 \t token=ĠMike\n",
      "i=20 \t word_id=17 \t label=7 \t token=ĠK\n",
      "\t word_id repeats\n",
      "i=21 \t word_id=17 \t label=7 \t token=rim\n",
      "\t word_id repeats\n",
      "i=22 \t word_id=17 \t label=7 \t token=hol\n",
      "\t word_id repeats\n",
      "i=23 \t word_id=17 \t label=7 \t token=z\n",
      "i=24 \t word_id=18 \t label=0 \t token=Ġ,\n",
      "i=25 \t word_id=19 \t label=7 \t token=ĠJ\n",
      "\t word_id repeats\n",
      "i=26 \t word_id=19 \t label=7 \t token=ock\n",
      "i=27 \t word_id=20 \t label=7 \t token=ĠWish\n",
      "\t word_id repeats\n",
      "i=28 \t word_id=20 \t label=7 \t token=art\n",
      "i=29 \t word_id=21 \t label=0 \t token=Ġ.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tokens': ['<s>',\n",
       "  'ĠTeams',\n",
       "  'Ġincluded',\n",
       "  'ĠSpecial',\n",
       "  'ĠForces',\n",
       "  'Ġsoldiers',\n",
       "  'Ġ,',\n",
       "  'ĠRichard',\n",
       "  'ĠDun',\n",
       "  'wood',\n",
       "  'y',\n",
       "  'Ġ(',\n",
       "  'Ġprofessional',\n",
       "  'Ġj',\n",
       "  'ockey',\n",
       "  'Ġ)',\n",
       "  'Ġ,',\n",
       "  'ĠSteven',\n",
       "  'ĠEast',\n",
       "  'Ġ,',\n",
       "  'ĠMike',\n",
       "  'ĠK',\n",
       "  'rim',\n",
       "  'hol',\n",
       "  'z',\n",
       "  'Ġ,',\n",
       "  'ĠJ',\n",
       "  'ock',\n",
       "  'ĠWish',\n",
       "  'art',\n",
       "  'Ġ.',\n",
       "  '</s>'],\n",
       " 'word_ids': [None,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  19,\n",
       "  20,\n",
       "  20,\n",
       "  21,\n",
       "  None],\n",
       " 'input_ids': [0,\n",
       "  16280,\n",
       "  1165,\n",
       "  3672,\n",
       "  8717,\n",
       "  3878,\n",
       "  2156,\n",
       "  2431,\n",
       "  6367,\n",
       "  1845,\n",
       "  219,\n",
       "  36,\n",
       "  2038,\n",
       "  1236,\n",
       "  16303,\n",
       "  4839,\n",
       "  2156,\n",
       "  5031,\n",
       "  953,\n",
       "  2156,\n",
       "  1483,\n",
       "  229,\n",
       "  6103,\n",
       "  9649,\n",
       "  329,\n",
       "  2156,\n",
       "  344,\n",
       "  3343,\n",
       "  27383,\n",
       "  2013,\n",
       "  479,\n",
       "  2],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'matched_ner_tags': [-100,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  0,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  7,\n",
       "  0,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  0,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  0,\n",
       "  -100]}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build some kind of preprocesssing function that works with the .map() method\n",
    "# then, adapt the preprocessing function to align labels with tokenized tokens\n",
    "def tokenize_and_align_ner_tags(instance_data, verbose=False):\n",
    "    \"\"\"verbosity = 0 (no logs), 1 (few logs), or 2 (all logs)\"\"\"\n",
    "    # tokenize \"words\" field and extract items\n",
    "    inputs = tokenizer(instance_data[\"words\"], is_split_into_words=True)\n",
    "    tokens = inputs.tokens()\n",
    "    word_ids = inputs.word_ids()\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    # declare variables to use in the loop below\n",
    "    labels = instance_data[\"ner_tags\"]\n",
    "    simple_word_ids = word_ids[1:-1]\n",
    "    simple_tokens = tokens[1:-1]\n",
    "    previous_word_id = False\n",
    "    previous_label = False\n",
    "    label_index = -1\n",
    "    match_labels = []\n",
    "    # loop start\n",
    "    for i in range(len(simple_word_ids)):\n",
    "        word_id = simple_word_ids[i]\n",
    "        if word_id==previous_word_id and type(word_id)==type(previous_word_id): # type(False) = bool (handle previous_word_id = False)\n",
    "            if verbose:\n",
    "                print(\"\\t word_id repeats\")\n",
    "            # add previous_label\n",
    "            match_labels.append(int(previous_label)) # int(False) = 0 (handle previous_word_id = False)\n",
    "        else:\n",
    "            # increment label_index and get label via label_index\n",
    "            label_index += 1\n",
    "            label = labels[label_index]\n",
    "            match_labels.append(label)\n",
    "        # update previous_word_id and previous_label\n",
    "        previous_word_id = word_id\n",
    "        previous_label = label\n",
    "        if verbose:\n",
    "            # logs\n",
    "            print(f\"i={i} \\t word_id={word_id} \\t label={label} \\t token={tokens[i+1]}\")\n",
    "    # loop end\n",
    "    match_labels = [-100] + match_labels + [-100]\n",
    "    return_items = {\n",
    "        \"tokens\": tokens,\n",
    "        \"word_ids\": word_ids,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"matched_ner_tags\": match_labels\n",
    "    }\n",
    "    return return_items\n",
    "#\n",
    "tokenize_and_align_ner_tags(fewnerd_dataset[\"train\"][8]) # 8, 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ae773940-8bd5-4fd6-8f49-14f81811a164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'words', 'ner_tags', 'fine_ner_tags', 'tokens', 'word_ids', 'input_ids', 'attention_mask', 'matched_ner_tags'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_tokenized = fewnerd_dataset[\"dev\"].map(tokenize_and_align_ner_tags)\n",
    "dev_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d189ec6-af6c-4375-a812-11df68b65c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d551b268-013a-489f-a072-6bfb890dec8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '8', 'words': ['Only', 'one', 'of', 'four', 'entered', 'Corvettes', '-', 'GT1', 'C6R', 'of', 'Luc', 'Alphand', 'Aventures', '-', 'eventually', 'finished', 'the', 'race', ',', 'taking', 'second', 'place', 'in', 'class', '.'], 'ner_tags': [0, 0, 0, 0, 0, 8, 8, 8, 8, 0, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'fine_ner_tags': [0, 0, 0, 0, 0, 59, 59, 59, 59, 0, 37, 37, 37, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 0, 0, 8, 8, 8, 8, 0, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 25)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 8\n",
    "print(fewnerd_all[i])\n",
    "ner_tags = fewnerd_all[i][\"ner_tags\"]\n",
    "ner_tags, len(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd245323-8430-4454-b890-57bdff0dcc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '3',\n",
       " 'words': ['Pakistani',\n",
       "  'scientists',\n",
       "  'and',\n",
       "  'engineers',\n",
       "  \"'\",\n",
       "  'working',\n",
       "  'at',\n",
       "  'IAEA',\n",
       "  'became',\n",
       "  'aware',\n",
       "  'of',\n",
       "  'advancing',\n",
       "  'Indian',\n",
       "  'nuclear',\n",
       "  'program',\n",
       "  'towards',\n",
       "  'making',\n",
       "  'the',\n",
       "  'bombs',\n",
       "  '.'],\n",
       " 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'fine_ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  32,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewnerd_all[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f6f89ec8-f8fc-45a1-9528-8142688a399d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ner_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mner_tags\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ner_tags' is not defined"
     ]
    }
   ],
   "source": [
    "ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33584eda-0e88-431e-a251-9b33e16d79b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([None,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  None],\n",
       " 26)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids, len(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6612ac8a-a3d2-4dfa-8f66-bb8e2268789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import copy\n",
    "#simple_word_ids = copy.copy(word_ids[1:-1])\n",
    "#simple_tokens = copy.copy(tokens[1:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "85d17202-1759-40a2-9b04-1ddf65af9677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \t word_id=0 \t label=0 \t token=ĠKnown\n",
      "i=1 \t word_id=1 \t label=0 \t token=Ġlocally\n",
      "i=2 \t word_id=2 \t label=0 \t token=Ġas\n",
      "i=3 \t word_id=3 \t label=0 \t token=Ġ``\n",
      "i=4 \t word_id=4 \t label=8 \t token=ĠFair\n",
      "i=5 \t word_id=4 \t label=8 \t token=bottom\n",
      "i=6 \t word_id=5 \t label=8 \t token=ĠBob\n",
      "i=7 \t word_id=5 \t label=8 \t token=s\n",
      "i=8 \t word_id=6 \t label=0 \t token=Ġ``\n",
      "i=9 \t word_id=7 \t label=0 \t token=Ġit\n",
      "i=10 \t word_id=8 \t label=0 \t token=Ġis\n",
      "i=11 \t word_id=9 \t label=0 \t token=Ġnow\n",
      "i=12 \t word_id=10 \t label=0 \t token=Ġpreserved\n",
      "i=13 \t word_id=11 \t label=0 \t token=Ġat\n",
      "i=14 \t word_id=12 \t label=0 \t token=Ġthe\n",
      "i=15 \t word_id=13 \t label=2 \t token=ĠHenry\n",
      "i=16 \t word_id=14 \t label=2 \t token=ĠFord\n",
      "i=17 \t word_id=15 \t label=2 \t token=ĠMuseum\n",
      "i=18 \t word_id=16 \t label=0 \t token=Ġin\n",
      "i=19 \t word_id=17 \t label=4 \t token=ĠDear\n",
      "i=20 \t word_id=17 \t label=4 \t token=born\n",
      "i=21 \t word_id=18 \t label=0 \t token=Ġ,\n",
      "i=22 \t word_id=19 \t label=4 \t token=ĠMichigan\n",
      "i=23 \t word_id=20 \t label=0 \t token=Ġ.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " -100]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variables to use in the loop\n",
    "simple_word_ids = word_ids[1:-1]\n",
    "simple_tokens = tokens[1:-1]\n",
    "previous_word_id = False\n",
    "previous_label = False\n",
    "label_index = 0\n",
    "match_labels = []\n",
    "# loop\n",
    "for i in range(len(simple_word_ids)):\n",
    "    word_id = simple_word_ids[i]\n",
    "    if word_id==previous_word_id:\n",
    "        # add previous_label\n",
    "        match_labels.append(int(previous_label)) # int(False) = 0 (evaluates False to 0)\n",
    "    else:\n",
    "        # incremente label_index\n",
    "        label_index += 1\n",
    "        # get label via label_index\n",
    "        label = labels[label_index]\n",
    "        # add label to match_labels\n",
    "        match_labels.append(label)\n",
    "    # update previous_word_id\n",
    "    previous_word_id = word_id\n",
    "    # update previous_label\n",
    "    previous_label = label\n",
    "    # logs\n",
    "    print(f\"i={i} \\t word_id={word_id} \\t label={label} \\t token={tokens[i+1]}\")\n",
    "\n",
    "full_match_labels = [-100] + match_labels + [-100]\n",
    "full_match_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c7fbc0d2-6fd8-4c0b-9487-af57c3b727f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_match_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382b1f9b-1fbe-40c3-85e5-a9895929c88f",
   "metadata": {},
   "source": [
    "Load the [**fewnerd**](https://arxiv.org/pdf/2105.07464v6.pdf) dataset and read the according [**publication**](https://aclanthology.org/2021.acl-long.248/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6744a4c6-3fef-43bd-95fb-55c79f67a666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'fine_ner_tags'],\n",
       "        num_rows: 136002\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['tokens', 'fine_ner_tags'],\n",
       "        num_rows: 24001\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'fine_ner_tags'],\n",
       "        num_rows: 28236\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['tokens', 'fine_ner_tags'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "# 0.1\n",
    "# 100 * 0.1 = 10 => 10 for test, 90 for train + valid; 90 * 0.1 = 9 for valid => 81 for train\n",
    "# 0.15\n",
    "# 100 * 0.15 = 15 => 15 for test, 85 for train + valid; 85 * 0.15 = 12.75 for valid => 72.75 for train\n",
    "# 0.2\n",
    "# 100 * 0.2 = 20 => 20 for test, 80 for train + valid; 80 * 0.2 = 16 for valid => 64 for train\n",
    "dataset_cc = concatenate_datasets([fewnerd[\"train\"], fewnerd[\"validation\"], fewnerd[\"test\"]])\n",
    "dev_split = dataset_cc.train_test_split(test_size=4)[\"test\"]\n",
    "trainvalid_test_splits = dataset_cc.train_test_split(test_size=0.15) # train 81% valid 9% test 10%\n",
    "test_split = trainvalid_test_splits[\"test\"]\n",
    "trainvalid_split = trainvalid_test_splits[\"train\"]\n",
    "train_valid_split = trainvalid_split.train_test_split(test_size=0.15)\n",
    "valid_split = train_valid_split[\"test\"]\n",
    "train_split = train_valid_split[\"train\"]\n",
    "dataset_fewnerd = DatasetDict({\n",
    "    \"train\": train_split,\n",
    "    \"valid\": valid_split,\n",
    "    \"test\": test_split,\n",
    "    \"dev\": dev_split\n",
    "}).remove_columns([\"id\", \"ner_tags\"])\n",
    "dataset_fewnerd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbaa5e4-323f-4bc3-a033-441b265ca943",
   "metadata": {},
   "source": [
    "<font style=\"font-weight:300\">✔</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
