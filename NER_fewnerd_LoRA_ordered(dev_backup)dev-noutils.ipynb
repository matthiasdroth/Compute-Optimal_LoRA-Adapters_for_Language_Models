{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f55bab0",
   "metadata": {},
   "source": [
    "- *It works!*\n",
    "- DONE: use `max_batchsize` from `utils`\n",
    "- DONE: make **dedicated notebook** to (i) compare `conll2003` to `fewnerd` and (ii) to bring `fewnerd` into the same format.\n",
    "- DONE: use fewnerd\n",
    "- DONE: get total flops count with **einops**\n",
    "- DONE: make custom splits\n",
    "- DONE: reorder the notebook cells:\n",
    "  - DONE: model (LoRA) and tokenizer (save peftconfig if necessary)\n",
    "  - DONE: dataset etc\n",
    "  - DONE: training, metrics and saving (tokenizer and model)\n",
    "  - DONE: inference via loading saved items (probably tokenizer, model and peftconfig – OR follow this [guide](https://huggingface.co/docs/peft/v0.9.0/en/package_reference/lora#peft.LoraModel))\n",
    "- DONE: In **Using the fine-tuned model**, [merge and unload](https://huggingface.co/docs/peft/v0.6.2/en/package_reference/tuners#peft.LoraModel.merge_and_unload) or [reinstantiate](https://huggingface.co/docs/peft/v0.6.2/en/task_guides/token-classification-lora#inference) the LoRA model!\n",
    "- DONE: adjust batch size – and if necessary epochs – for 3000 training steps `num_steps = train_instances*epochs/batch_size` $\\geq$ 3000 $\\Rightarrow$ `batch_size` $\\leq$ `train_instances*epochs/3000 = train_instances/1000` $\\Rightarrow$ `batch_size` $\\leq$ `train_instances / 1000` for `epochs = 3`<br>DONE: But do it like this:\n",
    "  - DONE: get max batch size for model (= max_batchsize_by_model)\n",
    "  - DONE: specify trainig split\n",
    "  - DONE: get max batch size for training split length (=max_batchsize_by_trainsplit)\n",
    "  - DONE: impose max batch size of 32\n",
    "  - DONE: the batch size is the minimum of these three numbers\n",
    "- DONE: build `results.json` (consider pandas series) via dict. It holds: splits, specified loraconfig details, flops, metrics (per epoch)\n",
    "- DONE: use the uuid library to save `results.json` under `results_{uuid}.json`\n",
    "- DONE: declare variable `split` and use it to select splits as well as for logging it in `results_{uuid}.json`.\n",
    "- DONE: use split `dev` and determine the learning rate for LoRA models. It seems that with `accelerate`, the maximum accepted learning rate is `5e-4` since fails for higher learning rates.\n",
    "- outside the training loop make a dummy classification report using scikit-learn with dummy labels and dummy predictions (both formatted as required by classification report)\n",
    "- at the end of each epoch, bring the labels and predictions into the required format\n",
    "- write a compute metrics function that returns a [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) as dict, then log the relevant values\n",
    "- check everything once more\n",
    "- add comments\n",
    "- make new notebook copy for sweep\n",
    "- Check once more the wandb [ablation study](https://wandb.ai/ayush-thakur/dl-question-bank/reports/What-s-the-Optimal-Batch-Size-to-Train-a-Neural-Network---VmlldzoyMDkyNDU) on batch sizes!\n",
    "\n",
    "# NER with `fewnerd` and LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4b5296-cb60-4875-a7f3-96e2c0c03fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-14 01:45:44,291] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/matthias/.cache/huggingface/token\n",
      "Login successful\n",
      "All random seeds set to 42.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "from peft import LoraConfig, TaskType, PeftModel, PeftConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from uuid import uuid4\n",
    "from utils import set_seed#, get_max_instance, get_max_batchsize\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "from evaluate import load\n",
    "from tqdm.auto import tqdm\n",
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler, pipeline, AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from huggingface_hub import login\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "from deepspeed.profiling.flops_profiler import FlopsProfiler\n",
    "\n",
    "load_dotenv()\n",
    "login(token=os.getenv(\"HUGGINGFACE_API_KEY\"))\n",
    "logs_dict = {}\n",
    "logs_dict[\"seed\"] = set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9864ca-1c11-44bd-8bf5-b1db6cb243d5",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f272eac3-45d7-4b20-99d7-2da88507b515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b5fe577-f14f-4f86-a4a8-c2b85662178c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'art', 'building', 'event', 'location', 'organization', 'other', 'person', 'product']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'0': 'O',\n",
       "  '1': 'art',\n",
       "  '2': 'building',\n",
       "  '3': 'event',\n",
       "  '4': 'location',\n",
       "  '5': 'organization',\n",
       "  '6': 'other',\n",
       "  '7': 'person',\n",
       "  '8': 'product'},\n",
       " {'O': '0',\n",
       "  'art': '1',\n",
       "  'building': '2',\n",
       "  'event': '3',\n",
       "  'location': '4',\n",
       "  'organization': '5',\n",
       "  'other': '6',\n",
       "  'person': '7',\n",
       "  'product': '8'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"DFKI-SLT/few-nerd\", \"supervised\")\n",
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "label_names = ner_feature.feature.names\n",
    "print(label_names)\n",
    "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "id2label, label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "091ef51a-7595-4d88-b57d-5d5a636f4ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA config:\n",
      "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.TOKEN_CLS: 'TOKEN_CLS'>, inference_mode=False, r=64, target_modules={'key', 'value', 'query_proj', 'query', 'value_proj', 'key_proj'}, lora_alpha=8, lora_dropout=0.2, fan_in_fan_out=False, bias='lora_only', use_rslora=True, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={})\n",
      "\n",
      "base_model type:\n",
      "<class 'transformers.models.roberta.modeling_roberta.RobertaForTokenClassification'>\n",
      "\n",
      "adapter_model type:\n",
      "<class 'peft.peft_model.PeftModelForTokenClassification'>\n",
      "\n",
      "trainable parameters:\n",
      "9446409\n",
      "\n",
      "nall parameters:\n",
      "363765778\n",
      "\n",
      "trainable fraction:\n",
      "0.02597\n"
     ]
    }
   ],
   "source": [
    "base_model_id = \"FacebookAI/roberta-large\"\n",
    "logs_dict[\"base_model_id\"] = base_model_id\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    base_model_id,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True\n",
    ")\n",
    "# LoRA model\n",
    "# datasets:      3 values [1%, 10%, 100%]\n",
    "# lora_rank:    10 values [1, ..., 512]\n",
    "# lora_dropout:  5 values [0, 0.1, 0.2, 0.3, 0.4]\n",
    "# lora_bias:     3 values [\"all\", \"none\", \"lora_only\"]\n",
    "# => 3 x 10 x 5 x 3 = 450 sweeps per notebook\n",
    "# => start with 2 dataset values (10%, 100%) and 3 rank values (2, 8, 32) => 6 sweeps\n",
    "LoRA_params_dict = {\n",
    "    \"r\": 64,\n",
    "    \"target_modules\": [\"query\", \"key\", \"value\", \"query_proj\", \"key_proj\", \"value_proj\"],\n",
    "    \"bias\": \"lora_only\",\n",
    "    \"use_rslora\": True,\n",
    "    \"task_type\": TaskType.TOKEN_CLS,\n",
    "    \"lora_dropout\": 0.2\n",
    "}\n",
    "logs_dict[\"LoRA_params_dict\"] = LoRA_params_dict\n",
    "# r =   1 => (  156681, 354476050, 0.00044)\n",
    "# r =   2 => (  304137, 354623506, 0.00086)\n",
    "# r =   4 => (  599049, 354918418, 0.00169)\n",
    "# r =   8 => ( 1188873, 355508242, 0.00334)\n",
    "# r =  16 => ( 2368521, 356687890, 0.00664)\n",
    "# r =  32 => ( 4727817, 359047186, 0.01317)\n",
    "# r =  64 => ( 9446409, 363765778, 0.02597)\n",
    "# r = 128 => (18883593, 373202962, 0.0506)\n",
    "# r = 256 => (37757961, 392077330, 0.0963)\n",
    "# r = 512 => (75506697, 429826066, 0.17567)\n",
    "config = LoraConfig(\n",
    "    # GUIDE   => https://huggingface.co/docs/peft/main/en/conceptual_guides/lora#common-lora-parameters-in-peft\n",
    "    # https://huggingface.co/docs/peft/main/en/conceptual_guides/lora#common-lora-parameters-in-peft:~:text=use_rslora%3A%20When%20set%20to%20True%2C%20uses%20Rank%2DStabilized%20LoRA%20which%20sets%20the%20adapter%20scaling%20factor\n",
    "    # https://arxiv.org/abs/2312.03732, \n",
    "    r = LoRA_params_dict[\"r\"],\n",
    "    target_modules=LoRA_params_dict[\"target_modules\"],\n",
    "    bias=LoRA_params_dict[\"bias\"],\n",
    "    use_rslora=LoRA_params_dict[\"use_rslora\"],\n",
    "    task_type=LoRA_params_dict[\"task_type\"],\n",
    "    lora_dropout=LoRA_params_dict[\"lora_dropout\"]\n",
    ")\n",
    "logs_dict[\"LoraConfig\"] = str(config)\n",
    "print(f\"LoRA config:\\n{config}\\n\")\n",
    "adapter_model = prepare_model_for_kbit_training(base_model)\n",
    "adapter_model = get_peft_model(adapter_model, config)\n",
    "print(f\"base_model type:\\n{type(base_model)}\\n\\nadapter_model type:\\n{type(adapter_model)}\")\n",
    "trainable_params, all_params = adapter_model.get_nb_trainable_parameters()\n",
    "trainable_fraction = round(trainable_params/all_params, 5)\n",
    "logs_dict[\"LoRA_model_trainable_params\"] = trainable_params\n",
    "logs_dict[\"LoRA_model_all_params\"] = all_params\n",
    "logs_dict[\"LoRA_model_trainable_fraction\"] = trainable_fraction\n",
    "print(f\"\\ntrainable parameters:\\n{trainable_params}\")\n",
    "print(f\"\\nnall parameters:\\n{all_params}\")\n",
    "print(f\"\\ntrainable fraction:\\n{trainable_fraction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "330cc610-6a56-4631-b226-df0419ac07f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer is fast: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='FacebookAI/roberta-large', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_prefix_space=True)\n",
    "logs_dict[\"tokenizer\"] = base_model_id\n",
    "print(f\"tokenizer is fast: {tokenizer.is_fast}\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8461807-8ab1-4253-ac37-c335c9423fa7",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ad64867-2e55-45ce-b7eb-4cc521a02074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
       "        num_rows: 131767\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
       "        num_rows: 18824\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
       "        num_rows: 37648\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"DFKI-SLT/few-nerd\", \"supervised\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9221b549-f8f4-4ed6-9ac2-98d9429614d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 188239\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "fewnerd_all_processed = concatenate_datasets([raw_datasets[\"train\"], raw_datasets[\"validation\"], raw_datasets[\"test\"]]).map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "fewnerd_all_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cecc64b1-a3bf-4e22-9ac6-8718e78998d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_100: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 136002\n",
       "    })\n",
       "    train_10: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 13601\n",
       "    })\n",
       "    train_1: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1361\n",
       "    })\n",
       "    valid_100: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 24001\n",
       "    })\n",
       "    valid_10: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2401\n",
       "    })\n",
       "    valid_1: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 241\n",
       "    })\n",
       "    test_100: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 28236\n",
       "    })\n",
       "    test_10: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2824\n",
       "    })\n",
       "    test_1: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 283\n",
       "    })\n",
       "    train_dev: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 120\n",
       "    })\n",
       "    valid_dev: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 32\n",
       "    })\n",
       "    test_dev: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter dataset by length if necessary\n",
    "trainvalid_test_splits = fewnerd_all_processed.train_test_split(test_size=0.15)\n",
    "test_split_100 = trainvalid_test_splits[\"test\"]\n",
    "test_split_10 = test_split_100.train_test_split(test_size = 0.1)[\"test\"]\n",
    "test_split_1 = test_split_100.train_test_split(test_size = 0.01)[\"test\"]\n",
    "trainvalid_split = trainvalid_test_splits[\"train\"]\n",
    "train_valid_split = trainvalid_split.train_test_split(test_size=0.15)\n",
    "valid_split_100 = train_valid_split[\"test\"]\n",
    "valid_split_10 = valid_split_100.train_test_split(test_size = 0.1)[\"test\"]\n",
    "valid_split_1 = valid_split_100.train_test_split(test_size = 0.01)[\"test\"]\n",
    "train_split_100 = train_valid_split[\"train\"]\n",
    "train_split_10 = train_split_100.train_test_split(test_size = 0.1)[\"test\"]\n",
    "train_split_1 = train_split_100.train_test_split(test_size = 0.01)[\"test\"]\n",
    "dev_train_split = train_split_100.train_test_split(test_size = 120)[\"test\"]\n",
    "dev_valid_split = valid_split_100.train_test_split(test_size = 32)[\"test\"]\n",
    "dev_test_split = test_split_100.train_test_split(test_size = 8)[\"test\"]\n",
    "fewnerd_dsd = DatasetDict({\n",
    "    \"train_100\": train_split_100,\n",
    "    \"train_10\": train_split_10,\n",
    "    \"train_1\": train_split_1,\n",
    "    \"valid_100\": valid_split_100,\n",
    "    \"valid_10\": valid_split_10,\n",
    "    \"valid_1\": valid_split_1,\n",
    "    \"test_100\": test_split_100,\n",
    "    \"test_10\": test_split_10,\n",
    "    \"test_1\": test_split_1,\n",
    "    \"train_dev\": dev_train_split,\n",
    "    \"valid_dev\": dev_valid_split,\n",
    "    \"test_dev\": dev_test_split\n",
    "})\n",
    "fewnerd_dsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "847fc32c-5cb0-44d3-b6d9-45f5f2202a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('train_100', 'valid_100', 'test_100')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = \"100\" # \"100\", \"10\", \"1\", \"dev\"\n",
    "assert split in (\"100\", \"10\", \"1\", \"dev\"), f\"Split '{split}' is not a valid choice.\"\n",
    "train_split = f\"train_{split}\"\n",
    "valid_split = f\"valid_{split}\"\n",
    "test_split = f\"test_{split}\"\n",
    "train_split, valid_split, test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ce12c9-fad5-4c94-af66-d77aade82d78",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b954012-9bee-4710-85ae-4c81582f5644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchsize_dict = {\n",
    "    # values have been found manually, via trial and error\n",
    "    \"100\": 32,\n",
    "    \"10\": 8,\n",
    "    \"1\": 1,\n",
    "    \"dev\": 1\n",
    "}\n",
    "batch_size = batchsize_dict[split]\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "608c18ab-8a49-468d-9975-04bc6f0630a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForTokenClassification(tokenizer=RobertaTokenizerFast(name_or_path='FacebookAI/roberta-large', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}, padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd71e709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    0,    4,    4,    4,    4,    4,    4,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    4,    4,    4,    4,\n",
       "            4,    0,    0,    0,    0,    0,    0,    4,    0,    4,    0,    4,\n",
       "            0, -100],\n",
       "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    1,    2,    0,    0,    0,    0,    0,    0,\n",
       "            0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([fewnerd_dsd[train_split][i] for i in [2, 3]])\n",
    "batch[\"labels\"] # As we can see, the second set of labels has been padded to the length of the first one using -100s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ba75a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = load(\"seqeval\")\n",
    "\n",
    "# def compute_metrics(eval_preds):\n",
    "#     logits, labels = eval_preds\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     # Remove ignored index (special tokens) and convert to labels\n",
    "#     true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "#     true_predictions = [\n",
    "#         [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#         for prediction, label in zip(predictions, labels)\n",
    "#     ]\n",
    "#     all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "#     return {\n",
    "#         \"precision\": all_metrics[\"overall_precision\"],\n",
    "#         \"recall\": all_metrics[\"overall_recall\"],\n",
    "#         \"f1\": all_metrics[\"overall_f1\"],\n",
    "#         \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b560aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7d0f1ff258a0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7d0f1ff26830>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    fewnerd_dsd[train_split],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "valid_dataloader = DataLoader(fewnerd_dsd[valid_split], collate_fn=data_collator, batch_size=batch_size)\n",
    "train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1911f061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ner_logs/FacebookAI-roberta-large'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = AdamW(adapter_model.parameters(), lr=5e-4) # 5e-4 works\n",
    "accelerator = Accelerator()\n",
    "acc_model, optimizer, train_dataloader, valid_dataloader = accelerator.prepare(\n",
    "    adapter_model,\n",
    "    optimizer,\n",
    "    train_dataloader,\n",
    "    valid_dataloader\n",
    ")\n",
    "model_folder = re.sub(\"/\", \"-\", base_model_id)\n",
    "output_dir = f\"ner_logs/{model_folder}\"\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "897170de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_steps (all epochs):\t12753\n",
      "num_warmup_steps (first epoch):\t500\n"
     ]
    }
   ],
   "source": [
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "logs_dict[\"num_training_steps\"] = num_training_steps\n",
    "num_warmup_steps = min(500, round(0.15 * num_update_steps_per_epoch)) # 500 or 15% of one epoch, whichever is less\n",
    "logs_dict[\"num_warmup_steps\"] = num_warmup_steps\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "print(f\"training_steps (all epochs):\\t{num_training_steps}\\nnum_warmup_steps (first epoch):\\t{num_warmup_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe65a0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ecb2a6bc6e4bfc8a954dc43b68d713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12753 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-14 01:45:51,301] [INFO] [profiler.py:80:start_profile] Flops profiler started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/matthias/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training\n",
      "['input_ids shape: [32, 61]', 'attention_mask shape: [32, 61]', 'labels shape: [32, 61]']\n",
      "logits shape: [32, 61, 9], loss: 1.5455927848815918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# def postprocess(predictions, labels):\n",
    "#     predictions = predictions.detach().cpu().clone().numpy()\n",
    "#     labels = labels.detach().cpu().clone().numpy()\n",
    "#     # Remove ignored index (special tokens) and convert to labels\n",
    "#     true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "#     true_predictions = [\n",
    "#         [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#         for prediction, label in zip(predictions, labels)\n",
    "#     ]\n",
    "#     return true_labels, true_predictions\n",
    "\n",
    "def flatten_list_of_lists(list_of_lists):\n",
    "    flattened_list = []\n",
    "    for list_i in list_of_lists:\n",
    "        list_i = list_i.tolist() # handle tensors in case list_of_lists is a list of tensors\n",
    "        list_i = list(itertools.chain.from_iterable(list_i)) # flatten list_i using the standard library\n",
    "        flattened_list += list_i\n",
    "    return flattened_list\n",
    "\n",
    "prof = FlopsProfiler(acc_model) # deepspeed profiler\n",
    "flops_list = []\n",
    "training_start = True\n",
    "validation_start = True\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "start_time = time.time() # start time\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    acc_model.train()\n",
    "    prof.start_profile() # start profiling\n",
    "    for batch in train_dataloader:\n",
    "        outputs = acc_model(**batch)\n",
    "        if training_start:\n",
    "            print(\"\\ntraining\")\n",
    "            print([f\"{key} shape: {list(batch[key].shape)}\" for key in list(batch.keys())])\n",
    "            print(f\"logits shape: {list(outputs['logits'].shape)}, loss: {float(outputs['loss'])}\\n\")\n",
    "            training_start = False\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    prof.stop_profile() # stop profiling\n",
    "    total_flops = prof.get_total_flops()\n",
    "    flops_list.append(total_flops)\n",
    "    # Validation\n",
    "    acc_model.eval()\n",
    "    epoch_predictions = []\n",
    "    epoch_labels = []\n",
    "    for batch in valid_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = acc_model(**batch)\n",
    "        if validation_start:\n",
    "            print(\"validation\")\n",
    "            print([f\"{key} shape: {list(batch[key].shape)}\" for key in list(batch.keys())])\n",
    "            print(f\"logits shape: {list(outputs['logits'].shape)}, loss: {float(outputs['loss'])}\\n\")\n",
    "            validation_start = False\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "        assert len(predictions)==len(labels)\n",
    "        epoch_predictions.append(predictions)\n",
    "        epoch_labels.append(labels)\n",
    "    # collect both, predictions and labels, in one big list each\n",
    "    flat_epoch_predictions = flatten_list_of_lists(epoch_predictions)\n",
    "    flat_epoch_labels = flatten_list_of_lists(epoch_labels)\n",
    "    # get and print classification report\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # save model\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(acc_model)\n",
    "    unwrapped_model.save_pretrained(output_dir)\n",
    "    # save tokenizer\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "#\n",
    "stop_time = time.time()\n",
    "training_loop_time = str(datetime.timedelta(seconds = round(stop_time-start_time)))\n",
    "print(f\"training loop time: {training_loop_time}\")\n",
    "logs_dict[\"training_loop_time\"] = training_loop_time\n",
    "prof.end_profile() # end profiling\n",
    "logs_dict[\"flops_list\"] = flops_list\n",
    "print(flops_list)\n",
    "flops_array = np.array(flops_list)\n",
    "np.sum(flops_array), np.mean(flops_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6d08c-af2a-4cca-b80c-d7a9f16a94e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = [0, 1, 2, 2, 2]\n",
    "# y_pred = [0, 0, 2, 2, 1]\n",
    "y_true = flat_epoch_labels\n",
    "y_pred = flat_epoch_predictions\n",
    "\n",
    "#flat_epoch_predictions = flatten_list_of_lists(epoch_predictions)\n",
    "#flat_epoch_labels = flatten_list_of_lists(epoch_labels)\n",
    "\n",
    "\n",
    "metrics_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "accuracy = metrics_dict['accuracy']\n",
    "#print(f\"accuracy: {accuracy}\")\n",
    "macroavg_dict = metrics_dict[\"macro avg\"]\n",
    "str([f\"{key}: {macroavg_dict[key]}\" for key in list(macroavg_dict.keys())])\n",
    "#\n",
    "macroavg_dict[\"accuracy\"] = accuracy\n",
    "# https://stackoverflow.com/questions/52139110/how-to-change-the-order-of-keys-in-a-python-3-5-dictionary-using-another-list-a\n",
    "macroavg_dict_order = {k : macroavg_dict[k] for k in [\"accuracy\", \"precision\", \"recall\", \"f1-score\", \"support\"]}\n",
    "macroavg_dict_order[\"support\"] = round(macroavg_dict_order[\"support\"]) # 134762.0\n",
    "metrics = [f\"{key}: {macroavg_dict_order[key]}\" for key in macroavg_dict_order.keys()]\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115bc23e-85da-427f-aeb1-9e69bf79e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flat_predictions = flatten_list_of_lists(epoch_predictions)\n",
    "print(f\"flat predictions:\\n{flat_predictions}\")\n",
    "flat_labels = flatten_list_of_lists(epoch_labels)\n",
    "print(f\"\\nflat labels:\\n{flat_labels}\")\n",
    "assert set(flat_predictions)!=set([0]), \"Training failed!\\nOnly labels of value 0 were predicted.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf3fa8-9b93-4d2f-88f3-75b10fb19cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_epoch_labels = []\n",
    "print(len(epoch_labels))\n",
    "for i, epoch_labels_i in enumerate(epoch_labels):\n",
    "    epoch_labels_i = epoch_labels_i.tolist()\n",
    "    epoch_labels_i = list(itertools.chain.from_iterable(epoch_labels_i))\n",
    "    all_epoch_labels += epoch_labels_i\n",
    "    #print(f\"\\nepoch_labels_{i}:\\n{epoch_labels_i}\")\n",
    "print(len(all_epoch_labels))\n",
    "all_epoch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5ab774-94ba-4750-91a9-8fd462782540",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_epoch_predictions = []\n",
    "print(len(epoch_predictions))\n",
    "for i, epoch_predictions_i in enumerate(epoch_predictions):\n",
    "    epoch_predictions_i = epoch_predictions_i.tolist()\n",
    "    epoch_predictions_i = list(itertools.chain.from_iterable(epoch_predictions_i))\n",
    "    all_epoch_predictions += epoch_predictions_i\n",
    "    #print(f\"\\nepoch_predictions_{i}:\\n{epoch_predictions_i}\")\n",
    "print(len(all_epoch_predictions))\n",
    "all_epoch_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a728d0-a7d9-402e-996b-920cca241678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get\n",
    "\n",
    "\n",
    "list2d = [[1,2,3], [4,5,6], [7], [8,9]]\n",
    "merged = list(itertools.chain.from_iterable(list2d))\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4dcbbb-dbb0-4990-a026-7c8ed5f8649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(epoch_labels) # \"dev\" validation set has length 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b13640-c25d-4011-bbba-d21e01627847",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(epoch_labels), type(epoch_labels[0]), type(epoch_labels[0][0]), type(epoch_labels[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f997962-bac9-4342-8374-63462b5e2474",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(epoch_labels) # list of length 32 (for train_dev split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d181c139-1cfd-40d3-8dae-681ef8503b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 23\n",
    "epoch_labels[idx][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf73a85-7586-4635-b8fe-8e2be2aa7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_batch_of_labels = list(itertools.chain.from_iterable(epoch_labels))\n",
    "flat_batch_of_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f91ed9-0d29-4a5d-a003-38da95e8ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all labels for an epoch \n",
    "for i, epoch_labels_i in enumerate(epoch_labels):\n",
    "    epoch_labels_list_i = list(itertools.chain.from_iterable(epoch_labels_i.tolist()))\n",
    "    #epoch_labels_list_i = epoch_labels_i.tolist()\n",
    "    print(f\"i = {i}\\tlabels = {epoch_labels_list_i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd3d9e5-8a50-4800-a83e-e1bf551640eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = labels.tolist()\n",
    "print(len(labels_list))\n",
    "labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dae8958-2bda-4ac3-a88c-0b47c061fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = predictions.tolist()\n",
    "print(len(predictions_list))\n",
    "predictions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9d198e-043a-41ba-bf7c-24480fccacfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = [0, 1, 2, 2, 2]\n",
    "# y_pred = [0, 0, 2, 2, 1]\n",
    "y_true = flat_epoch_labels\n",
    "y_pred = flat_epoch_predictions\n",
    "\n",
    "#flat_epoch_predictions = flatten_list_of_lists(epoch_predictions)\n",
    "#flat_epoch_labels = flatten_list_of_lists(epoch_labels)\n",
    "\n",
    "\n",
    "metrics_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "accuracy = metrics_dict['accuracy']\n",
    "#print(f\"accuracy: {accuracy}\")\n",
    "macroavg_dict = metrics_dict[\"macro avg\"]\n",
    "str([f\"{key}: {macroavg_dict[key]}\" for key in list(macroavg_dict.keys())])\n",
    "#\n",
    "macroavg_dict[\"accuracy\"] = accuracy\n",
    "# https://stackoverflow.com/questions/52139110/how-to-change-the-order-of-keys-in-a-python-3-5-dictionary-using-another-list-a\n",
    "macroavg_dict_order = {k : macroavg_dict[k] for k in [\"accuracy\", \"precision\", \"recall\", \"f1-score\", \"support\"]}\n",
    "macroavg_dict_order[\"support\"] = round(macroavg_dict_order[\"support\"]) # 134762.0\n",
    "metrics = [f\"{key}: {macroavg_dict_order[key]}\" for key in macroavg_dict_order.keys()]\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0407dbe-3909-4b2a-8731-42266e571f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea11ab-9c07-4dfe-a377-a9368827fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "[f\"{key}: {macroavg_dict[key]}\" for key in macroavg_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e3b942-f61f-4c37-9581-4f1999abbf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [1, 1, 0]\n",
    "y_true = [1, 1, 1]\n",
    "print(classification_report(y_true, y_pred, labels=[1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452434c-96d4-4be4-94d2-c02474e6a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "macroavg_dict_order[\"support\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cb5cc1-285e-44dc-a408-1b179c9ba081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934e3956-c44b-4831-b1d1-ee7aad5ed53a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aedc2c2-0059-4b74-ab3f-126388ad27a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5269d6e-e4cb-49b5-93e3-0896a0f9ba4d",
   "metadata": {},
   "source": [
    "## Save logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36a6f5-abd3-4cc5-abf6-311ed3250556",
   "metadata": {},
   "outputs": [],
   "source": [
    "LoRA_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d82dce9-d795-4212-ae87-a4e4e2d0075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"logsdict\"\n",
    "filename += f\"__split={split}\"\n",
    "filename += f\"__r={LoRA_params_dict['r']}\"\n",
    "filename += f\"__bias={LoRA_params_dict['bias']}\"\n",
    "filename += f\"__loradroput=0point{str(LoRA_params_dict['lora_dropout'])[2:]}\"\n",
    "filename += f\"__uuid={uuid4()}\"\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f58157b-a936-4221-9271-cf8a37eedd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{output_dir}/{filename}.json\", \"w\") as outfile: \n",
    "\tjson.dump(logs_dict, outfile, indent=2)\n",
    "logs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afba571-3efb-4191-b61d-ea2690e61e17",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50068c2d-f2b5-44f9-9e7e-a258292e4332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load inference model\n",
    "config = PeftConfig.from_pretrained(output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "inference_model = PeftModel.from_pretrained(base_model, output_dir).merge_and_unload()\n",
    "print(f\"type(inference_model):\\n{type(inference_model)}\\n\")\n",
    "# get inputs from text (source: https://en.wikipedia.org/wiki/Konstanz#History)\n",
    "text = \"Konstanz was the birthplace of Count Ferdinand von Zeppelin, constructor of the famous Zeppelin airships.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f987d0b-c576-4487-ab74-03c86d9ddf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "with torch.no_grad():\n",
    "    logits = inference_model(**inputs).logits\n",
    "tokens = inputs.tokens()\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "print(f\"token (prediction)\\n\")\n",
    "for token, prediction in zip(tokens, predictions[0].numpy()):\n",
    "    print(f\"{token} ({id2label[str(prediction)]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f39b0ad-1be7-4b81-9d3e-933dab612028",
   "metadata": {},
   "source": [
    "$\\checkmark$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
