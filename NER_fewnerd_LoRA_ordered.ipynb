{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f55bab0",
   "metadata": {},
   "source": [
    "- *It works!*\n",
    "- DONE: use `max_batchsize` from `utils`\n",
    "- DONE: make **dedicated notebook** to (i) compare `conll2003` to `fewnerd` and (ii) to bring `fewnerd` into the same format.\n",
    "- DONE: use fewnerd\n",
    "- DONE: get total flops count with **einops**\n",
    "- DONE: make custom splits\n",
    "- DONE: reorder the notebook cells:\n",
    "  - DONE: model (LoRA) and tokenizer (save peftconfig if necessary)\n",
    "  - DONE: dataset etc\n",
    "  - DONE: training, metrics and saving (tokenizer and model)\n",
    "  - inference via loading saved items (probably tokenizer, model and peftconfig â€“ OR follow this [guide](https://huggingface.co/docs/peft/v0.9.0/en/package_reference/lora#peft.LoraModel))\n",
    "- In **Using the fine-tuned model**, [merge and unload](https://huggingface.co/docs/peft/v0.6.2/en/package_reference/tuners#peft.LoraModel.merge_and_unload) or [reinstantiate](https://huggingface.co/docs/peft/v0.6.2/en/task_guides/token-classification-lora#inference) the LoRA model!\n",
    "- build `results.json` (consider pandas series) via dict and save it. It holds: splits, specified loraconfig details, flops, metrics (per epoch)\n",
    "- adjust batch size â€“ and if necessary epochs â€“ for 3000 training steps `num_steps = train_instances*epochs/batch_size` $\\geq$ 3000 $\\Rightarrow$ `batch_size` $\\leq$ `train_instances*epochs/3000 = train_instances/1000` $\\Rightarrow$ `batch_size` $\\leq$ `train_instances / 1000` for `epochs = 3` \n",
    "- add comments\n",
    "- make new notebook copy for sweep\n",
    "\n",
    "## [Token classification](https://huggingface.co/course/chapter7/2?fw=pt)\n",
    "The first application we'll explore is token classification. This generic task encompasses any problem that can be formulated as \"attributing a label to each token in a sentence\", such as:\n",
    "- **Named entity recognition (NER)**: Find the entities (such as persons, locations, or organizations) in a sentence. This can be formulated as attributing a label to each token by having one class per entity and one class for \"no entity.\"\n",
    "- **Part-of-speech tagging (POS)**: Mark each word in a sentence as corresponding to a particular part of speech (such as noun, verb, adjective, etc.).\n",
    "- **Chunking**: Find the tokens that belong to the same entity. This task (which can be combined with POS or NER) can be formulated as attributing one label (usually B-) to any tokens that are at the beginning of a chunk, another label (usually I-) to tokens that are inside a chunk, and a third label (usually O) to tokens that don't belong to any chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4b5296-cb60-4875-a7f3-96e2c0c03fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-03 02:41:19,680] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/matthias/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from peft import LoraConfig, TaskType,prepare_model_for_kbit_training, get_peft_model\n",
    "from utils import get_max_instance, get_max_batchsize\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "from evaluate import load\n",
    "from tqdm.auto import tqdm\n",
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler, pipeline, AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from huggingface_hub import login\n",
    "from torch.utils.data import DataLoader\n",
    "from deepspeed.profiling.flops_profiler import FlopsProfiler\n",
    "\n",
    "load_dotenv()\n",
    "login(token=os.getenv(\"HUGGINGFACE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9864ca-1c11-44bd-8bf5-b1db6cb243d5",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5fe577-f14f-4f86-a4a8-c2b85662178c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'0': 'O',\n",
       "  '1': 'art',\n",
       "  '2': 'building',\n",
       "  '3': 'event',\n",
       "  '4': 'location',\n",
       "  '5': 'organization',\n",
       "  '6': 'other',\n",
       "  '7': 'person',\n",
       "  '8': 'product'},\n",
       " {'O': '0',\n",
       "  'art': '1',\n",
       "  'building': '2',\n",
       "  'event': '3',\n",
       "  'location': '4',\n",
       "  'organization': '5',\n",
       "  'other': '6',\n",
       "  'person': '7',\n",
       "  'product': '8'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"DFKI-SLT/few-nerd\", \"supervised\")\n",
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "label_names = ner_feature.feature.names\n",
    "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "id2label, label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a0699e7-2d6d-4e8d-98a8-ea77afd75809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"FacebookAI/roberta-large\"\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_id,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "091ef51a-7595-4d88-b57d-5d5a636f4ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA config:\n",
      "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.TOKEN_CLS: 'TOKEN_CLS'>, inference_mode=False, r=512, target_modules={'query_proj', 'value_proj', 'key', 'query', 'value', 'key_proj'}, lora_alpha=8, lora_dropout=0.2, fan_in_fan_out=False, bias='lora_only', use_rslora=True, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={})\n",
      "\n",
      "base_model type:\n",
      "<class 'transformers.models.roberta.modeling_roberta.RobertaForTokenClassification'>\n",
      "\n",
      "adapter_model type:\n",
      "<class 'peft.peft_model.PeftModelForTokenClassification'>\n",
      "\n",
      "trainable parameters:\n",
      "75506697\n",
      "\n",
      "all parameters:\n",
      "429826066\n",
      "\n",
      "trainable fraction:\n",
      "0.17567\n"
     ]
    }
   ],
   "source": [
    "# LoRA model\n",
    "# datasets:      3 values [1%, 10%, 100%]\n",
    "# lora_rank:    10 values [1, ..., 512]\n",
    "# lora_dropout:  5 values [0, 0.1, 0.2, 0.3, 0.4]\n",
    "# lora_bias:     3 values [\"all\", \"none\", \"lora_only\"]\n",
    "# => 3 x 10 x 5 x 3 = 450 sweeps per notebook\n",
    "# => start with 2 dataset values (10%, 100%) and 3 rank values (2, 8, 32) => 6 sweeps\n",
    "r = 512\n",
    "# r =   1 => (  156681, 354476050, 0.00044)\n",
    "# r =   2 => (  304137, 354623506, 0.00086)\n",
    "# r =   4 => (  599049, 354918418, 0.00169)\n",
    "# r =   8 => ( 1188873, 355508242, 0.00334)\n",
    "# r =  16 => ( 2368521, 356687890, 0.00664)\n",
    "# r =  32 => ( 4727817, 359047186, 0.01317)\n",
    "# r =  64 => ( 9446409, 363765778, 0.02597)\n",
    "# r = 128 => (18883593, 373202962, 0.0506)\n",
    "# r = 256 => (37757961, 392077330, 0.0963)\n",
    "# r = 512 => (75506697, 429826066, 0.17567)\n",
    "config = LoraConfig(\n",
    "    # GUIDE   => https://huggingface.co/docs/peft/main/en/conceptual_guides/lora#common-lora-parameters-in-peft\n",
    "    # https://huggingface.co/docs/peft/main/en/conceptual_guides/lora#common-lora-parameters-in-peft:~:text=use_rslora%3A%20When%20set%20to%20True%2C%20uses%20Rank%2DStabilized%20LoRA%20which%20sets%20the%20adapter%20scaling%20factor\n",
    "    # https://arxiv.org/abs/2312.03732, \n",
    "    r=r,\n",
    "    target_modules=[\"query\", \"key\", \"value\", \"query_proj\", \"key_proj\", \"value_proj\"],\n",
    "    bias=\"lora_only\",\n",
    "    use_rslora=True,\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    lora_dropout=0.2\n",
    ")\n",
    "print(f\"LoRA config:\\n{config}\\n\")\n",
    "adapter_model = prepare_model_for_kbit_training(base_model)\n",
    "adapter_model = get_peft_model(adapter_model, config)\n",
    "print(f\"base_model type:\\n{type(base_model)}\\n\\nadapter_model type:\\n{type(adapter_model)}\")\n",
    "trainable_params, all_params = adapter_model.get_nb_trainable_parameters()\n",
    "trainable_fraction = round(trainable_params/all_params, 5)\n",
    "print(f\"\\ntrainable parameters:\\n{trainable_params}\\n\\nall parameters:\\n{all_params}\\n\\ntrainable fraction:\\n{trainable_fraction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "330cc610-6a56-4631-b226-df0419ac07f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer is fast: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='FacebookAI/roberta-large', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_prefix_space=True)\n",
    "print(f\"tokenizer is fast: {tokenizer.is_fast}\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8461807-8ab1-4253-ac37-c335c9423fa7",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ad64867-2e55-45ce-b7eb-4cc521a02074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
       "        num_rows: 131767\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
       "        num_rows: 18824\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
       "        num_rows: 37648\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"DFKI-SLT/few-nerd\", \"supervised\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9221b549-f8f4-4ed6-9ac2-98d9429614d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 188239\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "fewnerd_all_processed = concatenate_datasets([raw_datasets[\"train\"], raw_datasets[\"validation\"], raw_datasets[\"test\"]]).map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "fewnerd_all_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cecc64b1-a3bf-4e22-9ac6-8718e78998d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_100: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 136002\n",
       "    })\n",
       "    train_10: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 13601\n",
       "    })\n",
       "    train_1: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1361\n",
       "    })\n",
       "    valid_100: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 24001\n",
       "    })\n",
       "    valid_10: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2401\n",
       "    })\n",
       "    valid_1: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 241\n",
       "    })\n",
       "    test_100: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 28236\n",
       "    })\n",
       "    test_10: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2824\n",
       "    })\n",
       "    test_1: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 283\n",
       "    })\n",
       "    dev_train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 16\n",
       "    })\n",
       "    dev_valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter dataset by length if necessary\n",
    "trainvalid_test_splits = fewnerd_all_processed.train_test_split(test_size=0.15)\n",
    "test_split_100 = trainvalid_test_splits[\"test\"]\n",
    "test_split_10 = test_split_100.train_test_split(test_size = 0.1)[\"test\"]\n",
    "test_split_1 = test_split_100.train_test_split(test_size = 0.01)[\"test\"]\n",
    "trainvalid_split = trainvalid_test_splits[\"train\"]\n",
    "train_valid_split = trainvalid_split.train_test_split(test_size=0.15)\n",
    "valid_split_100 = train_valid_split[\"test\"]\n",
    "valid_split_10 = valid_split_100.train_test_split(test_size = 0.1)[\"test\"]\n",
    "valid_split_1 = valid_split_100.train_test_split(test_size = 0.01)[\"test\"]\n",
    "train_split_100 = train_valid_split[\"train\"]\n",
    "train_split_10 = train_split_100.train_test_split(test_size = 0.1)[\"test\"]\n",
    "train_split_1 = train_split_100.train_test_split(test_size = 0.01)[\"test\"]\n",
    "dev_train_split = train_split_100.train_test_split(test_size = 16)[\"test\"]\n",
    "dev_valid_split = valid_split_100.train_test_split(test_size = 4)[\"test\"]\n",
    "fewnerd_dsd = DatasetDict({\n",
    "    \"train_100\": train_split_100,\n",
    "    \"train_10\": train_split_10,\n",
    "    \"train_1\": train_split_1,\n",
    "    \"valid_100\": valid_split_100,\n",
    "    \"valid_10\": valid_split_10,\n",
    "    \"valid_1\": valid_split_1,\n",
    "    \"test_100\": test_split_100,\n",
    "    \"test_10\": test_split_10,\n",
    "    \"test_1\": test_split_1,\n",
    "    \"dev_train\": dev_train_split,\n",
    "    \"dev_valid\": dev_valid_split\n",
    "})\n",
    "fewnerd_dsd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ce12c9-fad5-4c94-af66-d77aade82d78",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "608c18ab-8a49-468d-9975-04bc6f0630a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForTokenClassification(tokenizer=RobertaTokenizerFast(name_or_path='FacebookAI/roberta-large', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}, padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd71e709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    0,    0,    0,    0,    5,    5,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    5,    5,\n",
       "            5,    5,    5,    0, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100],\n",
       "        [-100,    4,    4,    4,    4,    4,    0,    0,    0,    0,    0,    0,\n",
       "            4,    4,    0,    0,    0,    4,    4,    4,    4,    4,    4,    4,\n",
       "            4,    0,    4,    4,    0,    4,    0,    0,    0,    0,    4,    0,\n",
       "            0,    0,    4,    4,    0, -100]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([fewnerd_dsd[\"dev_train\"][i] for i in [2, 3]])\n",
    "batch[\"labels\"] # As we can see, the second set of labels has been padded to the length of the first one using -100s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9ebf1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"seqeval\", module_type: \"metric\", features: {'predictions': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence')}, usage: \"\"\"\n",
       "Produces labelling scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions: List of List of predicted labels (Estimated targets as returned by a tagger)\n",
       "    references: List of List of reference labels (Ground truth (correct) target values)\n",
       "    suffix: True if the IOB prefix is after type, False otherwise. default: False\n",
       "    scheme: Specify target tagging scheme. Should be one of [\"IOB1\", \"IOB2\", \"IOE1\", \"IOE2\", \"IOBES\", \"BILOU\"].\n",
       "        default: None\n",
       "    mode: Whether to count correct entity labels with incorrect I/B tags as true positives or not.\n",
       "        If you want to only count exact matches, pass mode=\"strict\". default: None.\n",
       "    sample_weight: Array-like of shape (n_samples,), weights for individual samples. default: None\n",
       "    zero_division: Which value to substitute as a metric value when encountering zero division. Should be on of 0, 1,\n",
       "        \"warn\". \"warn\" acts as 0, but the warning is raised.\n",
       "\n",
       "Returns:\n",
       "    'scores': dict. Summary of the scores for overall and per type\n",
       "        Overall:\n",
       "            'accuracy': accuracy,\n",
       "            'precision': precision,\n",
       "            'recall': recall,\n",
       "            'f1': F1 score, also known as balanced F-score or F-measure,\n",
       "        Per type:\n",
       "            'precision': precision,\n",
       "            'recall': recall,\n",
       "            'f1': F1 score, also known as balanced F-score or F-measure\n",
       "Examples:\n",
       "\n",
       "    >>> predictions = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
       "    >>> references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
       "    >>> seqeval = evaluate.load(\"seqeval\")\n",
       "    >>> results = seqeval.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['MISC', 'PER', 'overall_precision', 'overall_recall', 'overall_f1', 'overall_accuracy']\n",
       "    >>> print(results[\"overall_f1\"])\n",
       "    0.5\n",
       "    >>> print(results[\"PER\"][\"f1\"])\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = load(\"seqeval\")\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ba75a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b954012-9bee-4710-85ae-4c81582f5644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/matthias/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size\t1\tworks!\n",
      "Batch size\t2\tworks!\n",
      "Batch size\t4\tworks!\n",
      "Batch size\t8\tworks!\n",
      "Batch size\t16\tworks!\n",
      "Batch size\t32\tworks!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(476, 32, 16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_max_instance\n",
    "max_instance, len_max_instance = get_max_instance(fewnerd_dsd[\"train_100\"])\n",
    "# get_max batch size\n",
    "max_batchsize = get_max_batchsize(adapter_model, max_instance, data_collator)\n",
    "batch_size = min(max_batchsize, 16) # diminishing speedup beyond batch_size=16 (and fewer loss minimization steps)\n",
    "len_max_instance, max_batchsize, batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7149eb1",
   "metadata": {},
   "source": [
    "> <font color=\"darkgreen\">ðŸ’¡ If the output directory you are using already exists, it needs to be a local clone of the repository you want to push to. If it isn't, you'll get an error when defining your `Trainer` and will need to set a new name.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3892469b",
   "metadata": {},
   "source": [
    "The above command returns the URL of the commit it just did, if you want to inspect it.\n",
    "\n",
    "The `Trainer` also drafts a model card with all the evaluation results and uploads it. At this stage, you can use the inference widget on the Model Hub to test your model and share it with your friends. You have successfully fine-tuned a model on a token classification task â€” congratulations!\n",
    "\n",
    "If you want to dive a bit more deeply into the training loop, we will now show you how to do the same thing using ðŸ¤— Accelerate.\n",
    "\n",
    "### A custom training loop\n",
    "Let's now take a look at the full training loop, so you can easily customize the parts you need. It will look a lot like what we did in [Chapter 3](https://huggingface.co/course/chapter3/4), with a few changes for the evaluation.\n",
    "\n",
    "#### Preparing everything for training\n",
    "First we need to build the DataLoaders from our datasets. We'll reuse our `data_collator` as a `collate_fn` and shuffle the training set, but not the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b560aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7f2b91f1dc90>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f2b91f1fe80>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = batch_size # 16 => 19min, 32 => 11min, 64 => 9min\n",
    "train_dataloader = DataLoader(\n",
    "    fewnerd_dsd[\"dev_train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "valid_dataloader = DataLoader(fewnerd_dsd[\"dev_valid\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68292f26",
   "metadata": {},
   "source": [
    "Once we have all those objects, we can send them to the `accelerator.prepare()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1911f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(adapter_model.parameters(), lr=2e-5) # also sweep learning rate!!!\n",
    "accelerator = Accelerator()\n",
    "acc_model, optimizer, train_dataloader, valid_dataloader = accelerator.prepare(\n",
    "    adapter_model,\n",
    "    optimizer,\n",
    "    train_dataloader,\n",
    "    valid_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a9652",
   "metadata": {},
   "source": [
    "> <font color=\"darkgreen\">ðŸš¨ If you're training on a TPU, you'll need to move all the code starting from the cell above into a dedicated training function. See [Chapter 3](https://huggingface.co/course/chapter3) for more details.</font>\n",
    "\n",
    "Now that we have sent our `train_dataloader` to `accelerator.prepare()`, we can use its length to compute the number of training steps. Remember that we should always do this after preparing the dataloader, as that method will change its length. We use a classic linear schedule from the learning rate to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "897170de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "num_training_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6342bf6d",
   "metadata": {},
   "source": [
    "Lastly, to push our model to the Hub, we will need to create a `Repository` object in a working folder. First log in to Hugging Face, if you're not logged in already. We'll determine the repository name from the model ID we want to give our model (feel free to replace the `repo_name` with your own choice; it just needs to contain your username, which is what the function `get_full_repo_name()` does):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38a925fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import Repository, get_full_repo_name\n",
    "model_name = \"bert-finetuned-ner-accelerate\"\n",
    "#repo_name = get_full_repo_name(model_name)\n",
    "#repo_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86070fb",
   "metadata": {},
   "source": [
    "Then we can clone that repository in a local folder. If it already exists, this local folder should be an existing clone of the repository we are working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd1fce8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_logs/FacebookAI-roberta-large\n"
     ]
    }
   ],
   "source": [
    "model_folder = re.sub(\"/\", \"-\", model_id)\n",
    "output_dir = f\"ner_logs/{model_folder}\"\n",
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1380b80",
   "metadata": {},
   "source": [
    "We can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. This will help us upload the intermediate models at the end of each epoch.\n",
    "\n",
    "#### Training loop\n",
    "We are now ready to write the full training loop. To simplify its evaluation part, we define this `postprocess()` function that takes predictions and labels and converts them to lists of strings, like our `metric` object expects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8d12525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55b1f4b",
   "metadata": {},
   "source": [
    "Then we can write the training loop. After defining a progress bar to follow how training goes, the loop has three parts:\n",
    "- The training in itself, which is the classic iteration over the `train_dataloader`, forward pass through the model, then backward pass and optimizer step.\n",
    "- The evaluation, in which there is a novelty after getting the outputs of our model on a batch: since two processes may have padded the inputs and labels to different shapes, we need to use `accelerator.pad_across_processes()` to make the predictions and labels the same shape before calling the `gather()` method. If we don't do this, the evaluation will either error out or hang forever. Then we send the results to `metric.add_batch()` and call `metric.compute()` once the evaluation loop is over.\n",
    "- Saving and uploading, where we first save the model and the tokenizer, then call `repo.push_to_hub()`. Notice that we use the argument `blocking=False` to tell the ðŸ¤— Hub library to push in an asynchronous process. This way, training continues normally and this (long) instruction is executed in the background.\n",
    "\n",
    "Here's the complete code for the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe65a0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b267d32faff142f1aad0647893d2bd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-03 02:41:59,114] [INFO] [profiler.py:80:start_profile] Flops profiler started\n",
      "\n",
      "training\n",
      "['input_ids shape: [16, 88]', 'attention_mask shape: [16, 88]', 'labels shape: [16, 88]']\n",
      "logits shape: [16, 88, 9], loss: 1.7704017162322998\n",
      "\n",
      "validation\n",
      "['input_ids shape: [4, 32]', 'attention_mask shape: [4, 32]', 'labels shape: [4, 32]']\n",
      "logits shape: [4, 32, 9], loss: 0.8765009641647339\n",
      "\n",
      "epoch 0: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'accuracy': 0.8589743589743589}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/.local/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: person seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/matthias/.local/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: location seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/matthias/.local/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: other seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/matthias/.local/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: organization seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/matthias/.local/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/matthias/.local/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-03 02:42:00,955] [INFO] [profiler.py:80:start_profile] Flops profiler started\n",
      "epoch 1: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'accuracy': 0.8589743589743589}\n",
      "[2024-03-03 02:42:02,850] [INFO] [profiler.py:80:start_profile] Flops profiler started\n",
      "epoch 2: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'accuracy': 0.8589743589743589}\n",
      "[2024-03-03 02:42:04,538] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[457926115328, 457078341632, 456813051904]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prof = FlopsProfiler(acc_model) # deepspeed profiler\n",
    "flops_list = []\n",
    "training_start = True\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    acc_model.train()\n",
    "    prof.start_profile() # start profiling\n",
    "    for batch in train_dataloader:\n",
    "        outputs = acc_model(**batch)\n",
    "        if training_start:\n",
    "            print(\"\\ntraining\")\n",
    "            print([f\"{key} shape: {list(batch[key].shape)}\" for key in list(batch.keys())])\n",
    "            print(f\"logits shape: {list(outputs['logits'].shape)}, loss: {float(outputs['loss'])}\\n\")\n",
    "            #training_start = False\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    prof.stop_profile() # stop profiling\n",
    "    total_flops = prof.get_total_flops()\n",
    "    flops_list.append(total_flops)\n",
    "    # Evaluation\n",
    "    acc_model.eval()\n",
    "    for batch in valid_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = acc_model(**batch)\n",
    "        if training_start:\n",
    "            print(\"validation\")\n",
    "            print([f\"{key} shape: {list(batch[key].shape)}\" for key in list(batch.keys())])\n",
    "            print(f\"logits shape: {list(outputs['logits'].shape)}, loss: {float(outputs['loss'])}\\n\")\n",
    "            training_start = False\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "    results = metric.compute()\n",
    "    print(f\"epoch {epoch}:\", {key: results[f\"overall_{key}\"] for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]})\n",
    "    # save acc_model\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(acc_model)\n",
    "    unwrapped_model.save_pretrained(output_dir)\n",
    "    # save tokenizer\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "#\n",
    "prof.end_profile() # end profiling\n",
    "flops_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "821a27c5-b6af-42bb-94f4-721a3f9c2e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1371817508864, 457272502954.6667)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flops_array = np.array(flops_list)\n",
    "np.sum(flops_array), np.mean(flops_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14741ac-8b4b-46ed-9ebe-c9b2ca4dabdd",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c985d689-c4b6-4e27-bb93-1dc1d8bde905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ner_logs/FacebookAI-roberta-large'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7cce8c0-6bb9-45ac-bbc8-262b51794af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adapter_config.json\t   merges.txt  special_tokens_map.json\ttokenizer.json\n",
      "adapter_model.safetensors  README.md   tokenizer_config.json\tvocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls 'ner_logs/FacebookAI-roberta-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53487377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'LABEL_1',\n",
       "  'score': 0.8647184,\n",
       "  'index': 1,\n",
       "  'word': 'My',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.80932593,\n",
       "  'index': 2,\n",
       "  'word': 'Ä name',\n",
       "  'start': 3,\n",
       "  'end': 7},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.8445053,\n",
       "  'index': 3,\n",
       "  'word': 'Ä is',\n",
       "  'start': 8,\n",
       "  'end': 10},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.8221055,\n",
       "  'index': 4,\n",
       "  'word': 'Ä Sylv',\n",
       "  'start': 11,\n",
       "  'end': 15},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.80996996,\n",
       "  'index': 5,\n",
       "  'word': 'ain',\n",
       "  'start': 15,\n",
       "  'end': 18},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.80984956,\n",
       "  'index': 6,\n",
       "  'word': 'Ä and',\n",
       "  'start': 19,\n",
       "  'end': 22},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.78882635,\n",
       "  'index': 7,\n",
       "  'word': 'Ä I',\n",
       "  'start': 23,\n",
       "  'end': 24},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.8518306,\n",
       "  'index': 8,\n",
       "  'word': 'Ä work',\n",
       "  'start': 25,\n",
       "  'end': 29},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.8421916,\n",
       "  'index': 9,\n",
       "  'word': 'Ä at',\n",
       "  'start': 30,\n",
       "  'end': 32},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.86573136,\n",
       "  'index': 10,\n",
       "  'word': 'Ä Hug',\n",
       "  'start': 33,\n",
       "  'end': 36},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.85786694,\n",
       "  'index': 11,\n",
       "  'word': 'ging',\n",
       "  'start': 36,\n",
       "  'end': 40},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.8620421,\n",
       "  'index': 12,\n",
       "  'word': 'Ä Face',\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.8259794,\n",
       "  'index': 13,\n",
       "  'word': 'Ä in',\n",
       "  'start': 46,\n",
       "  'end': 48},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.80838317,\n",
       "  'index': 14,\n",
       "  'word': 'Ä Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.841474,\n",
       "  'index': 15,\n",
       "  'word': '.',\n",
       "  'start': 57,\n",
       "  'end': 58}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the adapter and merge it into the base model: https://huggingface.co/docs/peft/v0.6.2/en/task_guides/token-classification-lora#inference\n",
    " # local folder for model checkpoint\n",
    "token_classifier = pipeline(\"token-classification\", model=output_dir)#, aggregation_strategy=\"simple\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32a0fd5e-f84c-4992-944d-93d8bae24ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ner_logs/FacebookAI-roberta-large'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f29c4a1-e75d-4261-bff7-0de3afc1e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import RobertaModelForTokenClassification\n",
    "from peft import AutoPeftModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9137385-f192-459d-8be8-943aa3c1cf4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FacebookAI/roberta-large'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.base_model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "747526ad-b385-4e1c-b7cf-0d80aa2f3720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'PeftModelForTokenClassification(\n  (base_model): LoraModel(\n    (model): RobertaForTokenClassification(\n      (roberta): RobertaModel(\n        (embeddings): RobertaEmbeddings(\n          (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n          (position_embeddings): Embedding(514, 1024, padding_idx=1)\n          (token_type_embeddings): Embedding(1, 1024)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): RobertaEncoder(\n          (layer): ModuleList(\n            (0-23): 24 x RobertaLayer(\n              (attention): RobertaAttention(\n                (self): RobertaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.2, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=512, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=512, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.2, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=512, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=512, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.2, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=512, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=512, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): RobertaSelfOutput(\n                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): RobertaIntermediate(\n                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): RobertaOutput(\n                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n      (classifier): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=1024, out_features=9, bias=True)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=1024, out_features=9, bias=True)\n        )\n      )\n    )\n  )\n)'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(config\u001b[38;5;241m.\u001b[39mbase_model_name_or_path)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#model = PeftModel.from_pretrained(inference_model, output_dir) # RobertaForTokenClassification\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaForTokenClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43minference_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:2600\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   2599\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m-> 2600\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2606\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2608\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2609\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2610\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2611\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2612\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2613\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2614\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   2615\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:430\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[1;32m    108\u001b[0m ):\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         has_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:164\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have -- or .. in repo_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'PeftModelForTokenClassification(\n  (base_model): LoraModel(\n    (model): RobertaForTokenClassification(\n      (roberta): RobertaModel(\n        (embeddings): RobertaEmbeddings(\n          (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n          (position_embeddings): Embedding(514, 1024, padding_idx=1)\n          (token_type_embeddings): Embedding(1, 1024)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): RobertaEncoder(\n          (layer): ModuleList(\n            (0-23): 24 x RobertaLayer(\n              (attention): RobertaAttention(\n                (self): RobertaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.2, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=512, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=512, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.2, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=512, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=512, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.2, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=512, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=512, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): RobertaSelfOutput(\n                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): RobertaIntermediate(\n                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): RobertaOutput(\n                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n      (classifier): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=1024, out_features=9, bias=True)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=1024, out_features=9, bias=True)\n        )\n      )\n    )\n  )\n)'."
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import RobertaForTokenClassification\n",
    "config = config\n",
    "inference_model = AutoPeftModelForTokenClassification.from_pretrained(\n",
    "    #config.base_model_name_or_path, # this ...\n",
    "    output_dir,                     # ... or this\n",
    "    #num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "#model = PeftModel.from_pretrained(inference_model, output_dir) # RobertaForTokenClassification\n",
    "model = RobertaForTokenClassification.from_pretrained(inference_model, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1bdb42-be91-43e8-a124-49aa6250ea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)#, aggregation_strategy=\"simple\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f39b0ad-1be7-4b81-9d3e-933dab612028",
   "metadata": {},
   "source": [
    "$\\checkmark$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
