{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f55bab0",
   "metadata": {},
   "source": [
    "# Token classification with LoRA\n",
    "Done:\n",
    "- count flops via [flops-profiler](https://pypi.org/project/flops-profiler/) in accelerate loop\n",
    "\n",
    "ToDo:\n",
    "- backup (always – never finished)\n",
    "- use LoRA model\n",
    "- find batch size automatically\n",
    "- add wandb sweep\n",
    "- tidy up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2af0502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-22 15:57:05,248] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'words', 'ner_tags', 'fine_ner_tags'],\n",
       "    num_rows: 188239\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "from transformers import pipeline, AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, get_scheduler\n",
    "from evaluate import load\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "from deepspeed.profiling.flops_profiler import FlopsProfiler\n",
    "fewnerd = load_dataset(\"DFKI-SLT/few-nerd\", \"supervised\")\n",
    "fewnerd_all = concatenate_datasets([fewnerd[\"train\"], fewnerd[\"validation\"], fewnerd[\"test\"]]).rename_column(\"tokens\", \"words\")\n",
    "fewnerd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d19680c-1de0-41cc-b2b9-51396abccfaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'words', 'ner_tags', 'fine_ner_tags'],\n",
       "    num_rows: 188024\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 104 # => 188024 (104 works, 105 doesn't work [CUDA OOM])\n",
    "fewnerd_all = fewnerd_all.filter(lambda example: len(example[\"words\"])<=x)\n",
    "fewnerd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7419537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Known',\n",
       "  'locally',\n",
       "  'as',\n",
       "  '``',\n",
       "  'Fairbottom',\n",
       "  'Bobs',\n",
       "  '``',\n",
       "  'it',\n",
       "  'is',\n",
       "  'now',\n",
       "  'preserved',\n",
       "  'at',\n",
       "  'the',\n",
       "  'Henry',\n",
       "  'Ford',\n",
       "  'Museum',\n",
       "  'in',\n",
       "  'Dearborn',\n",
       "  ',',\n",
       "  'Michigan',\n",
       "  '.'],\n",
       " [0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 4, 0, 4, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 22\n",
    "fewnerd_all[idx][\"words\"], fewnerd_all[idx][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6559bfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'art', 'building', 'event', 'location', 'organization', 'other', 'person', 'product']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'0': 'O',\n",
       "  '1': 'art',\n",
       "  '2': 'building',\n",
       "  '3': 'event',\n",
       "  '4': 'location',\n",
       "  '5': 'organization',\n",
       "  '6': 'other',\n",
       "  '7': 'person',\n",
       "  '8': 'product'},\n",
       " {'O': '0',\n",
       "  'art': '1',\n",
       "  'building': '2',\n",
       "  'event': '3',\n",
       "  'location': '4',\n",
       "  'organization': '5',\n",
       "  'other': '6',\n",
       "  'person': '7',\n",
       "  'product': '8'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = fewnerd_all.features[\"ner_tags\"].feature.names\n",
    "print(label_names)\n",
    "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "id2label, label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c3c4c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known locally as `` Fairbottom Bobs    `` it is now preserved at the Henry    Ford     Museum   in Dearborn , Michigan . \n",
      "O     O       O  O  product    product O  O  O  O   O         O  O   building building building O  location O location O \n"
     ]
    }
   ],
   "source": [
    "words = fewnerd_all[idx][\"words\"]\n",
    "labels = fewnerd_all[idx][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29750b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaTokenizerFast(name_or_path='FacebookAI/roberta-large', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"FacebookAI/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "print(tokenizer)\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbb96836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<s>',\n",
       "  'ĠKnown',\n",
       "  'Ġlocally',\n",
       "  'Ġas',\n",
       "  'Ġ``',\n",
       "  'ĠFair',\n",
       "  'bottom',\n",
       "  'ĠBob',\n",
       "  's',\n",
       "  'Ġ``',\n",
       "  'Ġit',\n",
       "  'Ġis',\n",
       "  'Ġnow',\n",
       "  'Ġpreserved',\n",
       "  'Ġat',\n",
       "  'Ġthe',\n",
       "  'ĠHenry',\n",
       "  'ĠFord',\n",
       "  'ĠMuseum',\n",
       "  'Ġin',\n",
       "  'ĠDear',\n",
       "  'born',\n",
       "  'Ġ,',\n",
       "  'ĠMichigan',\n",
       "  'Ġ.',\n",
       "  '</s>'],\n",
       " [None,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  None])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(fewnerd_all[idx][\"words\"], is_split_into_words=True)\n",
    "inputs.tokens(), inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e8718c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 4, 0, 4, 0]\n",
      "[-100, 0, 0, 0, 0, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 4, 4, 0, 4, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n",
    "\n",
    "labels = fewnerd_all[22][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14e3211d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 188024\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"words\"], truncation=True, is_split_into_words=True)\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "fewnerd_all_tokenized = fewnerd_all.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=fewnerd_all.column_names\n",
    ")\n",
    "fewnerd_all_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "def857db-b757-4904-8bee-8a14aad96ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_100: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 135847\n",
       "    })\n",
       "    valid_100: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 23973\n",
       "    })\n",
       "    test_100: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 28204\n",
       "    })\n",
       "    train_10: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 13585\n",
       "    })\n",
       "    valid_10: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2398\n",
       "    })\n",
       "    test_10: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2821\n",
       "    })\n",
       "    train_1: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1359\n",
       "    })\n",
       "    valid_1: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 240\n",
       "    })\n",
       "    test_1: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 283\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make splits\n",
    "dev_split = fewnerd_all_tokenized.train_test_split(test_size=4)[\"test\"]\n",
    "trainvalid_test_splits = fewnerd_all_tokenized.train_test_split(test_size=0.15)\n",
    "trainvalid_split = trainvalid_test_splits[\"train\"]\n",
    "test_split_100 = trainvalid_test_splits[\"test\"]\n",
    "test_split_10 = test_split_100.train_test_split(test_size=0.1)[\"test\"]\n",
    "test_split_1 = test_split_10.train_test_split(test_size=0.1)[\"test\"]\n",
    "train_valid_split = trainvalid_split.train_test_split(test_size=0.15)\n",
    "train_split_100 = train_valid_split[\"train\"]\n",
    "train_split_10 = train_split_100.train_test_split(test_size=0.1)[\"test\"]\n",
    "train_split_1 = train_split_10.train_test_split(test_size=0.1)[\"test\"]\n",
    "valid_split_100 = train_valid_split[\"test\"]\n",
    "valid_split_10 = valid_split_100.train_test_split(test_size=0.1)[\"test\"]\n",
    "valid_split_1 = valid_split_10.train_test_split(test_size=0.1)[\"test\"]\n",
    "fewnerd_ds = DatasetDict({\n",
    "    \"train_100\": train_split_100,\n",
    "    \"valid_100\": valid_split_100,\n",
    "    \"test_100\": test_split_100,\n",
    "    \"train_10\": train_split_10,\n",
    "    \"valid_10\": valid_split_10,\n",
    "    \"test_10\": test_split_10,\n",
    "    \"train_1\": train_split_1,\n",
    "    \"valid_1\": valid_split_1,\n",
    "    \"test_1\": test_split_1,\n",
    "    \"dev\": dev_split\n",
    "})\n",
    "fewnerd_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e17216a-6ab0-4b2e-bdea-63f58df6e027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "14\n",
      "14\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "instance = fewnerd_ds[\"dev\"][0]\n",
    "keys = instance.keys()\n",
    "print(keys)\n",
    "for key in keys:\n",
    "    print(len(instance[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd71e709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForTokenClassification(tokenizer=RobertaTokenizerFast(name_or_path='FacebookAI/roberta-large', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}, padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebfb61d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    4,    4,    4,    4,    4,    4,    4,    0,    0,    0,    0,\n",
       "            0,    0,    4,    4,    4,    4,    0,    4,    4,    0,    4,    0,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100],\n",
       "        [-100,    0,    0,    0,    0,    0,    4,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    4,    4,    4,    0,    4,    4,    4,    0,\n",
       "            4,    4,    4,    4,    0,    0,    0,    0,    4,    4,    4,    0,\n",
       "            0,    0,    0,    0,    0,    0, -100]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([fewnerd_ds[\"train_1\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8acf1d2-9176-4045-965a-1e571a7511fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 4, 0, 4, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9ebf1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25deb1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = fewnerd_ds[\"train\"][0][\"labels\"]\n",
    "#labels = [label_names[i] for i in labels]\n",
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ba75a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b560aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = False\n",
    "if dev:\n",
    "    train_dataloader = DataLoader(fewnerd_ds[\"dev\"], shuffle=True, collate_fn=data_collator, batch_size=8) # 8, 4, 2\n",
    "    eval_dataloader = DataLoader(fewnerd_ds[\"dev\"], collate_fn=data_collator, batch_size=8)\n",
    "else:\n",
    "    train_dataloader = DataLoader(fewnerd_ds[\"train_10\"], shuffle=True, collate_fn=data_collator, batch_size=8) # 8, 4, 2\n",
    "    eval_dataloader = DataLoader(fewnerd_ds[\"valid_10\"], collate_fn=data_collator, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88717727-9081-4149-a7a9-48df3b082949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.TOKEN_CLS: 'TOKEN_CLS'>, inference_mode=False, r=4, target_modules={'value_proj', 'query', 'query_proj', 'key_proj', 'key', 'value'}, lora_alpha=8, lora_dropout=0.2, fan_in_fan_out=False, bias='all', use_rslora=True, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading adapter weights from logs led to unexpected keys not found in the model:  ['classifier.modules_to_save.bias', 'classifier.original_module.bias']. \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from peft import PeftModel\n",
    "from peft import LoraConfig, prepare_model_for_int8_training, get_peft_model\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "# datasets:      3 values [1%, 10%, 100%]\n",
    "# lora_rank:    10 values [1, ..., 512]\n",
    "# lora_dropout:  3 values [0, 0.2, 0.4]\n",
    "\n",
    "r = 4\n",
    "config = LoraConfig(\n",
    "    # GUIDE   => https://huggingface.co/docs/peft/main/en/conceptual_guides/lora#common-lora-parameters-in-peft\n",
    "    # https://arxiv.org/abs/2312.03732, https://huggingface.co/docs/peft/main/en/conceptual_guides/lora#common-lora-parameters-in-peft:~:text=use_rslora%3A%20When%20set%20to%20True%2C%20uses%20Rank%2DStabilized%20LoRA%20which%20sets%20the%20adapter%20scaling%20factor\n",
    "    r=r,\n",
    "    target_modules=[\"query\", \"key\", \"value\", \"query_proj\", \"key_proj\", \"value_proj\"],\n",
    "    bias=\"all\",\n",
    "    use_rslora=True, \n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    lora_dropout=0.2\n",
    ")\n",
    "print(config)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = prepare_model_for_int8_training(model)\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9a9b41-a20b-4aee-9d6e-272fa32a3180",
   "metadata": {},
   "source": [
    "```\n",
    "model = LlamaForTokenClassification.from_pretrained(\n",
    "    model_id, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    ").bfloat16()\n",
    "peft_config = LoraConfig(task_type=TaskType.TOKEN_CLS, inference_mode=False, r=lora_r, lora_alpha=32, lora_dropout=0.1)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1911f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_dataloader,\n",
    "    eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "897170de",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8d12525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe65a0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2b8e2b40de40c791f9abe630810dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5097 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-22 16:06:19,558] [INFO] [profiler.py:80:start_profile] Flops profiler started\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"logs\"\n",
    "\n",
    "prof = FlopsProfiler(model) # deepspeed profiler\n",
    "profile_step = 5\n",
    "flops_list = []\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    prof.start_profile() # start profiling\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    #\n",
    "    prof.stop_profile() # stop profiling\n",
    "    total_flops = prof.get_total_flops()\n",
    "    flops_list.append(total_flops)\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "    results = metric.compute()\n",
    "    print(f\"epoch {epoch}:\", {key: results[f\"overall_{key}\"] for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]})\n",
    "    # Save model and tokenizer\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "#\n",
    "prof.end_profile() # end profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8bad0da-18dd-438a-a1d5-549169417da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1401275658240, 1392458122240, 1423246919680]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1405660233386.6667, 4216980700160)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_flops_list = np.array(flops_list)\n",
    "print(flops_list)\n",
    "np.mean(np_flops_list), np.sum(np_flops_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53487377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'LABEL_1',\n",
       "  'score': 0.75313264,\n",
       "  'word': 'My name is Sylvain and I work at Hugging Face in Brooklyn.',\n",
       "  'start': 0,\n",
       "  'end': 58}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = output_dir # local folder for model checkpoint\n",
    "token_classifier = pipeline(\"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4f74380-1f40-4c36-b72a-ec81725a2c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "4\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    rank = 2**i\n",
    "    print(rank)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
