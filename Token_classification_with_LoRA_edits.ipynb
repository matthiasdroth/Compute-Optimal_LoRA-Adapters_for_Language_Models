{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f55bab0",
   "metadata": {},
   "source": [
    "## Token classification with LoRA\n",
    "ToDo:\n",
    "- backup\n",
    "- count flops via [flops-profiler](https://pypi.org/project/flops-profiler/) in accelerate loop\n",
    "- use LoRA model\n",
    "- find batch size automatically\n",
    "- add wandb sweep\n",
    "- tidy up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2af0502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-21 02:20:16,039] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'words', 'ner_tags', 'fine_ner_tags'],\n",
       "    num_rows: 188239\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepspeed.profiling.flops_profiler import FlopsProfiler\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "fewnerd = load_dataset(\"DFKI-SLT/few-nerd\", \"supervised\")\n",
    "fewnerd_all = concatenate_datasets([fewnerd[\"train\"], fewnerd[\"validation\"], fewnerd[\"test\"]]).rename_column(\"tokens\", \"words\")\n",
    "fewnerd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d19680c-1de0-41cc-b2b9-51396abccfaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bbd5f0903764fe2860525984473e24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/188239 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'words', 'ner_tags', 'fine_ner_tags'],\n",
       "    num_rows: 188024\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results\n",
    "# does work   : 50, 80, 90, 100, 104\n",
    "# doesn't work: 105, 110, 120, 140\n",
    "x = 104 # => 188024\n",
    "fewnerd_all = fewnerd_all.filter(lambda example: len(example[\"words\"])<=x)\n",
    "fewnerd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7419537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paul', 'International', 'airport', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewnerd_all[0][\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "902edf20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewnerd_all[0][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42a2a6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'art', 'building', 'event', 'location', 'organization', 'other', 'person', 'product'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_feature = fewnerd_all.features[\"ner_tags\"]\n",
    "ner_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6559bfa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'art',\n",
       " 'building',\n",
       " 'event',\n",
       " 'location',\n",
       " 'organization',\n",
       " 'other',\n",
       " 'person',\n",
       " 'product']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = ner_feature.feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c3c4c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul International airport . \n",
      "O    O             O       O \n"
     ]
    }
   ],
   "source": [
    "words = fewnerd_all[0][\"words\"]\n",
    "labels = fewnerd_all[0][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29750b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='FacebookAI/roberta-large', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"FacebookAI/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efd4a2e3-1f5c-43c3-b8f8-dc1d019ec1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbb96836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'ĠPaul', 'ĠInternational', 'Ġairport', 'Ġ.', '</s>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(fewnerd_all[0][\"words\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4235d7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e8718c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7d5f3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0]\n",
      "[-100, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = fewnerd_all[0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14e3211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"words\"], truncation=True, is_split_into_words=True)\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc178482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305cebd290074e518a5c57cf7de7e5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/188024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 188024\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewnerd_all_tokenized = fewnerd_all.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=fewnerd_all.column_names\n",
    ")\n",
    "fewnerd_all_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "def857db-b757-4904-8bee-8a14aad96ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 135847\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 23973\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 28204\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make splits\n",
    "dev_split = fewnerd_all_tokenized.train_test_split(test_size=4)[\"test\"]\n",
    "trainvalid_test_splits = fewnerd_all_tokenized.train_test_split(test_size=0.15)\n",
    "test_split = trainvalid_test_splits[\"test\"]\n",
    "trainvalid_split = trainvalid_test_splits[\"train\"]\n",
    "train_valid_split = trainvalid_split.train_test_split(test_size=0.15)\n",
    "train_split = train_valid_split[\"train\"]\n",
    "valid_split = train_valid_split[\"test\"]\n",
    "fewnerd_ds = DatasetDict({\n",
    "    \"train\": train_split,\n",
    "    \"valid\": valid_split,\n",
    "    \"test\": test_split,\n",
    "    \"dev\": dev_split\n",
    "})\n",
    "fewnerd_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e17216a-6ab0-4b2e-bdea-63f58df6e027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "15\n",
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "instance = fewnerd_ds[\"dev\"][0]\n",
    "keys = instance.keys()\n",
    "print(keys)\n",
    "for key in keys:\n",
    "    print(len(instance[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd71e709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForTokenClassification(tokenizer=RobertaTokenizerFast(name_or_path='FacebookAI/roberta-large', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}, padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebfb61d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    0,    0,    0,    0,    0,    0,    4,    4,    0,    4,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    7,    8,    8,    0,    0,\n",
       "            5,    5,    5,    5,    7,    8,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    7,\n",
       "            8,    0,    0,    0,    0,    0, -100],\n",
       "        [-100,    0,    5,    6,    0,    0,    0,    0,    0,    0,    0, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([fewnerd_ds[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8acf1d2-9176-4045-965a-1e571a7511fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9ebf1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "metric = load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25deb1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = fewnerd_ds[\"train\"][0][\"labels\"]\n",
    "#labels = [label_names[i] for i in labels]\n",
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ba75a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca58a879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'0': 'O',\n",
       "  '1': 'art',\n",
       "  '2': 'building',\n",
       "  '3': 'event',\n",
       "  '4': 'location',\n",
       "  '5': 'organization',\n",
       "  '6': 'other',\n",
       "  '7': 'person',\n",
       "  '8': 'product'},\n",
       " {'O': '0',\n",
       "  'art': '1',\n",
       "  'building': '2',\n",
       "  'event': '3',\n",
       "  'location': '4',\n",
       "  'organization': '5',\n",
       "  'other': '6',\n",
       "  'person': '7',\n",
       "  'product': '8'})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "id2label, label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b68504e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b560aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(fewnerd_ds[\"train\"], shuffle=True, collate_fn=data_collator, batch_size=8) # 8, 4, 2\n",
    "eval_dataloader = DataLoader(fewnerd_ds[\"valid\"], collate_fn=data_collator, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96e61e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b21c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1911f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_dataloader,\n",
    "    eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "897170de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38a925fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mdroth/FacebookAI_roberta-large-finetuned-ner-accelerate'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "model_name = \"FacebookAI_roberta-large-finetuned-ner-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8d12525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58cd5736-df4e-42ac-afcf-68ef5f8d1891",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe65a0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06496944638a4e34b2491b6d0106ecdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-21 02:20:46,416] [INFO] [profiler.py:80:start_profile] Flops profiler started\n",
      "4637609522103936\n",
      "\n",
      "-------------------------- DeepSpeed Flops Profiler --------------------------\n",
      "Profile Summary at step 5:\n",
      "Notations:\n",
      "data parallel size (dp_size), model parallel size(mp_size),\n",
      "number of parameters (params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (flops), floating-point operations per second (FLOPS),\n",
      "fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n",
      "step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n",
      "\n",
      "params per GPU:                                                         354.32 M\n",
      "params of model = params per GPU * mp_size:                             0       \n",
      "fwd MACs per GPU:                                                       2317.76 TMACs\n",
      "fwd flops per GPU:                                                      4637.61 T\n",
      "fwd flops of model = fwd flops per GPU * mp_size:                       4637.61 T\n",
      "fwd latency:                                                            1.27 Ks \n",
      "fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    3.64 TFLOPS\n",
      "\n",
      "----------------------------- Aggregated Profile per GPU -----------------------------\n",
      "Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n",
      "depth 0:\n",
      "    params      - {'RobertaForTokenClassification': '354.32 M'}\n",
      "    MACs        - {'RobertaForTokenClassification': '2317.76 TMACs'}\n",
      "    fwd latency - {'RobertaForTokenClassification': '1.27 Ks'}\n",
      "depth 1:\n",
      "    params      - {'RobertaModel': '354.31 M'}\n",
      "    MACs        - {'RobertaModel': '2317.69 TMACs'}\n",
      "    fwd latency - {'RobertaModel': '1.27 Ks'}\n",
      "depth 2:\n",
      "    params      - {'RobertaEncoder': '302.31 M'}\n",
      "    MACs        - {'RobertaEncoder': '2317.69 TMACs'}\n",
      "    fwd latency - {'RobertaEncoder': '1.25 Ks'}\n",
      "depth 3:\n",
      "    params      - {'ModuleList': '302.31 M'}\n",
      "    MACs        - {'ModuleList': '2317.69 TMACs'}\n",
      "    fwd latency - {'ModuleList': '1.25 Ks'}\n",
      "depth 4:\n",
      "    params      - {'RobertaLayer': '302.31 M'}\n",
      "    MACs        - {'RobertaLayer': '2317.69 TMACs'}\n",
      "    fwd latency - {'RobertaLayer': '1.25 Ks'}\n",
      "depth 5:\n",
      "    params      - {'RobertaAttention': '100.81 M'}\n",
      "    MACs        - {'RobertaAttention': '787.83 TMACs'}\n",
      "    fwd latency - {'RobertaAttention': '591.15 s'}\n",
      "depth 6:\n",
      "    params      - {'Linear': '201.45 M'}\n",
      "    MACs        - {'Linear': '1529.87 TMACs'}\n",
      "    fwd latency - {'Linear': '501.13 s'}\n",
      "\n",
      "------------------------------ Detailed Profile per GPU ------------------------------\n",
      "Each module profile is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n",
      "\n",
      "RobertaForTokenClassification(\n",
      "  354.32 M = 100% Params, 2317.76 TMACs = 100% MACs, 1.27 Ks = 100% latency, 3.64 TFLOPS\n",
      "  (roberta): RobertaModel(\n",
      "    354.31 M = 100% Params, 2317.69 TMACs = 100% MACs, 1.27 Ks = 99.58% latency, 3.66 TFLOPS\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      52 M = 14.68% Params, 0 MACs = 0% MACs, 9.61 s = 0.75% latency, 4.05 GFLOPS\n",
      "      (word_embeddings): Embedding(51.47 M = 14.53% Params, 0 MACs = 0% MACs, 1.25 s = 0.1% latency, 0 FLOPS, 50265, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(526.34 K = 0.15% Params, 0 MACs = 0% MACs, 721.14 ms = 0.06% latency, 0 FLOPS, 514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1.02 K = 0% Params, 0 MACs = 0% MACs, 1.04 s = 0.08% latency, 0 FLOPS, 1, 1024)\n",
      "      (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 1.06 s = 0.08% latency, 36.55 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 921.77 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      302.31 M = 85.32% Params, 2317.69 TMACs = 100% MACs, 1.25 Ks = 98.47% latency, 3.7 TFLOPS\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 53.94 s = 4.24% latency, 3.58 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 26.26 s = 2.06% latency, 2.5 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 18.86 s = 1.48% latency, 2.64 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 4.47 s = 0.35% latency, 3.57 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.44 s = 0.27% latency, 4.63 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.4 s = 0.27% latency, 4.69 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 864.9 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.62 s = 0.52% latency, 2.41 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.48 s = 0.27% latency, 4.58 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 920.59 ms = 0.07% latency, 42.26 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 769.77 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.71 s = 1% latency, 5.01 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.75 s = 0.84% latency, 5.93 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.21 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.18 s = 1.03% latency, 4.84 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.07 s = 0.79% latency, 6.33 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 871.93 ms = 0.07% latency, 44.62 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 802.28 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.92 s = 4.08% latency, 3.72 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.61 s = 1.93% latency, 2.67 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.35 s = 1.36% latency, 2.87 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.46 s = 0.27% latency, 4.6 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.38 s = 0.27% latency, 4.71 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.37 s = 0.26% latency, 4.72 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 861.54 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.52 s = 0.51% latency, 2.45 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.44 s = 0.27% latency, 4.63 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 881.2 ms = 0.07% latency, 44.15 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 760.26 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.62 s = 0.99% latency, 5.05 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.74 s = 0.84% latency, 5.94 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.14 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.11 s = 1.03% latency, 4.87 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.02 s = 0.79% latency, 6.36 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 856.68 ms = 0.07% latency, 45.42 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 801.79 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.79 s = 4.07% latency, 3.73 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.55 s = 1.93% latency, 2.68 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.27 s = 1.36% latency, 2.88 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.47 s = 0.27% latency, 4.59 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.37 s = 0.26% latency, 4.72 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.36 s = 0.26% latency, 4.74 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 848.79 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.53 s = 0.51% latency, 2.45 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.46 s = 0.27% latency, 4.6 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 875.38 ms = 0.07% latency, 44.45 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 762.75 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.59 s = 0.99% latency, 5.06 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.73 s = 0.84% latency, 5.94 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.14 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.13 s = 1.03% latency, 4.86 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.05 s = 0.79% latency, 6.34 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 849.95 ms = 0.07% latency, 45.78 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 810.96 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.83 s = 4.07% latency, 3.73 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.56 s = 1.93% latency, 2.68 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.29 s = 1.36% latency, 2.88 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.47 s = 0.27% latency, 4.6 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.7 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.36 s = 0.26% latency, 4.74 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 846.21 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.52 s = 0.51% latency, 2.45 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.45 s = 0.27% latency, 4.61 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 876.1 ms = 0.07% latency, 44.41 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 764.17 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.58 s = 0.99% latency, 5.07 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.73 s = 0.84% latency, 5.94 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.14 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.17 s = 1.03% latency, 4.84 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.1 s = 0.79% latency, 6.31 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 853.04 ms = 0.07% latency, 45.61 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 798.8 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.8 s = 4.07% latency, 3.73 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.52 s = 1.93% latency, 2.68 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.24 s = 1.35% latency, 2.88 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.47 s = 0.27% latency, 4.6 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.37 s = 0.26% latency, 4.73 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.36 s = 0.26% latency, 4.74 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 853.97 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.53 s = 0.51% latency, 2.45 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.46 s = 0.27% latency, 4.61 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 868.05 ms = 0.07% latency, 44.82 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 772.74 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.6 s = 0.99% latency, 5.06 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.74 s = 0.84% latency, 5.94 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.14 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.16 s = 1.03% latency, 4.85 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.08 s = 0.79% latency, 6.32 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 850.7 ms = 0.07% latency, 45.73 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 802.57 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.88 s = 4.07% latency, 3.72 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.56 s = 1.93% latency, 2.67 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.3 s = 1.36% latency, 2.87 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.46 s = 0.27% latency, 4.6 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.38 s = 0.27% latency, 4.71 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.37 s = 0.26% latency, 4.73 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 845.12 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.52 s = 0.51% latency, 2.45 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.44 s = 0.27% latency, 4.63 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 877.53 ms = 0.07% latency, 44.34 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 762.33 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.61 s = 0.99% latency, 5.06 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.74 s = 0.84% latency, 5.93 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.13 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.18 s = 1.04% latency, 4.84 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.09 s = 0.79% latency, 6.32 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 856.17 ms = 0.07% latency, 45.44 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 801.44 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.94 s = 4.08% latency, 3.72 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.63 s = 1.93% latency, 2.67 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.32 s = 1.36% latency, 2.87 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.47 s = 0.27% latency, 4.59 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.37 s = 0.26% latency, 4.72 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.38 s = 0.27% latency, 4.72 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 850.31 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.56 s = 0.51% latency, 2.44 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.46 s = 0.27% latency, 4.6 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 894.88 ms = 0.07% latency, 43.48 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 765.83 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.6 s = 0.99% latency, 5.06 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.74 s = 0.84% latency, 5.93 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.14 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.2 s = 1.04% latency, 4.83 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.09 s = 0.79% latency, 6.31 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 865.96 ms = 0.07% latency, 44.93 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 804.81 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 52.02 s = 4.08% latency, 3.71 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.66 s = 1.94% latency, 2.66 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.37 s = 1.36% latency, 2.86 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.48 s = 0.27% latency, 4.58 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.7 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.38 s = 0.27% latency, 4.72 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 851.42 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.53 s = 0.51% latency, 2.44 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.45 s = 0.27% latency, 4.61 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 880.48 ms = 0.07% latency, 44.19 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 766.8 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.63 s = 0.99% latency, 5.05 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.76 s = 0.84% latency, 5.93 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.15 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.22 s = 1.04% latency, 4.83 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.12 s = 0.79% latency, 6.3 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 862.25 ms = 0.07% latency, 45.12 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 805.21 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.93 s = 4.08% latency, 3.72 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.58 s = 1.93% latency, 2.67 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.31 s = 1.36% latency, 2.87 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.48 s = 0.27% latency, 4.58 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.38 s = 0.27% latency, 4.72 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.38 s = 0.27% latency, 4.71 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 841.48 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.53 s = 0.51% latency, 2.45 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.47 s = 0.27% latency, 4.6 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 868.13 ms = 0.07% latency, 44.82 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 766.95 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.62 s = 0.99% latency, 5.05 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.77 s = 0.85% latency, 5.92 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.14 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.2 s = 1.04% latency, 4.83 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.11 s = 0.79% latency, 6.31 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 858.3 ms = 0.07% latency, 45.33 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 805.02 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 52 s = 4.08% latency, 3.72 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.64 s = 1.94% latency, 2.67 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.39 s = 1.37% latency, 2.86 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.48 s = 0.27% latency, 4.57 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.4 s = 0.27% latency, 4.69 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.7 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 852.85 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.51 s = 0.51% latency, 2.45 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.46 s = 0.27% latency, 4.6 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 855.01 ms = 0.07% latency, 45.5 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 766.15 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.63 s = 0.99% latency, 5.05 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.77 s = 0.85% latency, 5.92 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.14 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.2 s = 1.04% latency, 4.83 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.11 s = 0.79% latency, 6.3 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 856.44 ms = 0.07% latency, 45.43 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 805.72 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.95 s = 4.08% latency, 3.72 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.58 s = 1.93% latency, 2.67 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.33 s = 1.36% latency, 2.87 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.48 s = 0.27% latency, 4.58 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.38 s = 0.27% latency, 4.71 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.7 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 842.46 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.52 s = 0.51% latency, 2.45 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.47 s = 0.27% latency, 4.6 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 853.84 ms = 0.07% latency, 45.57 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 764.11 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.64 s = 0.99% latency, 5.04 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.78 s = 0.85% latency, 5.91 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.14 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.21 s = 1.04% latency, 4.83 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.12 s = 0.79% latency, 6.3 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 855.1 ms = 0.07% latency, 45.5 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 814.13 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.99 s = 4.08% latency, 3.72 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.6 s = 1.93% latency, 2.67 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.34 s = 1.36% latency, 2.87 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.48 s = 0.27% latency, 4.58 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.4 s = 0.27% latency, 4.68 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.7 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 840.91 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.51 s = 0.51% latency, 2.45 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.46 s = 0.27% latency, 4.6 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 856.24 ms = 0.07% latency, 45.44 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 764.39 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.64 s = 0.99% latency, 5.04 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.78 s = 0.85% latency, 5.91 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.14 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.23 s = 1.04% latency, 4.82 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.15 s = 0.8% latency, 6.28 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 851.76 ms = 0.07% latency, 45.68 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 803.66 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (12): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.97 s = 4.08% latency, 3.72 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.6 s = 1.93% latency, 2.67 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.32 s = 1.36% latency, 2.87 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.48 s = 0.27% latency, 4.57 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.4 s = 0.27% latency, 4.69 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.4 s = 0.27% latency, 4.69 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 840.57 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.53 s = 0.51% latency, 2.45 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.48 s = 0.27% latency, 4.58 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 852.66 ms = 0.07% latency, 45.63 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 775.9 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.64 s = 0.99% latency, 5.04 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.79 s = 0.85% latency, 5.91 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.14 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.21 s = 1.04% latency, 4.83 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.13 s = 0.8% latency, 6.29 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 851.15 ms = 0.07% latency, 45.71 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 804.3 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (13): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.95 s = 4.08% latency, 3.72 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.57 s = 1.93% latency, 2.67 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.33 s = 1.36% latency, 2.87 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.48 s = 0.27% latency, 4.58 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.41 s = 0.27% latency, 4.68 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.7 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 839.91 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.5 s = 0.51% latency, 2.46 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.46 s = 0.27% latency, 4.6 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 849.62 ms = 0.07% latency, 45.79 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 763.62 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.64 s = 0.99% latency, 5.04 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.79 s = 0.85% latency, 5.91 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.14 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.22 s = 1.04% latency, 4.82 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.13 s = 0.8% latency, 6.29 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 854.1 ms = 0.07% latency, 45.55 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 806.1 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (14): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.95 s = 4.08% latency, 3.72 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.57 s = 1.93% latency, 2.67 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.32 s = 1.36% latency, 2.87 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.49 s = 0.27% latency, 4.57 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.4 s = 0.27% latency, 4.69 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.4 s = 0.27% latency, 4.69 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 839.16 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.51 s = 0.51% latency, 2.45 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.48 s = 0.27% latency, 4.58 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 849.87 ms = 0.07% latency, 45.78 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 761.56 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.65 s = 0.99% latency, 5.04 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.79 s = 0.85% latency, 5.91 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.14 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.21 s = 1.04% latency, 4.83 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.13 s = 0.8% latency, 6.29 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 851.66 ms = 0.07% latency, 45.68 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 803.11 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (15): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.98 s = 4.08% latency, 3.72 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.59 s = 1.93% latency, 2.67 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.33 s = 1.36% latency, 2.87 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.49 s = 0.27% latency, 4.57 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.41 s = 0.27% latency, 4.68 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.7 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 840.9 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.51 s = 0.51% latency, 2.45 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.47 s = 0.27% latency, 4.59 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 848.69 ms = 0.07% latency, 45.84 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 764.24 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.66 s = 0.99% latency, 5.04 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.79 s = 0.85% latency, 5.91 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.14 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.22 s = 1.04% latency, 4.82 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.15 s = 0.8% latency, 6.28 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 849.88 ms = 0.07% latency, 45.78 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 799.43 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (16): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.96 s = 4.08% latency, 3.72 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.53 s = 1.93% latency, 2.68 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.28 s = 1.36% latency, 2.88 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.49 s = 0.27% latency, 4.57 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.4 s = 0.27% latency, 4.69 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.7 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 837.61 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.5 s = 0.51% latency, 2.46 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.47 s = 0.27% latency, 4.59 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 845.63 ms = 0.07% latency, 46.01 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 762.06 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.72 s = 1% latency, 5.01 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.8 s = 0.85% latency, 5.9 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.2 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.2 s = 1.04% latency, 4.83 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.13 s = 0.8% latency, 6.29 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 846.18 ms = 0.07% latency, 45.98 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 800.05 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (17): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.92 s = 4.08% latency, 3.72 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.55 s = 1.93% latency, 2.68 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.3 s = 1.36% latency, 2.87 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.49 s = 0.27% latency, 4.57 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.41 s = 0.27% latency, 4.68 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.7 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 848.82 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.5 s = 0.51% latency, 2.46 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.47 s = 0.27% latency, 4.6 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 844.41 ms = 0.07% latency, 46.08 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 767.1 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.64 s = 0.99% latency, 5.04 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.79 s = 0.85% latency, 5.91 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.13 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.2 s = 1.04% latency, 4.83 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.13 s = 0.8% latency, 6.29 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 845.28 ms = 0.07% latency, 46.03 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 799.94 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (18): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.87 s = 4.07% latency, 3.72 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.53 s = 1.93% latency, 2.68 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.27 s = 1.36% latency, 2.88 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.48 s = 0.27% latency, 4.57 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.69 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.7 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 839.11 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.52 s = 0.51% latency, 2.45 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.48 s = 0.27% latency, 4.58 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 847.08 ms = 0.07% latency, 45.93 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 764.76 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.64 s = 0.99% latency, 5.04 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.79 s = 0.85% latency, 5.91 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.14 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.19 s = 1.04% latency, 4.83 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.13 s = 0.8% latency, 6.29 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 847.7 ms = 0.07% latency, 45.9 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 801.1 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (19): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.87 s = 4.07% latency, 3.73 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.51 s = 1.92% latency, 2.68 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.27 s = 1.36% latency, 2.88 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.48 s = 0.27% latency, 4.57 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.7 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.7 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 840.76 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.5 s = 0.51% latency, 2.46 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.47 s = 0.27% latency, 4.6 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 844.88 ms = 0.07% latency, 46.05 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 764.06 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.65 s = 0.99% latency, 5.04 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.79 s = 0.85% latency, 5.91 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.14 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.2 s = 1.04% latency, 4.83 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.13 s = 0.8% latency, 6.29 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 844.91 ms = 0.07% latency, 46.05 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 800.21 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (20): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.84 s = 4.07% latency, 3.73 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.51 s = 1.92% latency, 2.68 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.28 s = 1.36% latency, 2.88 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.49 s = 0.27% latency, 4.57 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.7 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.7 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 840.57 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.49 s = 0.51% latency, 2.46 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.47 s = 0.27% latency, 4.6 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 841.78 ms = 0.07% latency, 46.22 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 762.54 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.64 s = 0.99% latency, 5.04 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.79 s = 0.85% latency, 5.91 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.14 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.19 s = 1.04% latency, 4.84 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.13 s = 0.8% latency, 6.29 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 840.67 ms = 0.07% latency, 46.28 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 799.77 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (21): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.83 s = 4.07% latency, 3.73 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.49 s = 1.92% latency, 2.68 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.25 s = 1.35% latency, 2.88 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.49 s = 0.27% latency, 4.57 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.69 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.7 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 838.45 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.5 s = 0.51% latency, 2.46 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.47 s = 0.27% latency, 4.59 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 848.19 ms = 0.07% latency, 45.87 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 765.06 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.64 s = 0.99% latency, 5.04 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.79 s = 0.85% latency, 5.91 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.14 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.19 s = 1.04% latency, 4.84 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.12 s = 0.79% latency, 6.3 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 838.96 ms = 0.07% latency, 46.37 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 803.22 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (22): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.81 s = 4.07% latency, 3.73 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.48 s = 1.92% latency, 2.68 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.25 s = 1.35% latency, 2.88 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.48 s = 0.27% latency, 4.57 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.7 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.7 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 839.31 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.49 s = 0.51% latency, 2.46 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.47 s = 0.27% latency, 4.6 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 841.2 ms = 0.07% latency, 46.25 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 761.71 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.63 s = 0.99% latency, 5.05 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.78 s = 0.85% latency, 5.91 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.13 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.19 s = 1.04% latency, 4.84 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.12 s = 0.79% latency, 6.3 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 841.96 ms = 0.07% latency, 46.21 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 802.36 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (23): RobertaLayer(\n",
      "          12.6 M = 3.56% Params, 96.57 TMACs = 4.17% MACs, 51.77 s = 4.07% latency, 3.73 TFLOPS\n",
      "          (attention): RobertaAttention(\n",
      "            4.2 M = 1.19% Params, 32.83 TMACs = 1.42% MACs, 24.47 s = 1.92% latency, 2.69 TFLOPS\n",
      "            (self): RobertaSelfAttention(\n",
      "              3.15 M = 0.89% Params, 24.86 TMACs = 1.07% MACs, 17.23 s = 1.35% latency, 2.89 TFLOPS\n",
      "              (query): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.49 s = 0.27% latency, 4.57 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.4 s = 0.27% latency, 4.69 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.39 s = 0.27% latency, 4.71 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 837.75 ms = 0.07% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 6.5 s = 0.51% latency, 2.46 TFLOPS\n",
      "              (dense): Linear(1.05 M = 0.3% Params, 7.97 TMACs = 0.34% MACs, 3.47 s = 0.27% latency, 4.6 TFLOPS, in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 843.5 ms = 0.07% latency, 46.13 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 763.13 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 12.63 s = 0.99% latency, 5.05 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.78 s = 0.85% latency, 5.91 TFLOPS, in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 1.13 s = 0.09% latency, 0 FLOPS)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 13.16 s = 1.03% latency, 4.85 TFLOPS\n",
      "            (dense): Linear(4.2 M = 1.18% Params, 31.87 TMACs = 1.38% MACs, 10.11 s = 0.79% latency, 6.31 TFLOPS, in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 839.63 ms = 0.07% latency, 46.34 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 797.92 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 783.43 ms = 0.06% latency, 0 FLOPS, p=0.1, inplace=False)\n",
      "  (classifier): Linear(9.22 K = 0% Params, 70.03 GMACs = 0% MACs, 1.23 s = 0.1% latency, 114.31 GFLOPS, in_features=1024, out_features=9, bias=True)\n",
      ")\n",
      "------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/Desktop/Compute-Optimal_LoRA_Adapters_for_Causal-LM/env/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: building seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/matthias/Desktop/Compute-Optimal_LoRA_Adapters_for_Causal-LM/env/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: location seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/matthias/Desktop/Compute-Optimal_LoRA_Adapters_for_Causal-LM/env/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: organization seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/matthias/Desktop/Compute-Optimal_LoRA_Adapters_for_Causal-LM/env/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: other seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/matthias/Desktop/Compute-Optimal_LoRA_Adapters_for_Causal-LM/env/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: product seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/matthias/Desktop/Compute-Optimal_LoRA_Adapters_for_Causal-LM/env/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: event seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/matthias/Desktop/Compute-Optimal_LoRA_Adapters_for_Causal-LM/env/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: person seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/matthias/Desktop/Compute-Optimal_LoRA_Adapters_for_Causal-LM/env/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: art seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'precision': 0.8309083339560405, 'recall': 0.783785960990444, 'f1': 0.8066595504337102, 'accuracy': 0.938159417036487}\n",
      "[2024-02-21 03:41:05,862] [INFO] [profiler.py:80:start_profile] Flops profiler started\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "prof = FlopsProfiler(model) # deepspeed profiler\n",
    "profile_step = 5\n",
    "flops_list = []\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    prof.start_profile() # start profiling\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    #\n",
    "    prof.stop_profile()  # stop profiling\n",
    "    total_flops = prof.get_total_flops()\n",
    "    print(f\"{total_flops}\")\n",
    "    flops_list.append(total_flops)\n",
    "    prof.print_model_profile(profile_step=profile_step)\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "        # pad predictions and labels before being gathered\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "    results = metric.compute()\n",
    "    print(f\"epoch {epoch}:\", {key: results[f\"overall_{key}\"] for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]})\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "#\n",
    "print(f\"{prof.get_total_flops()}\")\n",
    "prof.end_profile() # end profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eb7409",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53487377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = output_dir # local folder for model checkpoint\n",
    "token_classifier = pipeline(\"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55626d1-f6ab-4f4c-bd85-becedde4c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flops_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
