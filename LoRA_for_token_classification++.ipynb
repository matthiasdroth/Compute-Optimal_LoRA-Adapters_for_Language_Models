{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "406e4345-3bdb-45aa-bd69-da2cb12f4332",
   "metadata": {},
   "source": [
    "For details, see the original notebook on `LoRA_for_token_classifcation` [[link](https://github.com/matthiasdroth/Compute-Optimal_LoRA-Adapters_for_Language_Models/blob/main/LoRA_for_token_classification.ipynb)].\n",
    "\n",
    "ToDo:\n",
    "- change model to `FacebookAI/roberta-large`\n",
    "- change dataset to `DFKI-SLT/few-nerd`\n",
    "- change training to `accelerate`\n",
    "- count FLOPs via `einops` in training loop\n",
    "- add logic to find maximum batch size\n",
    "- add basic sweep and log to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce964de-ab92-498e-9732-42ee6ddd263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "checkpoint = \"FacebookAI/roberta-large\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de7eef5d-d68c-47e1-82ca-c2186e59690e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'words', 'ner_tags', 'fine_ner_tags'],\n",
       "    num_rows: 188239\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewnerd = load_dataset(\"DFKI-SLT/few-nerd\", \"supervised\")\n",
    "fewnerd_all = concatenate_datasets([fewnerd[\"train\"], fewnerd[\"validation\"], fewnerd[\"test\"]])\n",
    "#fewnerd_all = fewnerd_all.rename_column(\"tokens\", \"text\")\n",
    "fewnerd_all = fewnerd_all.rename_column(\"tokens\", \"words\")\n",
    "fewnerd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7334e0f1-3d2c-4344-a077-a25da647e71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Known',\n",
       " 'locally',\n",
       " 'as',\n",
       " '``',\n",
       " 'Fairbottom',\n",
       " 'Bobs',\n",
       " '``',\n",
       " 'it',\n",
       " 'is',\n",
       " 'now',\n",
       " 'preserved',\n",
       " 'at',\n",
       " 'the',\n",
       " 'Henry',\n",
       " 'Ford',\n",
       " 'Museum',\n",
       " 'in',\n",
       " 'Dearborn',\n",
       " ',',\n",
       " 'Michigan',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 22\n",
    "fewnerd_all[i][\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2107dc1b-8c12-47dd-a169-107e5662cba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 4, 0, 4, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewnerd_all[i][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff863ef-8423-498b-a782-5f288f991a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 21)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fewnerd_all[i][\"words\"]), len(fewnerd_all[i][\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91418514-8ece-430e-9a40-2affb3d8a42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'art',\n",
       " 'building',\n",
       " 'event',\n",
       " 'location',\n",
       " 'organization',\n",
       " 'other',\n",
       " 'person',\n",
       " 'product']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = fewnerd_all.features[\"ner_tags\"].feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9dc199d-530d-4c54-8e0e-f0d3cf0475f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known locally as `` Fairbottom Bobs    `` it is now preserved at the Henry    Ford     Museum   in Dearborn , Michigan . \n",
      "O     O       O  O  product    product O  O  O  O   O         O  O   building building building O  location O location O \n"
     ]
    }
   ],
   "source": [
    "i = 22\n",
    "words = fewnerd_all[i][\"words\"]\n",
    "labels = fewnerd_all[i][\"ner_tags\"]\n",
    "assert len(words)==len(labels)\n",
    "# https://github.com/matthiasdroth/Huggingface-course/blob/main/7.2-Token_classification.ipynb\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c4075b9-aae6-406a-87a1-f292b189a310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='FacebookAI/roberta-large', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "313490c8-a061-4d6a-aa94-1d80632612fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75492528-d3b0-4e85-8a93-df3b77552017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:\t0\tword:\tKnown\n",
      "label:\t0\tword:\tlocally\n",
      "label:\t0\tword:\tas\n",
      "label:\t0\tword:\t``\n",
      "label:\t8\tword:\tFairbottom\n",
      "label:\t8\tword:\tBobs\n",
      "label:\t0\tword:\t``\n",
      "label:\t0\tword:\tit\n",
      "label:\t0\tword:\tis\n",
      "label:\t0\tword:\tnow\n",
      "label:\t0\tword:\tpreserved\n",
      "label:\t0\tword:\tat\n",
      "label:\t0\tword:\tthe\n",
      "label:\t2\tword:\tHenry\n",
      "label:\t2\tword:\tFord\n",
      "label:\t2\tword:\tMuseum\n",
      "label:\t0\tword:\tin\n",
      "label:\t4\tword:\tDearborn\n",
      "label:\t0\tword:\t,\n",
      "label:\t4\tword:\tMichigan\n",
      "label:\t0\tword:\t.\n"
     ]
    }
   ],
   "source": [
    "i = 22\n",
    "words = fewnerd_all[i][\"words\"]\n",
    "labels = fewnerd_all[i][\"ner_tags\"]\n",
    "assert len(words)==len(labels)\n",
    "for j in range(len(words)):\n",
    "    print(f\"label:\\t{labels[j]}\\tword:\\t{words[j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4e0f9c4-0dd2-4c47-bb0c-2e6cd59e547c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 21)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words), len(labels) # tokenizing idx_tokens results in a longer list => adapt labels accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6af2fdce-3f84-459d-98af-bf5d78035947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 21)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 22\n",
    "inputs = tokenizer(fewnerd_all[i][\"words\"], is_split_into_words=True)\n",
    "tokens = inputs.tokens()\n",
    "word_ids = inputs.word_ids()\n",
    "input_ids = inputs.input_ids\n",
    "attention_mask = inputs.attention_mask\n",
    "assert len(tokens)==len(word_ids) and len(tokens)==len(input_ids) and len(tokens)==len(attention_mask)\n",
    "len(tokens), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33584eda-0e88-431e-a251-9b33e16d79b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([None,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  None],\n",
       " 26)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids, len(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6612ac8a-a3d2-4dfa-8f66-bb8e2268789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "labels_cp = copy.copy(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ee692-f127-4ff7-8895-529b911ef6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8fa4172-67df-487e-a550-9a5b63eceb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 0, 20)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process only vanilla tokens, not <s> and </s> tokens\n",
    "simple_word_ids = copy.copy(word_ids)[1:-1]\n",
    "len(simple_word_ids), simple_word_ids[0], simple_word_ids[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd9046c2-9917-4a5e-91aa-ab5cc5bced38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 'ĠKnown', 'Ġ.')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_tokens = tokens[1:-1]\n",
    "len(simple_tokens), simple_tokens[0], simple_tokens[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b41efcc7-19f6-4101-9a3f-5a28ba973c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ĠKnown',\n",
       " 'Ġlocally',\n",
       " 'Ġas',\n",
       " 'Ġ``',\n",
       " 'ĠFair',\n",
       " 'bottom',\n",
       " 'ĠBob',\n",
       " 's',\n",
       " 'Ġ``',\n",
       " 'Ġit',\n",
       " 'Ġis',\n",
       " 'Ġnow',\n",
       " 'Ġpreserved',\n",
       " 'Ġat',\n",
       " 'Ġthe',\n",
       " 'ĠHenry',\n",
       " 'ĠFord',\n",
       " 'ĠMuseum',\n",
       " 'Ġin',\n",
       " 'ĠDear',\n",
       " 'born',\n",
       " 'Ġ,',\n",
       " 'ĠMichigan',\n",
       " 'Ġ.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89ac1ebe-03f8-4393-b018-236cd1b0f799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "283f1c39-243d-43e4-8954-5e738c03e58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 4, 0, 4, 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad624916-430d-477a-b832-529d2ede3c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'ĠKnown',\n",
       " 'Ġlocally',\n",
       " 'Ġas',\n",
       " 'Ġ``',\n",
       " 'ĠFair',\n",
       " 'bottom',\n",
       " 'ĠBob',\n",
       " 's',\n",
       " 'Ġ``',\n",
       " 'Ġit',\n",
       " 'Ġis',\n",
       " 'Ġnow',\n",
       " 'Ġpreserved',\n",
       " 'Ġat',\n",
       " 'Ġthe',\n",
       " 'ĠHenry',\n",
       " 'ĠFord',\n",
       " 'ĠMuseum',\n",
       " 'Ġin',\n",
       " 'ĠDear',\n",
       " 'born',\n",
       " 'Ġ,',\n",
       " 'ĠMichigan',\n",
       " 'Ġ.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "385151c5-7bf0-4748-89a6-5208321506b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \t word_id=0 \t label=0 \t token=ĠKnown\n",
      "i=1 \t word_id=1 \t label=0 \t token=Ġlocally\n",
      "i=2 \t word_id=2 \t label=0 \t token=Ġas\n",
      "i=3 \t word_id=3 \t label=8 \t token=Ġ``\n",
      "i=4 \t word_id=4 \t label=8 \t token=ĠFair\n",
      "\n",
      "i=5 \t word_id=4 \t label=8 \t token=bottom\n",
      "i=6 \t word_id=5 \t label=0 \t token=ĠBob\n",
      "\n",
      "i=7 \t word_id=5 \t label=0 \t token=s\n",
      "i=8 \t word_id=6 \t label=0 \t token=Ġ``\n",
      "i=9 \t word_id=7 \t label=0 \t token=Ġit\n",
      "i=10 \t word_id=8 \t label=0 \t token=Ġis\n",
      "i=11 \t word_id=9 \t label=0 \t token=Ġnow\n",
      "i=12 \t word_id=10 \t label=0 \t token=Ġpreserved\n",
      "i=13 \t word_id=11 \t label=0 \t token=Ġat\n",
      "i=14 \t word_id=12 \t label=2 \t token=Ġthe\n",
      "i=15 \t word_id=13 \t label=2 \t token=ĠHenry\n",
      "i=16 \t word_id=14 \t label=2 \t token=ĠFord\n",
      "i=17 \t word_id=15 \t label=0 \t token=ĠMuseum\n",
      "i=18 \t word_id=16 \t label=4 \t token=Ġin\n",
      "i=19 \t word_id=17 \t label=0 \t token=ĠDear\n",
      "\n",
      "i=20 \t word_id=17 \t label=0 \t token=born\n",
      "i=21 \t word_id=18 \t label=4 \t token=Ġ,\n",
      "i=22 \t word_id=19 \t label=0 \t token=ĠMichigan\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m label_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# get label via label_index\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# add label to match_labels\u001b[39;00m\n\u001b[1;32m     19\u001b[0m match_labels\u001b[38;5;241m.\u001b[39mappend(label)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "previous_word_id = False\n",
    "previous_label = False\n",
    "label_index = 0\n",
    "match_labels = []\n",
    "for i in range(len(simple_word_ids)):\n",
    "    word_id = simple_word_ids[i]\n",
    "    if i==0:\n",
    "        match_labels.append(labels[0])\n",
    "    if word_id==previous_word_id and i>0:# and type(previous_word_id)==int: # second condition handles \"False==0\"\n",
    "        print()\n",
    "        # add previous_label\n",
    "        match_labels.append(previous_label)\n",
    "    else:\n",
    "        # incremente label_index\n",
    "        label_index += 1\n",
    "        # get label via label_index\n",
    "        label = labels[label_index]\n",
    "        # add label to match_labels\n",
    "        match_labels.append(label)\n",
    "    # update previous_word_id\n",
    "    previous_word_id = word_id\n",
    "    # update previous_label\n",
    "    previous_label = label\n",
    "    # logs\n",
    "    print(f\"i={i} \\t word_id={word_id} \\t label={label} \\t token={tokens[i+1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85d17202-1759-40a2-9b04-1ddf65af9677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \t word_id=0 \t label=0 \t token=ĠKnown\n",
      "i=1 \t word_id=1 \t label=0 \t token=Ġlocally\n",
      "i=2 \t word_id=2 \t label=0 \t token=Ġas\n",
      "i=3 \t word_id=3 \t label=0 \t token=Ġ``\n",
      "i=4 \t word_id=4 \t label=8 \t token=ĠFair\n",
      "i=5 \t word_id=4 \t label=8 \t token=bottom\n",
      "i=6 \t word_id=5 \t label=8 \t token=ĠBob\n",
      "i=7 \t word_id=5 \t label=8 \t token=s\n",
      "i=8 \t word_id=6 \t label=0 \t token=Ġ``\n",
      "i=9 \t word_id=7 \t label=0 \t token=Ġit\n",
      "i=10 \t word_id=8 \t label=0 \t token=Ġis\n",
      "i=11 \t word_id=9 \t label=0 \t token=Ġnow\n",
      "i=12 \t word_id=10 \t label=0 \t token=Ġpreserved\n",
      "i=13 \t word_id=11 \t label=0 \t token=Ġat\n",
      "i=14 \t word_id=12 \t label=0 \t token=Ġthe\n",
      "i=15 \t word_id=13 \t label=2 \t token=ĠHenry\n",
      "i=16 \t word_id=14 \t label=2 \t token=ĠFord\n",
      "i=17 \t word_id=15 \t label=2 \t token=ĠMuseum\n",
      "i=18 \t word_id=16 \t label=0 \t token=Ġin\n",
      "i=19 \t word_id=17 \t label=4 \t token=ĠDear\n",
      "i=20 \t word_id=17 \t label=4 \t token=born\n",
      "i=21 \t word_id=18 \t label=0 \t token=Ġ,\n",
      "i=22 \t word_id=19 \t label=4 \t token=ĠMichigan\n",
      "i=23 \t word_id=20 \t label=0 \t token=Ġ.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " -100]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_word_id = False\n",
    "previous_label = False\n",
    "label_index = 0\n",
    "match_labels = []\n",
    "for i in range(len(simple_word_ids)):\n",
    "    word_id = simple_word_ids[i]\n",
    "    if word_id==previous_word_id:\n",
    "        # add previous_label\n",
    "        match_labels.append(int(previous_label)) # int(False) = 0 evaluates False to 0\n",
    "    else:\n",
    "        # incremente label_index\n",
    "        label_index += 1\n",
    "        # get label via label_index\n",
    "        label = labels[label_index]\n",
    "        # add label to match_labels\n",
    "        match_labels.append(label)\n",
    "    # update previous_word_id\n",
    "    previous_word_id = word_id\n",
    "    # update previous_label\n",
    "    previous_label = label\n",
    "    # logs\n",
    "    print(f\"i={i} \\t word_id={word_id} \\t label={label} \\t token={tokens[i+1]}\")\n",
    "\n",
    "full_match_labels = [-100] + match_labels + [-100]\n",
    "full_match_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cacd8719-a8e7-4860-ad5d-878cc505179a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 26)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_match_labels), len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc6778e0-80d3-4d5a-aae6-b95f45e76e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label -100 \ttoken <s>\n",
      "label 0 \ttoken ĠKnown\n",
      "label 0 \ttoken Ġlocally\n",
      "label 0 \ttoken Ġas\n",
      "label 0 \ttoken Ġ``\n",
      "label 8 \ttoken ĠFair\n",
      "label 8 \ttoken bottom\n",
      "label 8 \ttoken ĠBob\n",
      "label 8 \ttoken s\n",
      "label 0 \ttoken Ġ``\n",
      "label 0 \ttoken Ġit\n",
      "label 0 \ttoken Ġis\n",
      "label 0 \ttoken Ġnow\n",
      "label 0 \ttoken Ġpreserved\n",
      "label 0 \ttoken Ġat\n",
      "label 0 \ttoken Ġthe\n",
      "label 2 \ttoken ĠHenry\n",
      "label 2 \ttoken ĠFord\n",
      "label 2 \ttoken ĠMuseum\n",
      "label 0 \ttoken Ġin\n",
      "label 4 \ttoken ĠDear\n",
      "label 4 \ttoken born\n",
      "label 0 \ttoken Ġ,\n",
      "label 4 \ttoken ĠMichigan\n",
      "label 0 \ttoken Ġ.\n",
      "label -100 \ttoken </s>\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tokens)):\n",
    "    token = tokens[i]\n",
    "    label = full_match_labels[i]\n",
    "    print(f\"label {label} \\ttoken {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e82c1b1-6e2d-4c2d-a266-d41330cb66a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \t word_id=0 \t label=0 \t token=ĠKnown\n",
      "i=1 \t word_id=1 \t label=0 \t token=Ġlocally\n",
      "i=2 \t word_id=2 \t label=0 \t token=Ġas\n",
      "i=3 \t word_id=3 \t label=0 \t token=Ġ``\n",
      "i=4 \t word_id=4 \t label=8 \t token=ĠFair\n",
      "i=5 \t word_id=4 \t label=8 \t token=bottom\n",
      "i=6 \t word_id=5 \t label=8 \t token=ĠBob\n",
      "i=7 \t word_id=5 \t label=8 \t token=s\n",
      "i=8 \t word_id=6 \t label=0 \t token=Ġ``\n",
      "i=9 \t word_id=7 \t label=0 \t token=Ġit\n",
      "i=10 \t word_id=8 \t label=0 \t token=Ġis\n",
      "i=11 \t word_id=9 \t label=0 \t token=Ġnow\n",
      "i=12 \t word_id=10 \t label=0 \t token=Ġpreserved\n",
      "i=13 \t word_id=11 \t label=0 \t token=Ġat\n",
      "i=14 \t word_id=12 \t label=0 \t token=Ġthe\n",
      "i=15 \t word_id=13 \t label=2 \t token=ĠHenry\n",
      "i=16 \t word_id=14 \t label=2 \t token=ĠFord\n",
      "i=17 \t word_id=15 \t label=2 \t token=ĠMuseum\n",
      "i=18 \t word_id=16 \t label=0 \t token=Ġin\n",
      "i=19 \t word_id=17 \t label=4 \t token=ĠDear\n",
      "i=20 \t word_id=17 \t label=4 \t token=born\n",
      "i=21 \t word_id=18 \t label=0 \t token=Ġ,\n",
      "i=22 \t word_id=19 \t label=4 \t token=ĠMichigan\n",
      "i=23 \t word_id=20 \t label=0 \t token=Ġ.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " False,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " -100]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_word_id = False\n",
    "previous_label = False\n",
    "label_index = 0\n",
    "match_labels = []\n",
    "for i in range(len(simple_word_ids)):\n",
    "    word_id = simple_word_ids[i]\n",
    "    if word_id==previous_word_id:\n",
    "        # add previous_label\n",
    "        match_labels.append(previous_label)\n",
    "    else:\n",
    "        # incremente label_index\n",
    "        label_index += 1\n",
    "        # get label via label_index\n",
    "        label = labels[label_index]\n",
    "        # add label to match_labels\n",
    "        match_labels.append(label)\n",
    "    # update previous_word_id\n",
    "    previous_word_id = word_id\n",
    "    # update previous_label\n",
    "    previous_label = label\n",
    "    # logs\n",
    "    print(f\"i={i} \\t word_id={word_id} \\t label={label} \\t token={tokens[i+1]}\")\n",
    "\n",
    "full_match_labels = [-100] + match_labels + [-100]\n",
    "full_match_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10fcc905-b3a3-4521-abf4-e468c1780c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba785dd2-8bef-42ca-aff1-e2acdb88fc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(int, True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(0), type(False)==bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6e3059d-79f2-402a-b7d6-d3f54b388e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0==False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a37bd6a-45bc-493f-8e1d-70845b31d646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \t word_id=0 \t label=0 \t token=ĠKnown\n",
      "i=1 \t word_id=1 \t label=0 \t token=Ġlocally\n",
      "i=2 \t word_id=2 \t label=0 \t token=Ġas\n",
      "i=3 \t word_id=3 \t label=0 \t token=Ġ``\n",
      "i=4 \t word_id=4 \t label=8 \t token=ĠFair\n",
      "i=5 \t word_id=4 \t label=8 \t token=bottom\n",
      "i=6 \t word_id=5 \t label=8 \t token=ĠBob\n",
      "i=7 \t word_id=5 \t label=8 \t token=s\n",
      "i=8 \t word_id=6 \t label=0 \t token=Ġ``\n",
      "i=9 \t word_id=7 \t label=0 \t token=Ġit\n",
      "i=10 \t word_id=8 \t label=0 \t token=Ġis\n",
      "i=11 \t word_id=9 \t label=0 \t token=Ġnow\n",
      "i=12 \t word_id=10 \t label=0 \t token=Ġpreserved\n",
      "i=13 \t word_id=11 \t label=0 \t token=Ġat\n",
      "i=14 \t word_id=12 \t label=0 \t token=Ġthe\n",
      "i=15 \t word_id=13 \t label=2 \t token=ĠHenry\n",
      "i=16 \t word_id=14 \t label=2 \t token=ĠFord\n",
      "i=17 \t word_id=15 \t label=2 \t token=ĠMuseum\n",
      "i=18 \t word_id=16 \t label=0 \t token=Ġin\n",
      "i=19 \t word_id=17 \t label=4 \t token=ĠDear\n",
      "i=20 \t word_id=17 \t label=4 \t token=born\n",
      "i=21 \t word_id=18 \t label=0 \t token=Ġ,\n",
      "i=22 \t word_id=19 \t label=4 \t token=ĠMichigan\n",
      "i=23 \t word_id=20 \t label=0 \t token=Ġ.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " None,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " -100]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_word_id = False\n",
    "previous_label = False\n",
    "label_index = 0\n",
    "match_labels = []\n",
    "for i in range(len(simple_word_ids)):\n",
    "    word_id = simple_word_ids[i]\n",
    "    if word_id==previous_word_id:\n",
    "        # add previous_label\n",
    "        match_labels.append(previous_label)\n",
    "    else:\n",
    "        # incremente label_index\n",
    "        label_index += 1\n",
    "        # get label via label_index\n",
    "        label = labels[label_index]\n",
    "        # add label to match_labels\n",
    "        match_labels.append(label)\n",
    "    # update previous_word_id\n",
    "    previous_word_id = word_id\n",
    "    # update previous_label\n",
    "    previous_label = label\n",
    "    # logs\n",
    "    print(f\"i={i} \\t word_id={word_id} \\t label={label} \\t token={tokens[i+1]}\")\n",
    "\n",
    "full_match_labels = [-100] + match_labels + [-100]\n",
    "full_match_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f892135-3cab-4196-9aa6-e5b36a8e9863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_match_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5773f5f-9605-443d-b473-d663f87e99db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'ĠKnown',\n",
       " 'Ġlocally',\n",
       " 'Ġas',\n",
       " 'Ġ``',\n",
       " 'ĠFair',\n",
       " 'bottom',\n",
       " 'ĠBob',\n",
       " 's',\n",
       " 'Ġ``',\n",
       " 'Ġit',\n",
       " 'Ġis',\n",
       " 'Ġnow',\n",
       " 'Ġpreserved',\n",
       " 'Ġat',\n",
       " 'Ġthe',\n",
       " 'ĠHenry',\n",
       " 'ĠFord',\n",
       " 'ĠMuseum',\n",
       " 'Ġin',\n",
       " 'ĠDear',\n",
       " 'born',\n",
       " 'Ġ,',\n",
       " 'ĠMichigan',\n",
       " 'Ġ.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17bbd0f6-02cb-4396-a563-05372086babf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \tlabel_index=0 \tlabel=0 \tnew_word=False \tword_id=0 \ttoken=<s>\n",
      "i=1 \tlabel_index=1 \tlabel=0 \tnew_word=True \tword_id=1 \ttoken=ĠKnown\n",
      "i=2 \tlabel_index=2 \tlabel=0 \tnew_word=True \tword_id=2 \ttoken=Ġlocally\n",
      "i=3 \tlabel_index=3 \tlabel=0 \tnew_word=True \tword_id=3 \ttoken=Ġas\n",
      "i=4 \tlabel_index=4 \tlabel=0 \tnew_word=True \tword_id=4 \ttoken=Ġ``\n",
      "i=5 \tlabel_index=4 \tlabel=8 \tnew_word=False \tword_id=4 \ttoken=ĠFair\n",
      "i=6 \tlabel_index=5 \tlabel=8 \tnew_word=True \tword_id=5 \ttoken=bottom\n",
      "i=7 \tlabel_index=5 \tlabel=8 \tnew_word=False \tword_id=5 \ttoken=ĠBob\n",
      "i=8 \tlabel_index=6 \tlabel=8 \tnew_word=True \tword_id=6 \ttoken=s\n",
      "i=9 \tlabel_index=7 \tlabel=0 \tnew_word=True \tword_id=7 \ttoken=Ġ``\n",
      "i=10 \tlabel_index=8 \tlabel=0 \tnew_word=True \tword_id=8 \ttoken=Ġit\n",
      "i=11 \tlabel_index=9 \tlabel=0 \tnew_word=True \tword_id=9 \ttoken=Ġis\n",
      "i=12 \tlabel_index=10 \tlabel=0 \tnew_word=True \tword_id=10 \ttoken=Ġnow\n",
      "i=13 \tlabel_index=11 \tlabel=0 \tnew_word=True \tword_id=11 \ttoken=Ġpreserved\n",
      "i=14 \tlabel_index=12 \tlabel=0 \tnew_word=True \tword_id=12 \ttoken=Ġat\n",
      "i=15 \tlabel_index=13 \tlabel=0 \tnew_word=True \tword_id=13 \ttoken=Ġthe\n",
      "i=16 \tlabel_index=14 \tlabel=2 \tnew_word=True \tword_id=14 \ttoken=ĠHenry\n",
      "i=17 \tlabel_index=15 \tlabel=2 \tnew_word=True \tword_id=15 \ttoken=ĠFord\n",
      "i=18 \tlabel_index=16 \tlabel=2 \tnew_word=True \tword_id=16 \ttoken=ĠMuseum\n",
      "i=19 \tlabel_index=17 \tlabel=0 \tnew_word=True \tword_id=17 \ttoken=Ġin\n",
      "i=20 \tlabel_index=17 \tlabel=4 \tnew_word=False \tword_id=17 \ttoken=ĠDear\n",
      "i=21 \tlabel_index=18 \tlabel=4 \tnew_word=True \tword_id=18 \ttoken=born\n",
      "i=22 \tlabel_index=19 \tlabel=0 \tnew_word=True \tword_id=19 \ttoken=Ġ,\n",
      "i=23 \tlabel_index=20 \tlabel=4 \tnew_word=True \tword_id=20 \ttoken=ĠMichigan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_index = 0\n",
    "previous_word_id = False\n",
    "matched_labels_partial = []\n",
    "for i in range(len(simple_word_ids)):\n",
    "    word_id = simple_word_ids[i]\n",
    "    label = labels_cp[label_index]\n",
    "    new_word = word_id!=previous_word_id\n",
    "    if new_word:\n",
    "        label_index += 1\n",
    "    matched_labels_partial.append(label)\n",
    "    print(f\"i={i} \\tlabel_index={label_index} \\tlabel={label} \\tnew_word={new_word} \\tword_id={word_id} \\ttoken={tokens[i]}\")\n",
    "    previous_word_id = word_id\n",
    "\n",
    "len(matched_labels_partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b3b5224-a2e6-4328-821f-9fd3b4bbcb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \tlabel_index=0 \tlabel=0 \tnew_word=False \tword_id=0 \ttoken=<s>\n",
      "i=1 \tlabel_index=1 \tlabel=0 \tnew_word=True \tword_id=1 \ttoken=ĠKnown\n",
      "i=2 \tlabel_index=2 \tlabel=0 \tnew_word=True \tword_id=2 \ttoken=Ġlocally\n",
      "i=3 \tlabel_index=3 \tlabel=0 \tnew_word=True \tword_id=3 \ttoken=Ġas\n",
      "i=4 \tlabel_index=4 \tlabel=0 \tnew_word=True \tword_id=4 \ttoken=Ġ``\n",
      "i=5 \tlabel_index=4 \tlabel=8 \tnew_word=False \tword_id=4 \ttoken=ĠFair\n",
      "i=6 \tlabel_index=5 \tlabel=8 \tnew_word=True \tword_id=5 \ttoken=bottom\n",
      "i=7 \tlabel_index=5 \tlabel=8 \tnew_word=False \tword_id=5 \ttoken=ĠBob\n",
      "i=8 \tlabel_index=6 \tlabel=8 \tnew_word=True \tword_id=6 \ttoken=s\n",
      "i=9 \tlabel_index=7 \tlabel=0 \tnew_word=True \tword_id=7 \ttoken=Ġ``\n",
      "i=10 \tlabel_index=8 \tlabel=0 \tnew_word=True \tword_id=8 \ttoken=Ġit\n",
      "i=11 \tlabel_index=9 \tlabel=0 \tnew_word=True \tword_id=9 \ttoken=Ġis\n",
      "i=12 \tlabel_index=10 \tlabel=0 \tnew_word=True \tword_id=10 \ttoken=Ġnow\n",
      "i=13 \tlabel_index=11 \tlabel=0 \tnew_word=True \tword_id=11 \ttoken=Ġpreserved\n",
      "i=14 \tlabel_index=12 \tlabel=0 \tnew_word=True \tword_id=12 \ttoken=Ġat\n",
      "i=15 \tlabel_index=13 \tlabel=0 \tnew_word=True \tword_id=13 \ttoken=Ġthe\n",
      "i=16 \tlabel_index=14 \tlabel=2 \tnew_word=True \tword_id=14 \ttoken=ĠHenry\n",
      "i=17 \tlabel_index=15 \tlabel=2 \tnew_word=True \tword_id=15 \ttoken=ĠFord\n",
      "i=18 \tlabel_index=16 \tlabel=2 \tnew_word=True \tword_id=16 \ttoken=ĠMuseum\n",
      "i=19 \tlabel_index=17 \tlabel=0 \tnew_word=True \tword_id=17 \ttoken=Ġin\n",
      "i=20 \tlabel_index=17 \tlabel=4 \tnew_word=False \tword_id=17 \ttoken=ĠDear\n",
      "i=21 \tlabel_index=18 \tlabel=4 \tnew_word=True \tword_id=18 \ttoken=born\n",
      "i=22 \tlabel_index=19 \tlabel=0 \tnew_word=True \tword_id=19 \ttoken=Ġ,\n",
      "i=23 \tlabel_index=20 \tlabel=4 \tnew_word=True \tword_id=20 \ttoken=ĠMichigan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_index = 0\n",
    "previous_word_id = False\n",
    "matched_labels = []\n",
    "for i in range(len(simple_word_ids)):\n",
    "    word_id = simple_word_ids[i]\n",
    "    label = labels_cp[label_index]\n",
    "    new_word = word_id!=previous_word_id\n",
    "    if new_word:\n",
    "        label_index += 1\n",
    "    matched_labels.append(label)\n",
    "    print(f\"i={i} \\tlabel_index={label_index} \\tlabel={label} \\tnew_word={new_word} \\tword_id={word_id} \\ttoken={tokens[i]}\")\n",
    "    previous_word_id = word_id\n",
    "\n",
    "len(matched_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c133dc4-d86b-45f9-a623-31a9d6264601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " -100]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_labels_all = [-100] + matched_labels + [-100]\n",
    "matched_labels_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11b65bee-4fe9-46d4-8751-be77069505db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matched_labels_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3eae0e4-8d19-499e-884d-71c5dc72fdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \tlabel_index=0 \tlabel=0 \tnew_word=True \tword_id=None\tprevious_word_id=False \ttoken=<s>\n",
      "i=1 \tlabel_index=1 \tlabel=0 \tnew_word=True \tword_id=0\tprevious_word_id=None \ttoken=ĠKnown\n",
      "i=2 \tlabel_index=2 \tlabel=0 \tnew_word=True \tword_id=1\tprevious_word_id=0 \ttoken=Ġlocally\n",
      "i=3 \tlabel_index=3 \tlabel=0 \tnew_word=True \tword_id=2\tprevious_word_id=1 \ttoken=Ġas\n",
      "i=4 \tlabel_index=4 \tlabel=0 \tnew_word=True \tword_id=3\tprevious_word_id=2 \ttoken=Ġ``\n",
      "i=5 \tlabel_index=5 \tlabel=8 \tnew_word=True \tword_id=4\tprevious_word_id=3 \ttoken=ĠFair\n",
      "i=6 \tlabel_index=5 \tlabel=8 \tnew_word=False \tword_id=4\tprevious_word_id=4 \ttoken=bottom\n",
      "i=7 \tlabel_index=6 \tlabel=8 \tnew_word=True \tword_id=5\tprevious_word_id=4 \ttoken=ĠBob\n",
      "i=8 \tlabel_index=6 \tlabel=0 \tnew_word=False \tword_id=5\tprevious_word_id=5 \ttoken=s\n",
      "i=9 \tlabel_index=7 \tlabel=0 \tnew_word=True \tword_id=6\tprevious_word_id=5 \ttoken=Ġ``\n",
      "i=10 \tlabel_index=8 \tlabel=0 \tnew_word=True \tword_id=7\tprevious_word_id=6 \ttoken=Ġit\n",
      "i=11 \tlabel_index=9 \tlabel=0 \tnew_word=True \tword_id=8\tprevious_word_id=7 \ttoken=Ġis\n",
      "i=12 \tlabel_index=10 \tlabel=0 \tnew_word=True \tword_id=9\tprevious_word_id=8 \ttoken=Ġnow\n",
      "i=13 \tlabel_index=11 \tlabel=0 \tnew_word=True \tword_id=10\tprevious_word_id=9 \ttoken=Ġpreserved\n",
      "i=14 \tlabel_index=12 \tlabel=0 \tnew_word=True \tword_id=11\tprevious_word_id=10 \ttoken=Ġat\n",
      "i=15 \tlabel_index=13 \tlabel=0 \tnew_word=True \tword_id=12\tprevious_word_id=11 \ttoken=Ġthe\n",
      "i=16 \tlabel_index=14 \tlabel=2 \tnew_word=True \tword_id=13\tprevious_word_id=12 \ttoken=ĠHenry\n",
      "i=17 \tlabel_index=15 \tlabel=2 \tnew_word=True \tword_id=14\tprevious_word_id=13 \ttoken=ĠFord\n",
      "i=18 \tlabel_index=16 \tlabel=2 \tnew_word=True \tword_id=15\tprevious_word_id=14 \ttoken=ĠMuseum\n",
      "i=19 \tlabel_index=17 \tlabel=0 \tnew_word=True \tword_id=16\tprevious_word_id=15 \ttoken=Ġin\n",
      "i=20 \tlabel_index=18 \tlabel=4 \tnew_word=True \tword_id=17\tprevious_word_id=16 \ttoken=ĠDear\n",
      "i=21 \tlabel_index=18 \tlabel=0 \tnew_word=False \tword_id=17\tprevious_word_id=17 \ttoken=born\n",
      "i=22 \tlabel_index=19 \tlabel=0 \tnew_word=True \tword_id=18\tprevious_word_id=17 \ttoken=Ġ,\n",
      "i=23 \tlabel_index=20 \tlabel=4 \tnew_word=True \tword_id=19\tprevious_word_id=18 \ttoken=ĠMichigan\n",
      "i=24 \tlabel_index=21 \tlabel=0 \tnew_word=True \tword_id=20\tprevious_word_id=19 \ttoken=Ġ.\n"
     ]
    }
   ],
   "source": [
    "# match labels to word_ids: if word_id repeats, attach previous label once more to labels_matched\n",
    "label_index = -1\n",
    "previous_word_id = False\n",
    "for i in range(len(word_ids)-1):\n",
    "    word_id = word_ids[i]\n",
    "    #word_id = word_id if word_id!=None else -100\n",
    "    label = labels_cp[label_index]\n",
    "    new_word = word_id!=previous_word_id\n",
    "    if new_word:\n",
    "        label_index += 1\n",
    "    print(f\"i={i} \\tlabel_index={label_index} \\tlabel={label} \\tnew_word={new_word} \\tword_id={word_id}\\tprevious_word_id={previous_word_id} \\ttoken={tokens[i]}\")\n",
    "    previous_word_id = word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2f4bc6a-8a9d-4d18-95ef-d63f563f3b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 in (2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7515585d-27eb-4a6b-9c51-0409e54797bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \tlabel_index=0 \tlabel=0 \tboole=True \tword_id=None\tprevious_word_id=False \ttoken=<s>\n",
      "i=1 \tlabel_index=1 \tlabel=0 \tboole=True \tword_id=0\tprevious_word_id=None \ttoken=ĠKnown\n",
      "i=2 \tlabel_index=2 \tlabel=0 \tboole=True \tword_id=1\tprevious_word_id=0 \ttoken=Ġlocally\n",
      "i=3 \tlabel_index=3 \tlabel=0 \tboole=True \tword_id=2\tprevious_word_id=1 \ttoken=Ġas\n",
      "i=4 \tlabel_index=4 \tlabel=0 \tboole=True \tword_id=3\tprevious_word_id=2 \ttoken=Ġ``\n",
      "i=5 \tlabel_index=5 \tlabel=8 \tboole=True \tword_id=4\tprevious_word_id=3 \ttoken=ĠFair\n",
      "i=6 \tlabel_index=5 \tlabel=8 \tboole=False \tword_id=4\tprevious_word_id=4 \ttoken=bottom\n",
      "i=7 \tlabel_index=6 \tlabel=8 \tboole=True \tword_id=5\tprevious_word_id=4 \ttoken=ĠBob\n",
      "i=8 \tlabel_index=6 \tlabel=0 \tboole=False \tword_id=5\tprevious_word_id=5 \ttoken=s\n",
      "i=9 \tlabel_index=7 \tlabel=0 \tboole=True \tword_id=6\tprevious_word_id=5 \ttoken=Ġ``\n",
      "i=10 \tlabel_index=8 \tlabel=0 \tboole=True \tword_id=7\tprevious_word_id=6 \ttoken=Ġit\n",
      "i=11 \tlabel_index=9 \tlabel=0 \tboole=True \tword_id=8\tprevious_word_id=7 \ttoken=Ġis\n",
      "i=12 \tlabel_index=10 \tlabel=0 \tboole=True \tword_id=9\tprevious_word_id=8 \ttoken=Ġnow\n",
      "i=13 \tlabel_index=11 \tlabel=0 \tboole=True \tword_id=10\tprevious_word_id=9 \ttoken=Ġpreserved\n",
      "i=14 \tlabel_index=12 \tlabel=0 \tboole=True \tword_id=11\tprevious_word_id=10 \ttoken=Ġat\n",
      "i=15 \tlabel_index=13 \tlabel=0 \tboole=True \tword_id=12\tprevious_word_id=11 \ttoken=Ġthe\n",
      "i=16 \tlabel_index=14 \tlabel=2 \tboole=True \tword_id=13\tprevious_word_id=12 \ttoken=ĠHenry\n",
      "i=17 \tlabel_index=15 \tlabel=2 \tboole=True \tword_id=14\tprevious_word_id=13 \ttoken=ĠFord\n",
      "i=18 \tlabel_index=16 \tlabel=2 \tboole=True \tword_id=15\tprevious_word_id=14 \ttoken=ĠMuseum\n",
      "i=19 \tlabel_index=17 \tlabel=0 \tboole=True \tword_id=16\tprevious_word_id=15 \ttoken=Ġin\n",
      "i=20 \tlabel_index=18 \tlabel=4 \tboole=True \tword_id=17\tprevious_word_id=16 \ttoken=ĠDear\n",
      "i=21 \tlabel_index=18 \tlabel=0 \tboole=False \tword_id=17\tprevious_word_id=17 \ttoken=born\n",
      "i=22 \tlabel_index=19 \tlabel=0 \tboole=True \tword_id=18\tprevious_word_id=17 \ttoken=Ġ,\n",
      "i=23 \tlabel_index=20 \tlabel=4 \tboole=True \tword_id=19\tprevious_word_id=18 \ttoken=ĠMichigan\n",
      "i=24 \tlabel_index=21 \tlabel=0 \tboole=True \tword_id=20\tprevious_word_id=19 \ttoken=Ġ.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word_ids)):\n\u001b[1;32m      5\u001b[0m     word_id \u001b[38;5;241m=\u001b[39m word_ids[i]\n\u001b[0;32m----> 6\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[43mlabels_cp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m     boole \u001b[38;5;241m=\u001b[39m word_id\u001b[38;5;241m!=\u001b[39mprevious_word_id \u001b[38;5;129;01mor\u001b[39;00m tokens[i]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m boole:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# match labels to word_ids: if word_id repeats, attach previous label once more to labels_matched\n",
    "label_index = -1\n",
    "previous_word_id = False\n",
    "for i in range(len(word_ids)):\n",
    "    word_id = word_ids[i]\n",
    "    label = labels_cp[label_index]\n",
    "    boole = word_id!=previous_word_id or tokens[i]==\"</s>\"\n",
    "    if boole:\n",
    "        label_index += 1\n",
    "    print(f\"i={i} \\tlabel_index={label_index} \\tlabel={label} \\tboole={boole} \\tword_id={word_id}\\tprevious_word_id={previous_word_id} \\ttoken={tokens[i]}\")\n",
    "    previous_word_id = word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91dee706-69d9-4a54-8c2c-12ece1d75e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \tlabel_index=0 \tlabel=0 \tboole=True \tword_id=None\tprevious_word_id=False \ttoken=<s>\n",
      "i=1 \tlabel_index=1 \tlabel=0 \tboole=True \tword_id=0\tprevious_word_id=None \ttoken=ĠKnown\n",
      "i=2 \tlabel_index=2 \tlabel=0 \tboole=True \tword_id=1\tprevious_word_id=0 \ttoken=Ġlocally\n",
      "i=3 \tlabel_index=3 \tlabel=0 \tboole=True \tword_id=2\tprevious_word_id=1 \ttoken=Ġas\n",
      "i=4 \tlabel_index=4 \tlabel=0 \tboole=True \tword_id=3\tprevious_word_id=2 \ttoken=Ġ``\n",
      "i=5 \tlabel_index=5 \tlabel=8 \tboole=True \tword_id=4\tprevious_word_id=3 \ttoken=ĠFair\n",
      "i=6 \tlabel_index=5 \tlabel=8 \tboole=False \tword_id=4\tprevious_word_id=4 \ttoken=bottom\n",
      "i=7 \tlabel_index=6 \tlabel=8 \tboole=True \tword_id=5\tprevious_word_id=4 \ttoken=ĠBob\n",
      "i=8 \tlabel_index=6 \tlabel=0 \tboole=False \tword_id=5\tprevious_word_id=5 \ttoken=s\n",
      "i=9 \tlabel_index=7 \tlabel=0 \tboole=True \tword_id=6\tprevious_word_id=5 \ttoken=Ġ``\n",
      "i=10 \tlabel_index=8 \tlabel=0 \tboole=True \tword_id=7\tprevious_word_id=6 \ttoken=Ġit\n",
      "i=11 \tlabel_index=9 \tlabel=0 \tboole=True \tword_id=8\tprevious_word_id=7 \ttoken=Ġis\n",
      "i=12 \tlabel_index=10 \tlabel=0 \tboole=True \tword_id=9\tprevious_word_id=8 \ttoken=Ġnow\n",
      "i=13 \tlabel_index=11 \tlabel=0 \tboole=True \tword_id=10\tprevious_word_id=9 \ttoken=Ġpreserved\n",
      "i=14 \tlabel_index=12 \tlabel=0 \tboole=True \tword_id=11\tprevious_word_id=10 \ttoken=Ġat\n",
      "i=15 \tlabel_index=13 \tlabel=0 \tboole=True \tword_id=12\tprevious_word_id=11 \ttoken=Ġthe\n",
      "i=16 \tlabel_index=14 \tlabel=2 \tboole=True \tword_id=13\tprevious_word_id=12 \ttoken=ĠHenry\n",
      "i=17 \tlabel_index=15 \tlabel=2 \tboole=True \tword_id=14\tprevious_word_id=13 \ttoken=ĠFord\n",
      "i=18 \tlabel_index=16 \tlabel=2 \tboole=True \tword_id=15\tprevious_word_id=14 \ttoken=ĠMuseum\n",
      "i=19 \tlabel_index=17 \tlabel=0 \tboole=True \tword_id=16\tprevious_word_id=15 \ttoken=Ġin\n",
      "i=20 \tlabel_index=18 \tlabel=4 \tboole=True \tword_id=17\tprevious_word_id=16 \ttoken=ĠDear\n",
      "i=21 \tlabel_index=18 \tlabel=0 \tboole=False \tword_id=17\tprevious_word_id=17 \ttoken=born\n",
      "i=22 \tlabel_index=19 \tlabel=0 \tboole=True \tword_id=18\tprevious_word_id=17 \ttoken=Ġ,\n",
      "i=23 \tlabel_index=20 \tlabel=4 \tboole=True \tword_id=19\tprevious_word_id=18 \ttoken=ĠMichigan\n",
      "i=24 \tlabel_index=21 \tlabel=0 \tboole=True \tword_id=20\tprevious_word_id=19 \ttoken=Ġ.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word_ids)):\n\u001b[1;32m      5\u001b[0m     word_id \u001b[38;5;241m=\u001b[39m word_ids[i]\n\u001b[0;32m----> 6\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[43mlabels_cp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m     boole \u001b[38;5;241m=\u001b[39m word_id\u001b[38;5;241m!=\u001b[39mprevious_word_id \u001b[38;5;129;01mor\u001b[39;00m tokens[i]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m boole:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# match labels to word_ids: if word_id repeats, attach previous label once more to labels_matched\n",
    "label_index = -1\n",
    "previous_word_id = False\n",
    "for i in range(len(word_ids)):\n",
    "    word_id = word_ids[i]\n",
    "    label = labels_cp[label_index]\n",
    "    boole = word_id!=previous_word_id or tokens[i]==\"</s>\"\n",
    "    if boole:\n",
    "        label_index += 1\n",
    "    print(f\"i={i} \\tlabel_index={label_index} \\tlabel={label} \\tboole={boole} \\tword_id={word_id}\\tprevious_word_id={previous_word_id} \\ttoken={tokens[i]}\")\n",
    "    previous_word_id = word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1678c26f-2dfc-4149-96e7-d4b7ff5a38e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \tlabel_index=0 \tlabel=0 \tword_id=None \ttoken=<s>\n",
      "i=1 \tlabel_index=1 \tlabel=0 \tword_id=0 \ttoken=ĠKnown\n",
      "i=2 \tlabel_index=2 \tlabel=0 \tword_id=1 \ttoken=Ġlocally\n",
      "i=3 \tlabel_index=3 \tlabel=0 \tword_id=2 \ttoken=Ġas\n",
      "i=4 \tlabel_index=4 \tlabel=8 \tword_id=3 \ttoken=Ġ``\n",
      "i=5 \tlabel_index=5 \tlabel=8 \tword_id=4 \ttoken=ĠFair\n",
      "i=6 \tlabel_index=5 \tlabel=8 \tword_id=4 \ttoken=bottom\n",
      "i=7 \tlabel_index=6 \tlabel=0 \tword_id=5 \ttoken=ĠBob\n",
      "i=8 \tlabel_index=6 \tlabel=0 \tword_id=5 \ttoken=s\n",
      "i=9 \tlabel_index=7 \tlabel=0 \tword_id=6 \ttoken=Ġ``\n",
      "i=10 \tlabel_index=8 \tlabel=0 \tword_id=7 \ttoken=Ġit\n",
      "i=11 \tlabel_index=9 \tlabel=0 \tword_id=8 \ttoken=Ġis\n",
      "i=12 \tlabel_index=10 \tlabel=0 \tword_id=9 \ttoken=Ġnow\n",
      "i=13 \tlabel_index=11 \tlabel=0 \tword_id=10 \ttoken=Ġpreserved\n",
      "i=14 \tlabel_index=12 \tlabel=0 \tword_id=11 \ttoken=Ġat\n",
      "i=15 \tlabel_index=13 \tlabel=2 \tword_id=12 \ttoken=Ġthe\n",
      "i=16 \tlabel_index=14 \tlabel=2 \tword_id=13 \ttoken=ĠHenry\n",
      "i=17 \tlabel_index=15 \tlabel=2 \tword_id=14 \ttoken=ĠFord\n",
      "i=18 \tlabel_index=16 \tlabel=0 \tword_id=15 \ttoken=ĠMuseum\n",
      "i=19 \tlabel_index=17 \tlabel=4 \tword_id=16 \ttoken=Ġin\n",
      "i=20 \tlabel_index=18 \tlabel=0 \tword_id=17 \ttoken=ĠDear\n",
      "i=21 \tlabel_index=18 \tlabel=0 \tword_id=17 \ttoken=born\n",
      "i=22 \tlabel_index=19 \tlabel=4 \tword_id=18 \ttoken=Ġ,\n",
      "i=23 \tlabel_index=20 \tlabel=0 \tword_id=19 \ttoken=ĠMichigan\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word_id\u001b[38;5;241m!=\u001b[39mprevious_word_id:\u001b[38;5;66;03m# or tokens[i]==\"</s>\":\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     label_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 10\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mlabels_cp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mlabel_index=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mlabel=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mword_id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mtoken=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m previous_word_id \u001b[38;5;241m=\u001b[39m word_id\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# use this! keep it unchanged and edit a copy instead! handle </s> via dedicated code!\n",
    "\n",
    "# match labels to word_ids: if word_id repeats, attach previous label once more to labels_matched\n",
    "label_index = -1\n",
    "previous_word_id = False\n",
    "for i in range(len(word_ids[:-1])):\n",
    "    word_id = word_ids[i]\n",
    "    if word_id!=previous_word_id:# or tokens[i]==\"</s>\":\n",
    "        label_index += 1\n",
    "    label = labels_cp[label_index]\n",
    "    print(f\"i={i} \\tlabel_index={label_index} \\tlabel={label} \\tword_id={word_id} \\ttoken={tokens[i]}\")\n",
    "    previous_word_id = word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1683cbd3-4850-452b-bc93-227533b5d43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\tword_id: None\tpolicy: update label_index\tlabel: -100\ttoken: <s>\n",
      "i: 1\tword_id: 0\tpolicy: update label_index\tlabel: 0\ttoken: ĠKnown\n",
      "i: 2\tword_id: 1\tpolicy: update label_index\tlabel: 0\ttoken: Ġlocally\n",
      "i: 3\tword_id: 2\tpolicy: update label_index\tlabel: 8\ttoken: Ġas\n",
      "i: 4\tword_id: 3\tpolicy: update label_index\tlabel: 8\ttoken: Ġ``\n",
      "i: 5\tword_id: 4\tpolicy: update label_index\tlabel: 0\ttoken: ĠFair\n",
      "i: 6\tword_id: 4\tpolicy: reuse label_index\tlabel: 0\ttoken: bottom\n",
      "i: 7\tword_id: 5\tpolicy: update label_index\tlabel: 0\ttoken: ĠBob\n",
      "i: 8\tword_id: 5\tpolicy: reuse label_index\tlabel: 0\ttoken: s\n",
      "i: 9\tword_id: 6\tpolicy: update label_index\tlabel: 0\ttoken: Ġ``\n",
      "i: 10\tword_id: 7\tpolicy: update label_index\tlabel: 0\ttoken: Ġit\n",
      "i: 11\tword_id: 8\tpolicy: update label_index\tlabel: 0\ttoken: Ġis\n",
      "i: 12\tword_id: 9\tpolicy: update label_index\tlabel: 0\ttoken: Ġnow\n",
      "i: 13\tword_id: 10\tpolicy: update label_index\tlabel: 0\ttoken: Ġpreserved\n",
      "i: 14\tword_id: 11\tpolicy: update label_index\tlabel: 2\ttoken: Ġat\n",
      "i: 15\tword_id: 12\tpolicy: update label_index\tlabel: 2\ttoken: Ġthe\n",
      "i: 16\tword_id: 13\tpolicy: update label_index\tlabel: 2\ttoken: ĠHenry\n",
      "i: 17\tword_id: 14\tpolicy: update label_index\tlabel: 0\ttoken: ĠFord\n",
      "i: 18\tword_id: 15\tpolicy: update label_index\tlabel: 4\ttoken: ĠMuseum\n",
      "i: 19\tword_id: 16\tpolicy: update label_index\tlabel: 0\ttoken: Ġin\n",
      "i: 20\tword_id: 17\tpolicy: update label_index\tlabel: 4\ttoken: ĠDear\n",
      "i: 21\tword_id: 17\tpolicy: reuse label_index\tlabel: 4\ttoken: born\n",
      "i: 22\tword_id: 18\tpolicy: update label_index\tlabel: 0\ttoken: Ġ,\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate label_index\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m     label_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 11\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word_id\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "previous_word_id = False\n",
    "label_index = 0\n",
    "for i in range(len(word_ids)):\n",
    "    word_id = word_ids[i]\n",
    "    if word_id==previous_word_id:\n",
    "        policy = \"reuse label_index\"\n",
    "        label = labels[label_index]\n",
    "    else:\n",
    "        policy = \"update label_index\"\n",
    "        label_index += 1\n",
    "        label = labels[label_index]\n",
    "    if word_id==None:\n",
    "        label=-100\n",
    "    print(f\"i: {i}\\tword_id: {word_id}\\tpolicy: {policy}\\tlabel: {label}\\ttoken: {tokens[i]}\")\n",
    "    previous_word_id = word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48bcfb21-957e-4a54-b35a-c0809d9d46e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 4, 0, 4, 0], 21)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5bab407-f108-4a77-a7d2-52c0a14b9674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids[25]==None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7f3131e-d532-43e3-97db-29610637ca3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1], [0, 1, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "mylist = [0, 1]\n",
    "mylist2 = copy.copy(mylist)\n",
    "mylist2.append(2)\n",
    "mylist, mylist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aa9d0e85-d7ba-4892-aef3-15582a1cb02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 21\n",
      "22 21\n"
     ]
    }
   ],
   "source": [
    "labels_cp = copy.copy(labels)\n",
    "print(len(labels_cp), len(labels))\n",
    "labels_cp.append(0)\n",
    "print(len(labels_cp), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61c2fd25-53e9-43d6-a6dd-d76342c48eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \tlabel_index=0 \tlabel=0 \tword_id=None \ttoken=<s>\n",
      "i=1 \tlabel_index=1 \tlabel=0 \tword_id=0 \ttoken=ĠKnown\n",
      "i=2 \tlabel_index=2 \tlabel=0 \tword_id=1 \ttoken=Ġlocally\n",
      "i=3 \tlabel_index=3 \tlabel=0 \tword_id=2 \ttoken=Ġas\n",
      "i=4 \tlabel_index=4 \tlabel=8 \tword_id=3 \ttoken=Ġ``\n",
      "i=5 \tlabel_index=5 \tlabel=8 \tword_id=4 \ttoken=ĠFair\n",
      "i=6 \tlabel_index=5 \tlabel=8 \tword_id=4 \ttoken=bottom\n",
      "i=7 \tlabel_index=6 \tlabel=0 \tword_id=5 \ttoken=ĠBob\n",
      "i=8 \tlabel_index=6 \tlabel=0 \tword_id=5 \ttoken=s\n",
      "i=9 \tlabel_index=7 \tlabel=0 \tword_id=6 \ttoken=Ġ``\n",
      "i=10 \tlabel_index=8 \tlabel=0 \tword_id=7 \ttoken=Ġit\n",
      "i=11 \tlabel_index=9 \tlabel=0 \tword_id=8 \ttoken=Ġis\n",
      "i=12 \tlabel_index=10 \tlabel=0 \tword_id=9 \ttoken=Ġnow\n",
      "i=13 \tlabel_index=11 \tlabel=0 \tword_id=10 \ttoken=Ġpreserved\n",
      "i=14 \tlabel_index=12 \tlabel=0 \tword_id=11 \ttoken=Ġat\n",
      "i=15 \tlabel_index=13 \tlabel=2 \tword_id=12 \ttoken=Ġthe\n",
      "i=16 \tlabel_index=14 \tlabel=2 \tword_id=13 \ttoken=ĠHenry\n",
      "i=17 \tlabel_index=15 \tlabel=2 \tword_id=14 \ttoken=ĠFord\n",
      "i=18 \tlabel_index=16 \tlabel=0 \tword_id=15 \ttoken=ĠMuseum\n",
      "i=19 \tlabel_index=17 \tlabel=4 \tword_id=16 \ttoken=Ġin\n",
      "i=20 \tlabel_index=18 \tlabel=0 \tword_id=17 \ttoken=ĠDear\n",
      "i=21 \tlabel_index=18 \tlabel=0 \tword_id=17 \ttoken=born\n",
      "i=22 \tlabel_index=19 \tlabel=4 \tword_id=18 \ttoken=Ġ,\n",
      "i=23 \tlabel_index=20 \tlabel=0 \tword_id=19 \ttoken=ĠMichigan\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word_id\u001b[38;5;241m!=\u001b[39mprevious_word_id:\u001b[38;5;66;03m# or tokens[i]==\"</s>\":\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     label_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 10\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mlabels_cp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mlabel_index=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mlabel=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mword_id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mtoken=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m previous_word_id \u001b[38;5;241m=\u001b[39m word_id\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# use this! keep it unchanged and edit a copy instead! handle </s> via dedicated code!\n",
    "\n",
    "# match labels to word_ids: if word_id repeats, attach previous label once more to labels_matched\n",
    "label_index = -1\n",
    "previous_word_id = False\n",
    "for i in range(len(word_ids[:-1])):\n",
    "    word_id = word_ids[i]\n",
    "    if word_id!=previous_word_id:# or tokens[i]==\"</s>\":\n",
    "        label_index += 1\n",
    "    label = labels_cp[label_index]\n",
    "    print(f\"i={i} \\tlabel_index={label_index} \\tlabel={label} \\tword_id={word_id} \\ttoken={tokens[i]}\")\n",
    "    previous_word_id = word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81add41c-9d16-4126-97ed-e86d406e0958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \tlabel_index=0 \tlabel=0 \tboole=True \tword_id=None\tprevious_word_id=False \ttoken=<s>\n",
      "i=1 \tlabel_index=1 \tlabel=0 \tboole=True \tword_id=0\tprevious_word_id=None \ttoken=ĠKnown\n",
      "i=2 \tlabel_index=2 \tlabel=0 \tboole=True \tword_id=1\tprevious_word_id=0 \ttoken=Ġlocally\n",
      "i=3 \tlabel_index=3 \tlabel=0 \tboole=True \tword_id=2\tprevious_word_id=1 \ttoken=Ġas\n",
      "i=4 \tlabel_index=4 \tlabel=0 \tboole=True \tword_id=3\tprevious_word_id=2 \ttoken=Ġ``\n",
      "i=5 \tlabel_index=5 \tlabel=8 \tboole=True \tword_id=4\tprevious_word_id=3 \ttoken=ĠFair\n",
      "i=6 \tlabel_index=5 \tlabel=8 \tboole=False \tword_id=4\tprevious_word_id=4 \ttoken=bottom\n",
      "i=7 \tlabel_index=6 \tlabel=8 \tboole=True \tword_id=5\tprevious_word_id=4 \ttoken=ĠBob\n",
      "i=8 \tlabel_index=6 \tlabel=0 \tboole=False \tword_id=5\tprevious_word_id=5 \ttoken=s\n",
      "i=9 \tlabel_index=7 \tlabel=0 \tboole=True \tword_id=6\tprevious_word_id=5 \ttoken=Ġ``\n",
      "i=10 \tlabel_index=8 \tlabel=0 \tboole=True \tword_id=7\tprevious_word_id=6 \ttoken=Ġit\n",
      "i=11 \tlabel_index=9 \tlabel=0 \tboole=True \tword_id=8\tprevious_word_id=7 \ttoken=Ġis\n",
      "i=12 \tlabel_index=10 \tlabel=0 \tboole=True \tword_id=9\tprevious_word_id=8 \ttoken=Ġnow\n",
      "i=13 \tlabel_index=11 \tlabel=0 \tboole=True \tword_id=10\tprevious_word_id=9 \ttoken=Ġpreserved\n",
      "i=14 \tlabel_index=12 \tlabel=0 \tboole=True \tword_id=11\tprevious_word_id=10 \ttoken=Ġat\n",
      "i=15 \tlabel_index=13 \tlabel=0 \tboole=True \tword_id=12\tprevious_word_id=11 \ttoken=Ġthe\n",
      "i=16 \tlabel_index=14 \tlabel=2 \tboole=True \tword_id=13\tprevious_word_id=12 \ttoken=ĠHenry\n",
      "i=17 \tlabel_index=15 \tlabel=2 \tboole=True \tword_id=14\tprevious_word_id=13 \ttoken=ĠFord\n",
      "i=18 \tlabel_index=16 \tlabel=2 \tboole=True \tword_id=15\tprevious_word_id=14 \ttoken=ĠMuseum\n",
      "i=19 \tlabel_index=17 \tlabel=0 \tboole=True \tword_id=16\tprevious_word_id=15 \ttoken=Ġin\n",
      "i=20 \tlabel_index=18 \tlabel=4 \tboole=True \tword_id=17\tprevious_word_id=16 \ttoken=ĠDear\n",
      "i=21 \tlabel_index=18 \tlabel=0 \tboole=False \tword_id=17\tprevious_word_id=17 \ttoken=born\n",
      "i=22 \tlabel_index=19 \tlabel=0 \tboole=True \tword_id=18\tprevious_word_id=17 \ttoken=Ġ,\n",
      "i=23 \tlabel_index=20 \tlabel=4 \tboole=True \tword_id=19\tprevious_word_id=18 \ttoken=ĠMichigan\n",
      "i=24 \tlabel_index=21 \tlabel=0 \tboole=True \tword_id=20\tprevious_word_id=19 \ttoken=Ġ.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word_ids)):\n\u001b[1;32m      5\u001b[0m     word_id \u001b[38;5;241m=\u001b[39m word_ids[i]\n\u001b[0;32m----> 6\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[43mlabels_cp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m     boole \u001b[38;5;241m=\u001b[39m word_id\u001b[38;5;241m!=\u001b[39mprevious_word_id \u001b[38;5;129;01mor\u001b[39;00m tokens[i]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m boole:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# match labels to word_ids: if word_id repeats, attach previous label once more to labels_matched\n",
    "label_index = -1\n",
    "previous_word_id = False\n",
    "for i in range(len(word_ids)):\n",
    "    word_id = word_ids[i]\n",
    "    label = labels_cp[label_index]\n",
    "    boole = word_id!=previous_word_id or tokens[i]==\"</s>\"\n",
    "    if boole:\n",
    "        label_index += 1\n",
    "    print(f\"i={i} \\tlabel_index={label_index} \\tlabel={label} \\tboole={boole} \\tword_id={word_id}\\tprevious_word_id={previous_word_id} \\ttoken={tokens[i]}\")\n",
    "    previous_word_id = word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7997eb99-bc6a-49a5-aa59-ebab3d4db8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 21, 26, 26, True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words), len(labels), len(tokens), len(word_ids), word_ids[0]==None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f9d4d3cd-1e11-41fa-bde3-2793f28477fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0\tword_id=None\trepeated_word_id: False\n",
      "i=1\tword_id=0\trepeated_word_id: False\n",
      "i=2\tword_id=1\trepeated_word_id: False\n",
      "i=3\tword_id=2\trepeated_word_id: False\n",
      "i=4\tword_id=3\trepeated_word_id: False\n",
      "i=5\tword_id=4\trepeated_word_id: False\n",
      "i=6\tword_id=4\trepeated_word_id: True\n",
      "i=7\tword_id=5\trepeated_word_id: False\n",
      "i=8\tword_id=5\trepeated_word_id: True\n",
      "i=9\tword_id=6\trepeated_word_id: False\n",
      "i=10\tword_id=7\trepeated_word_id: False\n",
      "i=11\tword_id=8\trepeated_word_id: False\n",
      "i=12\tword_id=9\trepeated_word_id: False\n",
      "i=13\tword_id=10\trepeated_word_id: False\n",
      "i=14\tword_id=11\trepeated_word_id: False\n",
      "i=15\tword_id=12\trepeated_word_id: False\n",
      "i=16\tword_id=13\trepeated_word_id: False\n",
      "i=17\tword_id=14\trepeated_word_id: False\n",
      "i=18\tword_id=15\trepeated_word_id: False\n",
      "i=19\tword_id=16\trepeated_word_id: False\n",
      "i=20\tword_id=17\trepeated_word_id: False\n",
      "i=21\tword_id=17\trepeated_word_id: True\n",
      "i=22\tword_id=18\trepeated_word_id: False\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     label_index\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 10\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mword_id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mrepeated_word_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepeated_word_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m previous_word_id \u001b[38;5;241m=\u001b[39m word_id\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "previous_word_id = False\n",
    "label_index = 0\n",
    "for i in range(len(word_ids)):\n",
    "    word_id = word_ids[i]\n",
    "    repeated_word_id = (word_id==previous_word_id)\n",
    "    if repeated_word_id:\n",
    "        label = labels[label_index]\n",
    "    else:\n",
    "        label_index+=1\n",
    "        label = labels[label_index]\n",
    "        \n",
    "    print(f\"i={i}\\tword_id={word_id}\\trepeated_word_id: {repeated_word_id}\")\n",
    "    previous_word_id = word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a2557f0d-6286-4247-a2a5-bde45ae30309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0\tword_id=None\trepeated_word_id: False\n",
      "i=1\tword_id=0\trepeated_word_id: False\n",
      "i=2\tword_id=1\trepeated_word_id: False\n",
      "i=3\tword_id=2\trepeated_word_id: False\n",
      "i=4\tword_id=3\trepeated_word_id: False\n",
      "i=5\tword_id=4\trepeated_word_id: False\n",
      "i=6\tword_id=4\trepeated_word_id: True\n",
      "i=7\tword_id=5\trepeated_word_id: False\n",
      "i=8\tword_id=5\trepeated_word_id: True\n",
      "i=9\tword_id=6\trepeated_word_id: False\n",
      "i=10\tword_id=7\trepeated_word_id: False\n",
      "i=11\tword_id=8\trepeated_word_id: False\n",
      "i=12\tword_id=9\trepeated_word_id: False\n",
      "i=13\tword_id=10\trepeated_word_id: False\n",
      "i=14\tword_id=11\trepeated_word_id: False\n",
      "i=15\tword_id=12\trepeated_word_id: False\n",
      "i=16\tword_id=13\trepeated_word_id: False\n",
      "i=17\tword_id=14\trepeated_word_id: False\n",
      "i=18\tword_id=15\trepeated_word_id: False\n",
      "i=19\tword_id=16\trepeated_word_id: False\n",
      "i=20\tword_id=17\trepeated_word_id: False\n",
      "i=21\tword_id=17\trepeated_word_id: True\n",
      "i=22\tword_id=18\trepeated_word_id: False\n",
      "i=23\tword_id=19\trepeated_word_id: False\n",
      "i=24\tword_id=20\trepeated_word_id: False\n",
      "i=25\tword_id=None\trepeated_word_id: False\n"
     ]
    }
   ],
   "source": [
    "previous_word_id = False\n",
    "label_index = 0\n",
    "for i in range(len(word_ids)):\n",
    "    word_id = word_ids[i]\n",
    "    repeated_word_id = (word_id==previous_word_id)\n",
    "    if repeated_word_id:\n",
    "        label = labels[label_index]\n",
    "    else:\n",
    "        label_index+=1\n",
    "        \n",
    "    print(f\"i={i}\\tword_id={word_id}\\trepeated_word_id: {repeated_word_id}\")\n",
    "    previous_word_id = word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c933384b-3bd2-4025-9c97-6555c190a8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Dearborn', 4)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 17\n",
    "words[i], labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0930f603-0175-47fc-8bb5-69aedf064c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 26)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_ids), len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24e162a8-c4ab-4d8b-a6d6-042f49561704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 'born')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 21\n",
    "word_ids[i], tokens[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc83ce12-bdfc-44b1-b516-24922ff7d9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Known',\n",
       " 'locally',\n",
       " 'as',\n",
       " '``',\n",
       " 'Fairbottom',\n",
       " 'Bobs',\n",
       " '``',\n",
       " 'it',\n",
       " 'is',\n",
       " 'now',\n",
       " 'preserved',\n",
       " 'at',\n",
       " 'the',\n",
       " 'Henry',\n",
       " 'Ford',\n",
       " 'Museum',\n",
       " 'in',\n",
       " 'Dearborn',\n",
       " ',',\n",
       " 'Michigan',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(words))\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "005c01e2-486f-461a-85b2-47f1ccf2dd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 4, 0, 4, 0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(labels))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a7ab0a-3324-4f9b-9355-5346945b2782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match labels to word_ids: if word_id repeats, attach previous label once more to labels_matched\n",
    "####labels_matched = []\n",
    "label_index = -1\n",
    "previous_word_id = False\n",
    "for i in range(len(word_ids)):\n",
    "    word_id = word_ids[i]\n",
    "    ## handle \"None\" labels\n",
    "    ####if word_id==None:\n",
    "    ####    labels_matched.append(-100)\n",
    "    # handle all other labels\n",
    "    label = labels_cp[label_index]\n",
    "    ## handle repeating word_ids (word split into multiple tokens)\n",
    "    ####if word_id==previous_word_id and tokens[i]!=\"</s>\":\n",
    "    ####    labels_matched.append(previous_label)\n",
    "    ## handle new labels\n",
    "    #else:\n",
    "    if word_id!=previous_word_id or tokens[i]==\"</s>\":\n",
    "        #labels_matched.append(label)\n",
    "        label_index += 1\n",
    "    # output and updates\n",
    "    print(f\"i={i} \\tlabel_index={label_index} \\tlabel={label} \\tword_id={word_id} \\ttoken={tokens[i]}\")\n",
    "    previous_word_id = word_id\n",
    "    previous_label = label\n",
    "    \n",
    "#\n",
    "#labels_matched = labels_matched#[:-1]\n",
    "#assert len(labels_matched)==len(tokens)\n",
    "#labels_matched = labels_matched[:-1]\n",
    "#labels_matched, len(labels_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5724c29f-40ca-4260-b300-0d4d035c60ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \tlabel_index=0 \tlabel=0 \tword_id=None \ttoken=<s>\n",
      "i=1 \tlabel_index=1 \tlabel=0 \tword_id=0 \ttoken=ĠKnown\n",
      "i=2 \tlabel_index=2 \tlabel=0 \tword_id=1 \ttoken=Ġlocally\n",
      "i=3 \tlabel_index=3 \tlabel=0 \tword_id=2 \ttoken=Ġas\n",
      "i=4 \tlabel_index=4 \tlabel=0 \tword_id=3 \ttoken=Ġ``\n",
      "i=5 \tlabel_index=5 \tlabel=8 \tword_id=4 \ttoken=ĠFair\n",
      "i=6 \tlabel_index=5 \tlabel=8 \tword_id=4 \ttoken=bottom\n",
      "i=7 \tlabel_index=6 \tlabel=8 \tword_id=5 \ttoken=ĠBob\n",
      "i=8 \tlabel_index=6 \tlabel=0 \tword_id=5 \ttoken=s\n",
      "i=9 \tlabel_index=7 \tlabel=0 \tword_id=6 \ttoken=Ġ``\n",
      "i=10 \tlabel_index=8 \tlabel=0 \tword_id=7 \ttoken=Ġit\n",
      "i=11 \tlabel_index=9 \tlabel=0 \tword_id=8 \ttoken=Ġis\n",
      "i=12 \tlabel_index=10 \tlabel=0 \tword_id=9 \ttoken=Ġnow\n",
      "i=13 \tlabel_index=11 \tlabel=0 \tword_id=10 \ttoken=Ġpreserved\n",
      "i=14 \tlabel_index=12 \tlabel=0 \tword_id=11 \ttoken=Ġat\n",
      "i=15 \tlabel_index=13 \tlabel=0 \tword_id=12 \ttoken=Ġthe\n",
      "i=16 \tlabel_index=14 \tlabel=2 \tword_id=13 \ttoken=ĠHenry\n",
      "i=17 \tlabel_index=15 \tlabel=2 \tword_id=14 \ttoken=ĠFord\n",
      "i=18 \tlabel_index=16 \tlabel=2 \tword_id=15 \ttoken=ĠMuseum\n",
      "i=19 \tlabel_index=17 \tlabel=0 \tword_id=16 \ttoken=Ġin\n",
      "i=20 \tlabel_index=18 \tlabel=4 \tword_id=17 \ttoken=ĠDear\n",
      "i=21 \tlabel_index=18 \tlabel=0 \tword_id=17 \ttoken=born\n",
      "i=22 \tlabel_index=19 \tlabel=0 \tword_id=18 \ttoken=Ġ,\n",
      "i=23 \tlabel_index=20 \tlabel=4 \tword_id=19 \ttoken=ĠMichigan\n",
      "i=24 \tlabel_index=21 \tlabel=0 \tword_id=20 \ttoken=Ġ.\n",
      "i=25 \tlabel_index=22 \tlabel=0 \tword_id=None \ttoken=</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  -100],\n",
       " 27)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match labels to word_ids: if word_id repeats, attach previous label once more to labels_matched\n",
    "labels_matched = []\n",
    "label_index = -1\n",
    "previous_word_id = False\n",
    "for i in range(len(word_ids)):\n",
    "    word_id = word_ids[i]\n",
    "    ## handle \"None\" labels\n",
    "    if word_id==None:\n",
    "        labels_matched.append(-100)\n",
    "    # handle all other labels\n",
    "    label = labels_cp[label_index]\n",
    "    ## handle repeating word_ids (word split into multiple tokens)\n",
    "    if word_id==previous_word_id and tokens[i]!=\"</s>\":\n",
    "        labels_matched.append(previous_label)\n",
    "    ## handle new labels\n",
    "    else:\n",
    "        labels_matched.append(label)\n",
    "        label_index += 1\n",
    "    # output and updates\n",
    "    print(f\"i={i} \\tlabel_index={label_index} \\tlabel={label} \\tword_id={word_id} \\ttoken={tokens[i]}\")\n",
    "    previous_word_id = word_id\n",
    "    previous_label = label\n",
    "    \n",
    "#\n",
    "#labels_matched = labels_matched#[:-1]\n",
    "#assert len(labels_matched)==len(tokens)\n",
    "labels_matched = labels_matched[:-1]\n",
    "labels_matched, len(labels_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfd0aa0a-408e-4b0c-895a-a3406cc06469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \tlabel_index=0 \tlabel=0 \tword_id=None \ttoken=<s>\n",
      "i=1 \tlabel_index=1 \tlabel=0 \tword_id=0 \ttoken=ĠKnown\n",
      "i=2 \tlabel_index=2 \tlabel=0 \tword_id=1 \ttoken=Ġlocally\n",
      "i=3 \tlabel_index=3 \tlabel=0 \tword_id=2 \ttoken=Ġas\n",
      "i=4 \tlabel_index=4 \tlabel=0 \tword_id=3 \ttoken=Ġ``\n",
      "i=5 \tlabel_index=5 \tlabel=8 \tword_id=4 \ttoken=ĠFair\n",
      "i=6 \tlabel_index=5 \tlabel=8 \tword_id=4 \ttoken=bottom\n",
      "i=7 \tlabel_index=6 \tlabel=8 \tword_id=5 \ttoken=ĠBob\n",
      "i=8 \tlabel_index=6 \tlabel=0 \tword_id=5 \ttoken=s\n",
      "i=9 \tlabel_index=7 \tlabel=0 \tword_id=6 \ttoken=Ġ``\n",
      "i=10 \tlabel_index=8 \tlabel=0 \tword_id=7 \ttoken=Ġit\n",
      "i=11 \tlabel_index=9 \tlabel=0 \tword_id=8 \ttoken=Ġis\n",
      "i=12 \tlabel_index=10 \tlabel=0 \tword_id=9 \ttoken=Ġnow\n",
      "i=13 \tlabel_index=11 \tlabel=0 \tword_id=10 \ttoken=Ġpreserved\n",
      "i=14 \tlabel_index=12 \tlabel=0 \tword_id=11 \ttoken=Ġat\n",
      "i=15 \tlabel_index=13 \tlabel=0 \tword_id=12 \ttoken=Ġthe\n",
      "i=16 \tlabel_index=14 \tlabel=2 \tword_id=13 \ttoken=ĠHenry\n",
      "i=17 \tlabel_index=15 \tlabel=2 \tword_id=14 \ttoken=ĠFord\n",
      "i=18 \tlabel_index=16 \tlabel=2 \tword_id=15 \ttoken=ĠMuseum\n",
      "i=19 \tlabel_index=17 \tlabel=0 \tword_id=16 \ttoken=Ġin\n",
      "i=20 \tlabel_index=18 \tlabel=4 \tword_id=17 \ttoken=ĠDear\n",
      "i=21 \tlabel_index=18 \tlabel=0 \tword_id=17 \ttoken=born\n",
      "i=22 \tlabel_index=19 \tlabel=0 \tword_id=18 \ttoken=Ġ,\n",
      "i=23 \tlabel_index=20 \tlabel=4 \tword_id=19 \ttoken=ĠMichigan\n",
      "i=24 \tlabel_index=21 \tlabel=0 \tword_id=20 \ttoken=Ġ.\n",
      "i=25 \tlabel_index=22 \tlabel=0 \tword_id=None \ttoken=</s>\n"
     ]
    }
   ],
   "source": [
    "# match labels to word_ids: if word_id repeats, attach previous label once more to labels_matched\n",
    "####labels_matched = []\n",
    "label_index = -1\n",
    "previous_word_id = False\n",
    "for i in range(len(word_ids)):\n",
    "    word_id = word_ids[i]\n",
    "    ## handle \"None\" labels\n",
    "    ####if word_id==None:\n",
    "    ####    labels_matched.append(-100)\n",
    "    # handle all other labels\n",
    "    label = labels_cp[label_index]\n",
    "    ## handle repeating word_ids (word split into multiple tokens)\n",
    "    ####if word_id==previous_word_id and tokens[i]!=\"</s>\":\n",
    "    ####    labels_matched.append(previous_label)\n",
    "    ## handle new labels\n",
    "    #else:\n",
    "    if word_id!=previous_word_id or tokens[i]==\"</s>\":\n",
    "        #labels_matched.append(label)\n",
    "        label_index += 1\n",
    "    # output and updates\n",
    "    print(f\"i={i} \\tlabel_index={label_index} \\tlabel={label} \\tword_id={word_id} \\ttoken={tokens[i]}\")\n",
    "    previous_word_id = word_id\n",
    "    previous_label = label\n",
    "    \n",
    "#\n",
    "#labels_matched = labels_matched#[:-1]\n",
    "#assert len(labels_matched)==len(tokens)\n",
    "#labels_matched = labels_matched[:-1]\n",
    "#labels_matched, len(labels_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4775fdf-e995-458c-aca3-129fb2e3fff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29d93666-dd14-4e75-ae26-74d0b71794bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \tlabel_index=0 \tlabel=0 \tword_id=None \ttoken=<s>\n",
      "i=1 \tlabel_index=1 \tlabel=0 \tword_id=0 \ttoken=ĠKnown\n",
      "i=2 \tlabel_index=2 \tlabel=0 \tword_id=1 \ttoken=Ġlocally\n",
      "i=3 \tlabel_index=3 \tlabel=0 \tword_id=2 \ttoken=Ġas\n",
      "i=4 \tlabel_index=4 \tlabel=0 \tword_id=3 \ttoken=Ġ``\n",
      "i=5 \tlabel_index=5 \tlabel=8 \tword_id=4 \ttoken=ĠFair\n",
      "i=6 \tlabel_index=5 \tlabel=8 \tword_id=4 \ttoken=bottom\n",
      "i=7 \tlabel_index=6 \tlabel=8 \tword_id=5 \ttoken=ĠBob\n",
      "i=8 \tlabel_index=6 \tlabel=0 \tword_id=5 \ttoken=s\n",
      "i=9 \tlabel_index=7 \tlabel=0 \tword_id=6 \ttoken=Ġ``\n",
      "i=10 \tlabel_index=8 \tlabel=0 \tword_id=7 \ttoken=Ġit\n",
      "i=11 \tlabel_index=9 \tlabel=0 \tword_id=8 \ttoken=Ġis\n",
      "i=12 \tlabel_index=10 \tlabel=0 \tword_id=9 \ttoken=Ġnow\n",
      "i=13 \tlabel_index=11 \tlabel=0 \tword_id=10 \ttoken=Ġpreserved\n",
      "i=14 \tlabel_index=12 \tlabel=0 \tword_id=11 \ttoken=Ġat\n",
      "i=15 \tlabel_index=13 \tlabel=0 \tword_id=12 \ttoken=Ġthe\n",
      "i=16 \tlabel_index=14 \tlabel=2 \tword_id=13 \ttoken=ĠHenry\n",
      "i=17 \tlabel_index=15 \tlabel=2 \tword_id=14 \ttoken=ĠFord\n",
      "i=18 \tlabel_index=16 \tlabel=2 \tword_id=15 \ttoken=ĠMuseum\n",
      "i=19 \tlabel_index=17 \tlabel=0 \tword_id=16 \ttoken=Ġin\n",
      "i=20 \tlabel_index=18 \tlabel=4 \tword_id=17 \ttoken=ĠDear\n",
      "i=21 \tlabel_index=18 \tlabel=0 \tword_id=17 \ttoken=born\n",
      "i=22 \tlabel_index=19 \tlabel=0 \tword_id=18 \ttoken=Ġ,\n",
      "i=23 \tlabel_index=20 \tlabel=4 \tword_id=19 \ttoken=ĠMichigan\n",
      "i=24 \tlabel_index=21 \tlabel=0 \tword_id=20 \ttoken=Ġ.\n",
      "i=25 \tlabel_index=22 \tlabel=0 \tword_id=None \ttoken=</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " -100,\n",
       " 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match labels to word_ids: if word_id repeats, attach previous label once more to labels_matched\n",
    "labels.append(0)\n",
    "labels_matched = []\n",
    "label_index = -1\n",
    "previous_word_id = False\n",
    "for i in range(len(word_ids)):\n",
    "    word_id = word_ids[i]\n",
    "    ## handle \"None\" labels\n",
    "    if word_id==None:\n",
    "        labels_matched.append(-100)\n",
    "    # handle all other labels\n",
    "    label = labels[label_index]\n",
    "    ## handle repeating word_ids (word split into multiple tokens)\n",
    "    if word_id==previous_word_id and tokens[i]!=\"</s>\":\n",
    "        labels_matched.append(previous_label)\n",
    "    ## handle new labels\n",
    "    else:\n",
    "        labels_matched.append(label)\n",
    "        label_index += 1\n",
    "    # output and updates\n",
    "    print(f\"i={i} \\tlabel_index={label_index} \\tlabel={label} \\tword_id={word_id} \\ttoken={tokens[i]}\")\n",
    "    previous_word_id = word_id\n",
    "    previous_label = label\n",
    "    \n",
    "#\n",
    "#labels_matched = labels_matched#[:-1]\n",
    "#assert len(labels_matched)==len(tokens)\n",
    "labels_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70ce6d9b-c1e3-4d0b-8aef-b0ac0840431e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 26)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_matched), len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba054646-6b92-4216-a818-0f1df0067105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \tlabel_index=0 \tlabel=0 \tword_id=None \ttoken=<s>\n",
      "i=1 \tlabel_index=1 \tlabel=0 \tword_id=0 \ttoken=ĠKnown\n",
      "i=2 \tlabel_index=2 \tlabel=0 \tword_id=1 \ttoken=Ġlocally\n",
      "i=3 \tlabel_index=3 \tlabel=0 \tword_id=2 \ttoken=Ġas\n",
      "i=4 \tlabel_index=4 \tlabel=0 \tword_id=3 \ttoken=Ġ``\n",
      "i=5 \tlabel_index=5 \tlabel=8 \tword_id=4 \ttoken=ĠFair\n",
      "i=6 \tlabel_index=5 \tlabel=8 \tword_id=4 \ttoken=bottom\n",
      "i=7 \tlabel_index=6 \tlabel=8 \tword_id=5 \ttoken=ĠBob\n",
      "i=8 \tlabel_index=6 \tlabel=0 \tword_id=5 \ttoken=s\n",
      "i=9 \tlabel_index=7 \tlabel=0 \tword_id=6 \ttoken=Ġ``\n",
      "i=10 \tlabel_index=8 \tlabel=0 \tword_id=7 \ttoken=Ġit\n",
      "i=11 \tlabel_index=9 \tlabel=0 \tword_id=8 \ttoken=Ġis\n",
      "i=12 \tlabel_index=10 \tlabel=0 \tword_id=9 \ttoken=Ġnow\n",
      "i=13 \tlabel_index=11 \tlabel=0 \tword_id=10 \ttoken=Ġpreserved\n",
      "i=14 \tlabel_index=12 \tlabel=0 \tword_id=11 \ttoken=Ġat\n",
      "i=15 \tlabel_index=13 \tlabel=0 \tword_id=12 \ttoken=Ġthe\n",
      "i=16 \tlabel_index=14 \tlabel=2 \tword_id=13 \ttoken=ĠHenry\n",
      "i=17 \tlabel_index=15 \tlabel=2 \tword_id=14 \ttoken=ĠFord\n",
      "i=18 \tlabel_index=16 \tlabel=2 \tword_id=15 \ttoken=ĠMuseum\n",
      "i=19 \tlabel_index=17 \tlabel=0 \tword_id=16 \ttoken=Ġin\n",
      "i=20 \tlabel_index=18 \tlabel=4 \tword_id=17 \ttoken=ĠDear\n",
      "i=21 \tlabel_index=18 \tlabel=0 \tword_id=17 \ttoken=born\n",
      "i=22 \tlabel_index=19 \tlabel=0 \tword_id=18 \ttoken=Ġ,\n",
      "i=23 \tlabel_index=20 \tlabel=4 \tword_id=19 \ttoken=ĠMichigan\n",
      "i=24 \tlabel_index=21 \tlabel=0 \tword_id=20 \ttoken=Ġ.\n",
      "i=25 \tlabel_index=22 \tlabel=0 \tword_id=None \ttoken=</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " -100]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match labels to word_ids: if word_id repeats, attach previous label once more to labels_matched\n",
    "labels.append(0)\n",
    "labels_matched = []\n",
    "label_index = -1\n",
    "previous_word_id = False\n",
    "for i in range(len(word_ids)):\n",
    "    word_id = word_ids[i]\n",
    "    ## handle \"None\" labels\n",
    "    if word_id==None:\n",
    "        labels_matched.append(-100)\n",
    "    # handle all other labels\n",
    "    label = labels[label_index]\n",
    "    ## handle repeating word_ids (word split into multiple tokens)\n",
    "    if word_id==previous_word_id and tokens[i]!=\"</s>\":\n",
    "        labels_matched.append(previous_label)\n",
    "    ## handle new labels\n",
    "    else:\n",
    "        labels_matched.append(label)\n",
    "        label_index += 1\n",
    "    # output and updates\n",
    "    print(f\"i={i} \\tlabel_index={label_index} \\tlabel={label} \\tword_id={word_id} \\ttoken={tokens[i]}\")\n",
    "    previous_word_id = word_id\n",
    "    previous_label = label\n",
    "    \n",
    "#\n",
    "labels_matched = labels_matched[:-1]\n",
    "labels_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00249475-ff44-4204-9716-d5cc712f5f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40d94848-4d4a-4bce-8ddc-c84f9d81da8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \tlabel_index=0 \tlabel=0 \tword_id=None \ttoken=<s>\n",
      "i=1 \tlabel_index=1 \tlabel=0 \tword_id=0 \ttoken=ĠKnown\n",
      "i=2 \tlabel_index=2 \tlabel=0 \tword_id=1 \ttoken=Ġlocally\n",
      "i=3 \tlabel_index=3 \tlabel=0 \tword_id=2 \ttoken=Ġas\n",
      "i=4 \tlabel_index=4 \tlabel=0 \tword_id=3 \ttoken=Ġ``\n",
      "i=5 \tlabel_index=5 \tlabel=8 \tword_id=4 \ttoken=ĠFair\n",
      "i=6 \tlabel_index=5 \tlabel=8 \tword_id=4 \ttoken=bottom\n",
      "i=7 \tlabel_index=6 \tlabel=8 \tword_id=5 \ttoken=ĠBob\n",
      "i=8 \tlabel_index=6 \tlabel=0 \tword_id=5 \ttoken=s\n",
      "i=9 \tlabel_index=7 \tlabel=0 \tword_id=6 \ttoken=Ġ``\n",
      "i=10 \tlabel_index=8 \tlabel=0 \tword_id=7 \ttoken=Ġit\n",
      "i=11 \tlabel_index=9 \tlabel=0 \tword_id=8 \ttoken=Ġis\n",
      "i=12 \tlabel_index=10 \tlabel=0 \tword_id=9 \ttoken=Ġnow\n",
      "i=13 \tlabel_index=11 \tlabel=0 \tword_id=10 \ttoken=Ġpreserved\n",
      "i=14 \tlabel_index=12 \tlabel=0 \tword_id=11 \ttoken=Ġat\n",
      "i=15 \tlabel_index=13 \tlabel=0 \tword_id=12 \ttoken=Ġthe\n",
      "i=16 \tlabel_index=14 \tlabel=2 \tword_id=13 \ttoken=ĠHenry\n",
      "i=17 \tlabel_index=15 \tlabel=2 \tword_id=14 \ttoken=ĠFord\n",
      "i=18 \tlabel_index=16 \tlabel=2 \tword_id=15 \ttoken=ĠMuseum\n",
      "i=19 \tlabel_index=17 \tlabel=0 \tword_id=16 \ttoken=Ġin\n",
      "i=20 \tlabel_index=18 \tlabel=4 \tword_id=17 \ttoken=ĠDear\n",
      "i=21 \tlabel_index=18 \tlabel=0 \tword_id=17 \ttoken=born\n",
      "i=22 \tlabel_index=19 \tlabel=0 \tword_id=18 \ttoken=Ġ,\n",
      "i=23 \tlabel_index=20 \tlabel=4 \tword_id=19 \ttoken=ĠMichigan\n",
      "i=24 \tlabel_index=21 \tlabel=0 \tword_id=20 \ttoken=Ġ.\n",
      "i=25 \tlabel_index=22 \tlabel=0 \tword_id=None \ttoken=</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " -100,\n",
       " 0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match labels to word_ids: if word_id repeats, attach previous label once more to labels_matched\n",
    "labels.append(0)\n",
    "labels_matched = []\n",
    "label_index = -1\n",
    "previous_word_id = False\n",
    "for i in range(len(word_ids)):\n",
    "    word_id = word_ids[i]\n",
    "    ## handle \"None\" labels\n",
    "    if word_id==None:\n",
    "        labels_matched.append(-100)\n",
    "    # handle all other labels\n",
    "    label = labels[label_index]\n",
    "    ## handle repeating word_ids (word split into multiple tokens)\n",
    "    if word_id==previous_word_id and tokens[i]!=\"</s>\":\n",
    "        labels_matched.append(previous_label)\n",
    "    ## handle new labels\n",
    "    else:\n",
    "        labels_matched.append(label)\n",
    "        label_index += 1\n",
    "    # output and updates\n",
    "    print(f\"i={i} \\tlabel_index={label_index} \\tlabel={label} \\tword_id={word_id} \\ttoken={tokens[i]}\")\n",
    "    previous_word_id = word_id\n",
    "    previous_label = label\n",
    "    \n",
    "#\n",
    "labels_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b751f65-d24e-4897-87fc-65be40d75eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7708e20f-8a4e-48fb-a339-8994a8d7e42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 \tlabel_index=0 \tlabel=0 \tword_id=None \ttoken=<s>\n",
      "i=1 \tlabel_index=1 \tlabel=0 \tword_id=0 \ttoken=ĠKnown\n",
      "i=2 \tlabel_index=2 \tlabel=0 \tword_id=1 \ttoken=Ġlocally\n",
      "i=3 \tlabel_index=3 \tlabel=0 \tword_id=2 \ttoken=Ġas\n",
      "i=4 \tlabel_index=4 \tlabel=0 \tword_id=3 \ttoken=Ġ``\n",
      "i=5 \tlabel_index=5 \tlabel=8 \tword_id=4 \ttoken=ĠFair\n",
      "i=6 \tlabel_index=5 \tlabel=8 \tword_id=4 \ttoken=bottom\n",
      "i=7 \tlabel_index=6 \tlabel=8 \tword_id=5 \ttoken=ĠBob\n",
      "i=8 \tlabel_index=6 \tlabel=0 \tword_id=5 \ttoken=s\n",
      "i=9 \tlabel_index=7 \tlabel=0 \tword_id=6 \ttoken=Ġ``\n",
      "i=10 \tlabel_index=8 \tlabel=0 \tword_id=7 \ttoken=Ġit\n",
      "i=11 \tlabel_index=9 \tlabel=0 \tword_id=8 \ttoken=Ġis\n",
      "i=12 \tlabel_index=10 \tlabel=0 \tword_id=9 \ttoken=Ġnow\n",
      "i=13 \tlabel_index=11 \tlabel=0 \tword_id=10 \ttoken=Ġpreserved\n",
      "i=14 \tlabel_index=12 \tlabel=0 \tword_id=11 \ttoken=Ġat\n",
      "i=15 \tlabel_index=13 \tlabel=0 \tword_id=12 \ttoken=Ġthe\n",
      "i=16 \tlabel_index=14 \tlabel=2 \tword_id=13 \ttoken=ĠHenry\n",
      "i=17 \tlabel_index=15 \tlabel=2 \tword_id=14 \ttoken=ĠFord\n",
      "i=18 \tlabel_index=16 \tlabel=2 \tword_id=15 \ttoken=ĠMuseum\n",
      "i=19 \tlabel_index=17 \tlabel=0 \tword_id=16 \ttoken=Ġin\n",
      "i=20 \tlabel_index=18 \tlabel=4 \tword_id=17 \ttoken=ĠDear\n",
      "i=21 \tlabel_index=18 \tlabel=0 \tword_id=17 \ttoken=born\n",
      "i=22 \tlabel_index=19 \tlabel=0 \tword_id=18 \ttoken=Ġ,\n",
      "i=23 \tlabel_index=20 \tlabel=4 \tword_id=19 \ttoken=ĠMichigan\n",
      "i=24 \tlabel_index=21 \tlabel=0 \tword_id=20 \ttoken=Ġ.\n",
      "i=25 \tlabel_index=22 \tlabel=0 \tword_id=None \ttoken=</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " -100,\n",
       " 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match labels to word_ids: if word_id repeats, attach previous label once more to labels_matched\n",
    "labels.append(0)\n",
    "labels_matched = []\n",
    "label_index = -1\n",
    "previous_word_id = False\n",
    "for i in range(len(word_ids)):\n",
    "    word_id = word_ids[i]\n",
    "    ## handle \"None\" labels\n",
    "    if word_id==None:\n",
    "        labels_matched.append(-100)\n",
    "    # handle all other labels\n",
    "    label = labels[label_index]\n",
    "    ## handle repeating word_ids (word split into multiple tokens)\n",
    "    if word_id==previous_word_id and tokens[i]!=\"</s>\":\n",
    "        labels_matched.append(previous_label)\n",
    "    ## handle new labels\n",
    "    else:\n",
    "        labels_matched.append(label)\n",
    "        label_index += 1\n",
    "    # output and updates\n",
    "    print(f\"i={i} \\tlabel_index={label_index} \\tlabel={label} \\tword_id={word_id} \\ttoken={tokens[i]}\")\n",
    "    previous_word_id = word_id\n",
    "    previous_label = label\n",
    "    \n",
    "#\n",
    "labels_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69681079-a6f2-4aad-9384-6b3ea38a09c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 tokens and word_ids\n",
      "token: <s>\tword_id: None\n",
      "token: ĠKnown\tword_id: 0\n",
      "token: Ġlocally\tword_id: 1\n",
      "token: Ġas\tword_id: 2\n",
      "token: Ġ``\tword_id: 3\n",
      "token: ĠFair\tword_id: 4\n",
      "token: bottom\tword_id: 4\n",
      "token: ĠBob\tword_id: 5\n",
      "token: s\tword_id: 5\n",
      "token: Ġ``\tword_id: 6\n",
      "token: Ġit\tword_id: 7\n",
      "token: Ġis\tword_id: 8\n",
      "token: Ġnow\tword_id: 9\n",
      "token: Ġpreserved\tword_id: 10\n",
      "token: Ġat\tword_id: 11\n",
      "token: Ġthe\tword_id: 12\n",
      "token: ĠHenry\tword_id: 13\n",
      "token: ĠFord\tword_id: 14\n",
      "token: ĠMuseum\tword_id: 15\n",
      "token: Ġin\tword_id: 16\n",
      "token: ĠDear\tword_id: 17\n",
      "token: born\tword_id: 17\n",
      "token: Ġ,\tword_id: 18\n",
      "token: ĠMichigan\tword_id: 19\n",
      "token: Ġ.\tword_id: 20\n",
      "token: </s>\tword_id: None\n"
     ]
    }
   ],
   "source": [
    "#i = 22\n",
    "#inputs = tokenizer(fewnerd_all[ix][\"tokens\"], is_split_into_words=True)\n",
    "#tokens = inputs.tokens()\n",
    "#word_ids = inputs.word_ids()\n",
    "#assert len(tokens)==len(word_ids)\n",
    "print(f\"{len(tokens)} tokens and word_ids\")\n",
    "for j in range(len(tokens)):\n",
    "    print(f\"token: {tokens[j]}\\tword_id: {word_ids[j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ae56c2b-c65b-4c11-a773-ee86ee87c9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100],\n",
       " 25)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)\n",
    "match_labels = [-100] + labels + [-100]\n",
    "match_labels, len(match_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "992270c8-92c7-4162-88a0-275e61aa14a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([None,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  None],\n",
       " 26)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids = inputs.word_ids()\n",
    "word_ids, len(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "988799e2-180a-4afe-92e9-c5569d0db442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0\tword_id=None\tis_previous_word_id=False\n",
      "i=1\tword_id=0\tis_previous_word_id=False\n",
      "i=2\tword_id=1\tis_previous_word_id=False\n",
      "i=3\tword_id=2\tis_previous_word_id=False\n",
      "i=4\tword_id=3\tis_previous_word_id=False\n",
      "i=5\tword_id=4\tis_previous_word_id=False\n",
      "i=6\tword_id=4\tis_previous_word_id=True\n",
      "i=7\tword_id=5\tis_previous_word_id=False\n",
      "i=8\tword_id=5\tis_previous_word_id=True\n",
      "i=9\tword_id=6\tis_previous_word_id=False\n",
      "i=10\tword_id=7\tis_previous_word_id=False\n",
      "i=11\tword_id=8\tis_previous_word_id=False\n",
      "i=12\tword_id=9\tis_previous_word_id=False\n",
      "i=13\tword_id=10\tis_previous_word_id=False\n",
      "i=14\tword_id=11\tis_previous_word_id=False\n",
      "i=15\tword_id=12\tis_previous_word_id=False\n",
      "i=16\tword_id=13\tis_previous_word_id=False\n",
      "i=17\tword_id=14\tis_previous_word_id=False\n",
      "i=18\tword_id=15\tis_previous_word_id=False\n",
      "i=19\tword_id=16\tis_previous_word_id=False\n",
      "i=20\tword_id=17\tis_previous_word_id=False\n",
      "i=21\tword_id=17\tis_previous_word_id=True\n",
      "i=22\tword_id=18\tis_previous_word_id=False\n",
      "i=23\tword_id=19\tis_previous_word_id=False\n",
      "i=24\tword_id=20\tis_previous_word_id=False\n",
      "i=25\tword_id=None\tis_previous_word_id=False\n"
     ]
    }
   ],
   "source": [
    "previous_word_id = False\n",
    "for i in range(len(word_ids)):\n",
    "    word_id = word_ids[i]\n",
    "    is_previous_word_id = (word_id==previous_word_id)\n",
    "    print(f\"i={i}\\tword_id={word_ids[i]}\\tis_previous_word_id={is_previous_word_id}\")\n",
    "    # update for next iteration\n",
    "    previous_word_id = word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a85896e-4326-4e95-92b5-32544c980ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ner_tags = []\n",
    "# word_id==None => idx_ner_tag = -100\n",
    "# word_id==previous_word_id => idx_ner_tag = previous_ner_tag\n",
    "# append idx_ner_tag to list \"ner_tags\"\n",
    "len(tokens)==len(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "786c7284-f267-44d4-8af6-19c323cfcd8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ner_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mner_tags\u001b[49m), \u001b[38;5;28mlen\u001b[39m(tokens)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ner_tags' is not defined"
     ]
    }
   ],
   "source": [
    "len(idx_ner_tags), len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bad058c-f087-461c-8c63-eff8b4dbd4fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'idx_ner_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43midx_ner_tags\u001b[49m), idx_ner_tags\n",
      "\u001b[0;31mNameError\u001b[0m: name 'idx_ner_tags' is not defined"
     ]
    }
   ],
   "source": [
    "len(idx_ner_tags), idx_ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6de1c2-5724-462c-8abe-e244f310f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_tokens, len(idx_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfcc185-c243-43ac-887e-794a5b4a5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use list of ner_tags (idx_ner_tags) and list of word ids (=word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e110c-0d9a-4098-a213-d8461237d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064e53a-42c0-4a79-959e-1cdcbe4bd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(idx_ner_tags), len(word_ids), len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75ea00d-e33f-4753-9578-4746248d043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_ner_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4df139-e5eb-49cd-9223-8902e7837f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1cdcc2-0eec-43cb-9dc0-f0e52f054c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7c88a-4404-4b9f-bb15-59e3f4cd6e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de02dc-c52c-40ab-b189-22a03c283299",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f093cd36-6e04-4078-b627-f4b82c2ee13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids, len(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24388905-23f0-4f14-bea7-bc13d24d6ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = -1\n",
    "for i in range(len(tokens)):\n",
    "    new_labels = []\n",
    "    word_id = word_ids[i]\n",
    "    token = tokens[i]\n",
    "    if word_id==None:\n",
    "        #print(\"ignore -100\")\n",
    "        label = -100\n",
    "        #label_index += 1\n",
    "    elif word_id==previous_word_id:\n",
    "        print(\"previous label\")\n",
    "        label = previous_label\n",
    "    else:\n",
    "        print(\"label index +=1\")\n",
    "        label_index += 1\n",
    "    previous_word_id = word_id\n",
    "    previous_label = label\n",
    "    print(f\"i: {i}\\tlabel: {label}\\tword_id: {word_id}\\tlabel_index: {label_index}\\ttoken: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa23a16-d5fc-4cf5-92fd-c833546c6ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = -1\n",
    "for i in range(len(tokens)):\n",
    "    new_labels = []\n",
    "    word_id = word_ids[i]\n",
    "    token = tokens[i]\n",
    "    if word_id==None:\n",
    "        print(\"ignore -100\")\n",
    "        label = -100\n",
    "        #label_index += 1\n",
    "    elif word_id==previous_word_id:\n",
    "        print(\"previous label\")\n",
    "        label = previous_label\n",
    "    else:\n",
    "        print(\"next label\")\n",
    "        label_index += 1\n",
    "    previous_word_id = word_id\n",
    "    previous_label = label\n",
    "    print(f\"i: {i}\\tlabel: {label}\\tword_id: {word_id}\\tlabel_index: {label_index}\\ttoken: {token}\")\n",
    "# to do next:\n",
    "# - inspect instance\n",
    "# - check whether token is <s> or </s> (=> ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c324fd4-7951-4df9-9637-0cd232fe43f3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ee7ef-4315-47dd-b5d9-c1917b442bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4996a5ed-406e-48e7-866c-46487e05798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = 0\n",
    "for i in range(len(tokens)):\n",
    "    new_labels = []\n",
    "    word_id = word_ids[i]\n",
    "    token = tokens[i]\n",
    "    if word_id==None:\n",
    "        label = -100\n",
    "        label_index += 1\n",
    "    elif word_id==previous_word_id:\n",
    "        label = previous_label\n",
    "    else:\n",
    "        #label_index += 1\n",
    "    previous_word_id = word_id\n",
    "    previous_label = label\n",
    "    print(f\"i: {i}\\tlabel: {labels[label_index]}\\tword_id: {word_id}\\ttoken: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73566a66-bc6f-4b9c-b649-3dd9051059c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80647585-92bb-4647-9057-fd6a2c2a6ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe935c03-945a-444a-a780-dd786030f309",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = 0\n",
    "for i in range(29):\n",
    "    new_labels = []\n",
    "    word_id = word_ids[i]\n",
    "    if word_id==None:\n",
    "        #print(\"ignore -100\")\n",
    "        label = -100\n",
    "        label_index += 1\n",
    "    elif word_id==previous_word_id:\n",
    "        #print(\"previous label\")\n",
    "        label = previous_label\n",
    "    else:\n",
    "        #print(\"next label\")\n",
    "        #print(word_id)\n",
    "        label = labels[label_index]\n",
    "        label_index += 1\n",
    "    previous_word_id = word_id\n",
    "    previous_label = label\n",
    "    print(f\"label: {label}\\tword_id: {word_id}\\ti: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5cc6a-eb2e-4232-9c9a-83a4cd252afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_ids), word_ids[0], word_ids[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aab257-6126-4b1f-a6dc-f6e8b147e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_id==None => label = -100\n",
    "# new word_id => first / next label\n",
    "# same word_id => use previous label\n",
    "label_index = 0\n",
    "for i in range(len(word_ids)):\n",
    "    word_id = word_ids[i]\n",
    "    if word_id==None:\n",
    "        label = -100\n",
    "    else:\n",
    "        label = labels[label_index]\n",
    "    label_index += 1\n",
    "    previous_word_id = word_id\n",
    "    pervious_label = label\n",
    "    print(f\"i: {i}\\tword_id: {word_id}\\tlabel: {label}\")\n",
    "    if i==12:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d23fb4-2cda-4677-8e2c-13b7498e2785",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a932a5af-e22d-409a-b4d5-b8cff0662738",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = 0\n",
    "for i in range(len(word_ids)):\n",
    "    new_labels = []\n",
    "    word_id = word_ids[i]\n",
    "    if word_id==None:\n",
    "        label = -100\n",
    "        #label_index += 1\n",
    "    elif word_id==previous_word_id:\n",
    "        #print(\"previous label\")\n",
    "        label = previous_label\n",
    "    else:\n",
    "        #print(\"next label\")\n",
    "        #print(word_id)\n",
    "        label = labels[label_index]\n",
    "        label_index += 1\n",
    "    previous_word_id = word_id\n",
    "    previous_label = label\n",
    "    print(f\"label: {label}\\tlabel_index: {label_index}\\tword_id: {word_id}\\ti: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f129d8d-c946-4366-a70c-ae6ebf218016",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels), len(word_ids), len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49237874-b874-4dc1-ae38-0c594a30e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids, len(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcd59e8-0f5d-4b26-afdc-7a2ddef2625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac3222-920c-4c9e-9c11-953996166f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193fff8b-872c-4ff1-adc4-f43b8ef90363",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = []\n",
    "current_word = None\n",
    "for word_id in word_ids:\n",
    "    print(word_id)\n",
    "    if word_id != current_word:\n",
    "        # Start of a new word!\n",
    "        current_word = word_id\n",
    "        label = -100 if word_id is None else labels[word_id]\n",
    "        new_labels.append(label)\n",
    "#\n",
    "new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d012a9ad-38ce-435e-862d-921f668f46ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_word_id = False\n",
    "label_index = -2\n",
    "for i in range(len(word_ids)):\n",
    "    # word_id\n",
    "    word_id = word_ids[i]\n",
    "    # token\n",
    "    token = tokens[i]\n",
    "    # label\n",
    "    if word_id==previous_word_id:\n",
    "        # reuse previous_label\n",
    "        label = previous_label\n",
    "    else:\n",
    "        # update label\n",
    "        label_index += 1\n",
    "        #label = labels[label_index]\n",
    "    \n",
    "    # print\n",
    "    print(f\"word_id={word_id}\\ttoken={token}\\tlabel_index={label_index}\")\n",
    "    # previous_word_id\n",
    "    previous_word_id = word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dc02de-8c81-43fe-a92e-20446610ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4fd0c2-87ad-4be3-8e56-b1dc1f729830",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels), labels[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47472a6-bcdf-4c7a-b2a4-90c26d50e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate empty list of matched_labels\n",
    "# declare previous_word_id = False\n",
    "# loop over i in range(len(word_ids))\n",
    "# get word_id_i (=word_ids[i]\n",
    "# if word_id_i==None, append -100 to matched_labels\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec9150-bc67-4ff3-8a60-bc4bd8848658",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens), len(word_ids), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc39f42a-9d34-4458-a2b7-4f8cc7cf60d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e09273d-f07f-4b2e-a236-e8afc2961990",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids[-1]==None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646eebb-4db8-473f-b5d0-b9ac499798d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_word_id = False\n",
    "label_index = -1\n",
    "for i in range(len(tokens)):\n",
    "    token = tokens[i]\n",
    "    word_id = word_ids[i]\n",
    "    if word_id==None:\n",
    "        word_id = -100\n",
    "    if word_id!=previous_word_id and word_id!=-100:\n",
    "        #print(i)\n",
    "        label_index += 1\n",
    "    previous_word_id = word_id\n",
    "    label = labels[label_index]\n",
    "    print(f\"item\\t{i}\\tword_id\\t{word_id}\\tlabel\\t{label}\\tlabel_index\\t{label_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2701f3b6-24d3-4206-9e03-04a6b91e3c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_word_id = False\n",
    "label_index = 0\n",
    "for i in range(len(tokens)):\n",
    "    token = tokens[i]\n",
    "    word_id = word_ids[i]\n",
    "    if word_id!=previous_word_id:\n",
    "        label_index += 1\n",
    "    #print(labels[label_index])\n",
    "    print(f\"{i}\\t{word_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12754fc-fe05-4365-9361-641678942927",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_word_id = False\n",
    "word_id_index = 0\n",
    "for i in range(len(tokens)):\n",
    "    token = tokens[i]\n",
    "    word_id = word_ids[i]\n",
    "    if word_id!=previous_word_id:\n",
    "        word_id_index += 1\n",
    "    print(word_ids[word_id_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33cd91-f27b-47b8-846c-af2c9e6cb20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_word_ids = []\n",
    "previous_word_id = False\n",
    "word_id_index = 0\n",
    "for i in range(len(tokens)):\n",
    "    token = tokens[i]\n",
    "    word_id = word_ids[i]\n",
    "    if word_id!=previous_word_id:\n",
    "        word_id_index += 1\n",
    "    #print(word_ids[word_id_index])\n",
    "    matched_word_ids.append(word_ids[word_id_index])\n",
    "len(matched_word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539cfcda-f3e3-48b4-b149-5b835bed418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(matched_word_ids)):\n",
    "    print(matched_word_ids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa1da82-8d72-459d-9a83-461285ac4e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8864188e-7e9d-428a-9f9e-2beb03e54101",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7f3cbf-b222-44ec-ad93-9047e8fe92cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684631d4-da7a-4999-ae8a-20c455eb013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = 0\n",
    "for i in range(29):\n",
    "    new_labels = []\n",
    "    word_id = word_ids[i]\n",
    "    if word_id==None:\n",
    "        #print(\"ignore -100\")\n",
    "        comment = \"ignore -100\"\n",
    "        label = -100\n",
    "        label_index += 1\n",
    "    elif word_id==previous_word_id:\n",
    "        #print(\"previous label\")\n",
    "        comment = \"previous label\"\n",
    "        label = previous_label\n",
    "    else:\n",
    "        #print(\"next label\")\n",
    "        comment = \"next label\"\n",
    "        #print(word_id)\n",
    "        label = labels[label_index] # len(labels) = 21\n",
    "        label_index += 1\n",
    "    previous_word_id = word_id\n",
    "    previous_label = label\n",
    "    print(f\"i: {i}\\tword_id: {word_id}\\tlabel: {label}\\tlabel_index: {label_index}\\tcomment: {comment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25467128-6708-45f3-8256-ac782fe29e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "align_labels_with_tokens(labels, word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5bc3ba-4a32-48e0-8007-bff6a8db4f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964fe4cf-4cc9-41fd-be87-064e4bb3b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over word_ids and ner_tags (but word_ids is longer than ner_tags because word_ids comes from tokens and ner_tags from words)\n",
    "# inside the loop, get the word_id and check whether it is the same as the previous word id: if yes, set ner_tag to the previous ner_tag\n",
    "previous_word_id = False\n",
    "ner_tags = []\n",
    "ner_tag_i = -1\n",
    "for i, word_id in enumerate(word_ids):\n",
    "    ner_tag = idx_ner_tags[ner_tag_i]\n",
    "    if word_id==previous_word_id:\n",
    "        ner_tag=previous_ner_tag\n",
    "    else:\n",
    "        ner_tag_i += 1\n",
    "        previous_word_id = word_id\n",
    "    print(f\"i: {i}\\t ner_tag_i: {ner_tag_i}\")\n",
    "    ner_tags.append(ner_tag)\n",
    "    previous_ner_tag = ner_tag\n",
    "#\n",
    "len(ner_tags), len(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31379b10-3e7f-483c-9575-ed3eafcd1375",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_ner_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e80673c-1f65-43e8-8fd1-0fb872f8556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over word_ids and ner_tags (but word_ids is longer than ner_tags because word_ids comes from tokens and ner_tags from words)\n",
    "# inside the loop, get the word_id and check whether it is the same as the previous word id: if yes, set ner_tag to the previous word_tag\n",
    "\n",
    "# loop over tokens\n",
    "\n",
    "\n",
    "# set current_word_id = -100\n",
    "# set word_id_idx = 0\n",
    "# get next word_id\n",
    "# if word_id==current_word_id: append previous ner_tag once more\n",
    "current_word_id = False\n",
    "ner_tag_i = -1\n",
    "for i in range(len(tokens)):\n",
    "    # word_ids\n",
    "    word_id = word_ids[i]\n",
    "    if word_id==None:\n",
    "        word_id = -100   \n",
    "    if word_id==current_word_id:\n",
    "        ner_tag = current_ner_tag\n",
    "    else:\n",
    "        ner_tag_i +=1\n",
    "    current_word_id = word_id\n",
    "    # ner_tags\n",
    "    ner_tag = idx_ner_tags[ner_tag_i]\n",
    "    current_ner_tag = ner_tag\n",
    "    \n",
    "    print(f\"i = {i}\\tword_id = {word_id}\\tner_tag_i = {ner_tag_i}\\ttoken = {tokens[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f4bfad-d088-4186-ad0e-228bb7075f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522d7ef3-da6b-4e82-b258-fd125d49871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id_list = []\n",
    "current_word_id = False\n",
    "for i, word_id in enumerate(word_ids):\n",
    "    \n",
    "    \n",
    "    print(word_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef855ca-ce9e-442d-9d2c-b8b03c7f8f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_labels = []\n",
    "for i in range(len(tokens)):\n",
    "    print(f\"word_id:\\t{word_ids[i]}\\ttoken:\\t{tokens[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f9c462-d6be-4b4c-8d6f-7520988fec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e846a-2a74-44ce-bc01-15f7b95c9eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ff45d-ba99-438a-a910-9901f564cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb43fc8-ccc0-4f7b-a31b-729d84ec18ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = fewnerd_all[1][\"ner_tags\"]\n",
    "print(labels)\n",
    "word_ids = inputs.word_ids()\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfcbc14-0544-4106-9132-1c88b6f05150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d79bc-d803-408b-bffc-4d4d0758065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 22\n",
    "words = fewnerd_all[i][\"tokens\"]\n",
    "labels = fewnerd_all[i][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "print(line1)\n",
    "print(line2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf1f2aff-bc72-4b19-8826-1226a6718c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=True\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_SILENT=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1432db6f-76ee-4170-b283-73209c3fc987",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"Pakistani\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19db3fa-9544-4193-8e98-ec51f851c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = fewnerd_all.features[\"ner_tags\"].feature.names\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e436ab-0b72-4c8f-9884-7a678c6d0d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {k: v for k, v in enumerate(labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "id2label, label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a697ae9a-a493-45ac-b1cb-164154eec90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize an instance\n",
    "# get tokens\n",
    "# get word_ids\n",
    "# adapt labels to word_ids\n",
    "fewnerd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aca306-38bf-4763-8f19-4487623ddcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 22 # 17, 22\n",
    "fewnerd_all[i][\"words\"], fewnerd_all[i][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3459f327-184b-4531-94ef-7e951933ee6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c563a-6fec-407e-9261-408710a63430",
   "metadata": {},
   "outputs": [],
   "source": [
    "fewnerd_all[\"ner_tags\"][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42ce94-e874-44e9-9be1-750ea2467304",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in fewnerd_all.features[\"ner_tags\"]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58299f4-51be-45a6-a20f-b14964007f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_types = [\"Location\", \"Person\", \"ORG\", \"Building\", \"Art\", \"Product\", \"Event\", \"Misc\"]\n",
    "fine_types = [\n",
    "    # Location\n",
    "    \"GPE\", \"Body of Water\", \"Island\", \"Mountain\", \"Park\", \"Road/Transit\", \"Other\",\n",
    "\t# Person\n",
    "\t\"Actor\", \"Aritst/Author\", \"Director\", \"Politician\", \"Scholar\", \"Soldier\", \"Other\",\n",
    "\t# ORG\n",
    "\t\"Company\", \"Education\", \"Government\", \"Media\", \"Politician/party\", \"Religion\", \"Sports League\", \"Sports Team\", \"Show ORG\", \"Other\",\n",
    "\t# Building\n",
    "    \"Airport\", \"Hospital\", \"Hotel\", \"Library\", \"Restaurant\", \"Sports Facility\", \"Theater\", \"Other\",\n",
    "    # Art\n",
    "    \"Music\", \"Film\", \"Written Arg\", \"Broadcast\", \"Painting\", \"Other\",\n",
    "\t# Product\n",
    "\t\"Airplane\", \"Car\", \"Food\", \"Game\", \"Ship\", \"Software\", \"Train\", \"Weapon\", \"Other\",\n",
    "\t# Event\n",
    "\t\"Attack\", \"Election\", \"Natural Disaster\", \"Protest\", \"Sports Event\", \"Other\",\n",
    "\t# Misc\n",
    "    \"Astronomy\", \"Award\", \"Biology\", \"Chemistry\", \"Currency\", \"Disease\", \"Educational Degree\", \"God\", \"Language\", \"Law\", \"Living Thing\", \"Medical\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d20e92-f99c-4595-ba97-0082f35b0d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label_coarse = {str(i): label for i, label in enumerate(coarse_types)}\n",
    "label2id_coarse ={v: k for k, v in id2label_coarse.items()}\n",
    "id2label_coarse, label2id_coarse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9bf44c-ab86-40d4-b236-0ca5a38c1889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#id2label_fine = {str(i): label for i, label in enumerate(fine_types)}\n",
    "#label2id_fine ={v: k for k, v in id2label_fine.items()}\n",
    "#id2label_fine, label2id_fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caa633f-c618-42be-bb43-693c089d7fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "checkpoint = \"FacebookAI/roberta-large\"\n",
    "lr = 1e-3\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "# load model\n",
    "# load tokenizer\n",
    "# load dataset\n",
    "# get dataset instance (text and ner_labels)\n",
    "# align text with ner_labels\n",
    "# get tokenized text\n",
    "# align tokenized text with ner_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1607ed53-2aea-4ef5-a0e2-3d339c2932bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365b4e91-07ed-4ad7-8807-a06e51cb5594",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "my_text = \"Some fancy, non-squiggly text.\"\n",
    "my_text_tokenized = tokenizer.tokenize(my_text)\n",
    "my_text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2861b56a-6d2a-4ac7-b5cd-e9e950079d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text_encoded = tokenizer.encode(\"some fancy, non-squiggly text\")\n",
    "my_text_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a826a4-5bb7-453b-a36a-efe31eb1f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text_decoded = tokenizer.decode(my_text_encoded)\n",
    "my_text_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fcfb7e-34e8-472a-a19e-bff1dfb2884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fewnerd = load_dataset(\"DFKI-SLT/few-nerd\", \"supervised\")\n",
    "fewnerd_all = concatenate_datasets([fewnerd[\"train\"], fewnerd[\"validation\"], fewnerd[\"test\"]])\n",
    "fewnerd_all = fewnerd_all.rename_column(\"tokens\", \"text\")\n",
    "fewnerd_all.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c1c9fe-e60f-4566-9846-4aacc1fe1ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"some fancy, non-squiggly text\")#, is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4542ac-eaa6-4ce2-8ddd-3b5a1d80ddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(inputs.word_ids())==len(inputs.tokens()))\n",
    "inputs.word_ids(), inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38c4038-8bb9-4bad-a18e-0a3b5d45c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d839f8fb-c049-4857-b141-ac661ae97e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b1a822-fbda-45c9-ab38-f9a6a3583542",
   "metadata": {},
   "source": [
    "## Load dataset and metric\n",
    "The [BioNLP2004](https://huggingface.co/datasets/tner/bionlp2004) dataset includes tokens and tags for biological structures like DNA, RNA and proteins. Load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c485a4f1-4936-470e-9150-04274a8d1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fewnerd = load_dataset(\"DFKI-SLT/few-nerd\", \"supervised\")\n",
    "fewnerd_all = concatenate_datasets([fewnerd[\"train\"], fewnerd[\"validation\"], fewnerd[\"test\"]])\n",
    "fewnerd_all = fewnerd_all.rename_column(\"tokens\", \"text\")\n",
    "fewnerd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea8e20-d107-4464-a7e8-f37d690c446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 8\n",
    "fewnerd_all[\"ner_tags\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d840a13-9551-4abf-a6f4-c86d57dfb8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fewnerd_all[\"text\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3fa80-3038-42b6-bd33-47c2fb6fb3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 7\n",
    "len(fewnerd_all[\"ner_tags\"][i])==len(fewnerd_all[\"text\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382b1f9b-1fbe-40c3-85e5-a9895929c88f",
   "metadata": {},
   "source": [
    "Load the [**fewnerd**](https://arxiv.org/pdf/2105.07464v6.pdf) dataset and read the according [**publication**](https://aclanthology.org/2021.acl-long.248/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6744a4c6-3fef-43bd-95fb-55c79f67a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "# 0.1\n",
    "# 100 * 0.1 = 10 => 10 for test, 90 for train + valid; 90 * 0.1 = 9 for valid => 81 for train\n",
    "# 0.15\n",
    "# 100 * 0.15 = 15 => 15 for test, 85 for train + valid; 85 * 0.15 = 12.75 for valid => 72.75 for train\n",
    "# 0.2\n",
    "# 100 * 0.2 = 20 => 20 for test, 80 for train + valid; 80 * 0.2 = 16 for valid => 64 for train\n",
    "dataset_cc = concatenate_datasets([fewnerd[\"train\"], fewnerd[\"validation\"], fewnerd[\"test\"]])\n",
    "dev_split = dataset_cc.train_test_split(test_size=4)[\"test\"]\n",
    "trainvalid_test_splits = dataset_cc.train_test_split(test_size=0.15) # train 81% valid 9% test 10%\n",
    "test_split = trainvalid_test_splits[\"test\"]\n",
    "trainvalid_split = trainvalid_test_splits[\"train\"]\n",
    "train_valid_split = trainvalid_split.train_test_split(test_size=0.15)\n",
    "valid_split = train_valid_split[\"test\"]\n",
    "train_split = train_valid_split[\"train\"]\n",
    "dataset_fewnerd = DatasetDict({\n",
    "    \"train\": train_split,\n",
    "    \"valid\": valid_split,\n",
    "    \"test\": test_split,\n",
    "    \"dev\": dev_split\n",
    "}).remove_columns([\"id\", \"ner_tags\"])\n",
    "dataset_fewnerd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb871b-98d5-41c8-9071-668e831f9b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls 1_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42257715-c553-45db-87c1-a6afff481764",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls 1_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009444f9-98bf-4cad-828c-c256e757a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_file_path = \"/path/to/example.json\"\n",
    "json_file_path = \"1_ner/few_ner_labels.json\"\n",
    "\n",
    "with open(json_file_path, \"r\") as j:\n",
    "     contents = json.loads(j.read())\n",
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2a0f28-f38b-472f-99af-f1e0569446d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_file_path = \"/path/to/example.json\"\n",
    "json_file_path = \"1_ner/few_ner_labels.json\"\n",
    "\n",
    "with open(json_file_path, \"r\") as j:\n",
    "     contents = json.loads(j.read())\n",
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2980dcf3-d716-4419-bac7-4de970da91c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls 1_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f44b244-2b3f-4345-8921-91f407de01cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json import\n",
    "with open(\"1_ner/few_ner_labels.json\", \"r\") as file:\n",
    "    labels = load(file)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c5de8b-1c68-4535-ad44-340a5626f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.loads(\"1_ner/few_ner_labels.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72095cd-a268-477a-8bdd-14c0d8f87223",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cc[\"fine_ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858db3a-fee4-4e31-9a72-f23bec9c4be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fewnerd[\"dev\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d193b4b-6f9c-4275-84ff-769d2f53c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant keys: \"tokens\" (rename to \"words\"), \"fine_ner_tags\"; irrelevant keys: \"id\", \"ner_tags\"\n",
    "dataset_fewnerd[\"dev\"][0].keys() # keys: \"id\", \"tokens\", \"ner_tags\", \"fine_ner_tags\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16412a18-f152-4fd2-9149-6f41b10bfdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_fewnerd[\"dev\"][0][\"fine_ner_tags\"]) # use fine_ner_tags for a challenge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815afe64-5b0b-4343-aaef-dc570a8616e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 94\n",
    "dataset_fewnerd[\"train\"][i][\"fine_ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a923e0-be3f-4e5f-9c3f-9804c67ef024",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fewnerd[\"train\"][i][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416e0fd-e46a-4b7f-9a97-0c93e8f82c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1\n",
    "# 100 * 0.1 = 10 => 10 for test, 90 for train + valid; 90 * 0.1 = 9 for valid => 81 for train\n",
    "# 0.15\n",
    "# 100 * 0.15 = 15 => 15 for test, 85 for train + valid; 85 * 0.15 = 12.75 for valid => 72.75 for train\n",
    "# 0.2\n",
    "# 100 * 0.2 = 20 => 20 for test, 80 for train + valid; 80 * 0.2 = 16 for valid => 64 for train\n",
    "bionlp = load_dataset(\"tner/bionlp2004\")\n",
    "# each dataset instance is a dictionary with keys \"tokens\" (or \"words\") and \"tags\".\n",
    "# under these keys the dictionary has equally long lists of tokens and tags\n",
    "bionlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b0456b-e66d-482f-a8c5-801dfc40515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bionlp[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867f3ad-c0e4-45ca-a0b3-c3fe9d437902",
   "metadata": {},
   "outputs": [],
   "source": [
    "bionlp[\"train\"][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6bf286-b78c-48d6-bbac-c52e3ec8ea63",
   "metadata": {},
   "source": [
    "The `tags` values are defined in the label ids [dictionary](https://huggingface.co/datasets/tner/bionlp2004#label-id). The letter that prefixes each label indicates the token position: `B` is for the first token of an entity, `I` is for a token inside the entity, and `0` is for a token that is not part of an entity.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"O\": 0,\n",
    "    \"B-DNA\": 1,\n",
    "    \"I-DNA\": 2,\n",
    "    \"B-protein\": 3,\n",
    "    \"I-protein\": 4,\n",
    "    \"B-cell_type\": 5,\n",
    "    \"I-cell_type\": 6,\n",
    "    \"B-cell_line\": 7,\n",
    "    \"I-cell_line\": 8,\n",
    "    \"B-RNA\": 9,\n",
    "    \"I-RNA\": 10,\n",
    "}\n",
    "```\n",
    "\n",
    "Then load the [`seqeval`](https://huggingface.co/spaces/evaluate-metric/seqeval) framework which includes several metrics - precision, accuracy, F1, and recall - for evaluating sequence labeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d087ed-ece3-430a-9733-9437b240d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86813f06-fd0a-405a-bba1-258750f272e9",
   "metadata": {},
   "source": [
    "Now you can write an evaluation function to compute the metrics from the model predictions and labels, and return the precision, recall, $F_1$, and accuracy scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4e1eb7-fda8-42d8-b9bf-47157340bd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\n",
    "    \"O\",\n",
    "    \"B-DNA\",\n",
    "    \"I-DNA\",\n",
    "    \"B-protein\",\n",
    "    \"I-protein\",\n",
    "    \"B-cell_type\",\n",
    "    \"I-cell_type\",\n",
    "    \"B-cell_line\",\n",
    "    \"I-cell_line\",\n",
    "    \"B-RNA\",\n",
    "    \"I-RNA\",\n",
    "]\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5339a0-7ed6-4969-a03b-12593df8e5a5",
   "metadata": {},
   "source": [
    "## Preprocess dataset\n",
    "Initialize a tokenizer and make sure you set `is_split_into_words=True` because the text sequence has already been split into words. However, this doesn't mean it is tokenized yet (even though it may look like it!), and you'll need to further tokenize the words into subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976802dd-73cc-4a02-b5c8-89e5ac54792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17848124-ca64-4659-81c3-03a1e2d14ac4",
   "metadata": {},
   "source": [
    "You’ll also need to write a function to:\n",
    "1. Map each token to their respective word with the [`word_ids`](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding.word_ids) method.\n",
    "1. Ignore the special tokens by setting them to `-100`.\n",
    "1.  Label the first token of a given entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0532539-3b19-477c-ac5e-bb94492b1b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a789cbf4-494d-474e-aa75-ea551482334c",
   "metadata": {},
   "source": [
    "Use [`map`](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map) to apply the `tokenize_and_align_labels` function to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cda53bd-e439-4f5d-b769-888abafd2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_bionlp = bionlp.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_bionlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695fbfae-8c48-43df-ab53-d5ae96569905",
   "metadata": {},
   "source": [
    "Finally, create a data collator to pad the examples to the longest length in a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04568fe9-bfd6-423f-827e-6b7160863ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37211f5-f563-4c32-b2cf-0a5634e39bc4",
   "metadata": {},
   "source": [
    "## Train\n",
    "Now you're ready to create a [PeftModel](https://huggingface.co/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel). Start by loading the base `roberta-large` model, the number of expected labels, and the `id2label` and `label2id` dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899ea47-a002-4d05-ad8c-48526a26cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-DNA\",\n",
    "    2: \"I-DNA\",\n",
    "    3: \"B-protein\",\n",
    "    4: \"I-protein\",\n",
    "    5: \"B-cell_type\",\n",
    "    6: \"I-cell_type\",\n",
    "    7: \"B-cell_line\",\n",
    "    8: \"I-cell_line\",\n",
    "    9: \"B-RNA\",\n",
    "    10: \"I-RNA\",\n",
    "}\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-DNA\": 1,\n",
    "    \"I-DNA\": 2,\n",
    "    \"B-protein\": 3,\n",
    "    \"I-protein\": 4,\n",
    "    \"B-cell_type\": 5,\n",
    "    \"I-cell_type\": 6,\n",
    "    \"B-cell_line\": 7,\n",
    "    \"I-cell_line\": 8,\n",
    "    \"B-RNA\": 9,\n",
    "    \"I-RNA\": 10,\n",
    "}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=11,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bba790b-69cb-4075-9226-cf42e42d30e5",
   "metadata": {},
   "source": [
    "Define the [LoraConfig](https://huggingface.co/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig) with:\n",
    "- `task_type`, token classification (`TaskType.TOKEN_CLS`)\n",
    "- `r`, the dimension of the low-rank matrices\n",
    "- `lora_alpha`, scaling factor for the weight matrices\n",
    "- `lora_dropout`, dropout probability of the LoRA layers\n",
    "- `bias`, set to `all` to train all bias parameters\n",
    "\n",
    "> <font style=\"color:darkgreen\">💡 The weight matrix is scaled by `lora_alpha/r`, and a higher `lora_alpha` value assigns more weight to the LoRA activations. For performance, we recommend setting `bias` to `None` first, and then `lora_only`, before trying `all`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6420299-f3f9-41c6-91a7-173dca5df6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ae30a9-9dbf-4bf7-b4e8-9b9b3ef278bd",
   "metadata": {},
   "source": [
    "Pass the base model and `peft_config` to the [`get_peft_model()`](https://huggingface.co/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model) function to create a [PeftModel](https://huggingface.co/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel). You can check out how much more efficient training the PeftModel is compared to fully training the base model by printing out the trainable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae5ac5e-9e82-4159-8cff-9cafc719a50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c5873-b7c9-420c-a939-1f297f5457d0",
   "metadata": {},
   "source": [
    "From the 🤗 Transformers library, create a [TrainingArguments](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments) class and specify where you want to save the model to, the training hyperparameters, how to evaluate the model, and when to save the checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4126a3-9a0c-4199-b4c8-1df3a0f01409",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ner/logs/roberta-large-lora-token-classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d3adf7-af1d-4baa-a03f-39815d509bde",
   "metadata": {},
   "source": [
    "Pass the model, `TrainingArguments`, datasets, tokenizer, data collator and evaluation function to the [Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) class. The `Trainer` handles the training loop for you, and when you're ready, call [`train`](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train) to begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7939a17-eb40-4d16-8933-fb9eea344f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_bionlp[\"train\"],\n",
    "    eval_dataset=tokenized_bionlp[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa722ea6-7262-49d8-84e9-a5234358471f",
   "metadata": {},
   "source": [
    "## Share model\n",
    "Once training is complete, you can store and share your model on the Hub if you'd like. Log in to your HuggingFace account and enter your token when prompted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7932264d-c9c9-49d3-92ad-80a3f2247e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072ea915-496d-4c41-89c9-60aee466e894",
   "metadata": {},
   "source": [
    "Upload the model to a specific model repository on the Hub with the [`push_to_hub`](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.push_to_hub) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e336c1-4c28-4737-b552-3e9c293843f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"mdroth/roberta-large-lora-token-classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142a72a4-51c7-4df9-951c-cfb5fee2cf0e",
   "metadata": {},
   "source": [
    "##  Inference\n",
    "To use your model for inference, load the configuration and model: $1+1=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c229ee-c214-41f4-88e5-67f892dae927",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"mdroth/roberta-large-lora-token-classification\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "inference_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    config.base_model_name_or_path, num_labels=11, id2label=id2label, label2id=label2id\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(inference_model, peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df9220-a86d-4856-82ef-1d702412e6c2",
   "metadata": {},
   "source": [
    "Get some text to tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8236f66-2bd6-4d21-99e0-eaeba0fa57e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The activation of IL-2 gene expression and NF-kappa B through CD28 requires reactive oxygen production by 5-lipoxygenase.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96545b9e-0200-422d-a9f8-8ce703d836f0",
   "metadata": {},
   "source": [
    "Pass the inputs to the model, and print out the model prediction for each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf92b2ac-7f30-457b-a401-129c03390e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "tokens = inputs.tokens()\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "for token, prediction in zip(tokens, predictions[0].numpy()):\n",
    "    print((token, model.config.id2label[prediction]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbaa5e4-323f-4bc3-a033-441b265ca943",
   "metadata": {},
   "source": [
    "<font style=\"font-weight:300\">✔</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
