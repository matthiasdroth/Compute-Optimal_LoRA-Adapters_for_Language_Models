{
  "base_model_id": "FacebookAI/roberta-large",
  "LoRA_params_dict": {
    "r": 64,
    "target_modules": [
      "query",
      "key",
      "value",
      "query_proj",
      "key_proj",
      "value_proj"
    ],
    "bias": "lora_only",
    "use_rslora": true,
    "task_type": "TOKEN_CLS",
    "lora_dropout": 0.2
  },
  "LoraConfig": "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.TOKEN_CLS: 'TOKEN_CLS'>, inference_mode=False, r=64, target_modules={'key', 'value', 'query', 'value_proj', 'key_proj', 'query_proj'}, lora_alpha=8, lora_dropout=0.2, fan_in_fan_out=False, bias='lora_only', use_rslora=True, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={})",
  "LoRA_model_trainable_params": 9446409,
  "LoRA_model_all_params": 363765778,
  "LoRA_model_trainable_fraction": 0.02597,
  "tokenizer": "FacebookAI/roberta-large",
  "batch_size": 16,
  "num_training_steps": 258,
  "epoch_0_results": {
    "precision": 0.0,
    "recall": 0.0,
    "f1": 0.0,
    "accuracy": 0.7266285091305533
  },
  "epoch_1_results": {
    "precision": 0.0,
    "recall": 0.0,
    "f1": 0.0,
    "accuracy": 0.7266285091305533
  },
  "epoch_2_results": {
    "precision": 0.0,
    "recall": 0.0,
    "f1": 0.0,
    "accuracy": 0.7266285091305533
  },
  "training_loop_time": "0:02:46",
  "flops_list": [
    5266793944832,
    5192614297600,
    5290477487872
  ]
}