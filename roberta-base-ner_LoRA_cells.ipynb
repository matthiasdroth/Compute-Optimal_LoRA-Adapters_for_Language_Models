{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a51dd7f1-66b5-4259-b629-325467839741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 128\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 32\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 40\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, AdamW\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import LoraConfig, TaskType, PeftModel, PeftConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Load the tokenizer with add_prefix_space=True for compatibility with pre-tokenized inputs\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base', add_prefix_space=True)\n",
    "\n",
    "# Function to tokenize and align labels for the NER dataset\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, padding=\"max_length\", is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Maps tokens to their word in the input sequences\n",
    "        label_ids = np.ones(len(tokenized_inputs[\"input_ids\"][i]), dtype=int) * -100  # Initialize labels to -100\n",
    "        for word_idx, label in zip(word_ids, examples[\"ner_tags\"][i]):\n",
    "            if word_idx is not None:  # Set labels for words (not subtokens)\n",
    "                label_ids[word_idx] = label\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Load and preprocess the dataset using the correct path\n",
    "dataset = load_dataset(\"DFKI-SLT/few-nerd\", \"supervised\")\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_datasets[\"train\"] = tokenized_datasets[\"train\"].train_test_split(test_size=128)[\"test\"]\n",
    "tokenized_datasets[\"validation\"] = tokenized_datasets[\"train\"].train_test_split(test_size=32)[\"test\"]\n",
    "tokenized_datasets[\"test\"] = tokenized_datasets[\"train\"].train_test_split(test_size=40)[\"test\"]\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c4907f-8741-41e3-9920-376f249e550c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "peft.peft_model.PeftModelForTokenClassification"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data collator for padding and labels\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Load the model configured for NER with the appropriate number of labels\n",
    "model = AutoModelForTokenClassification.from_pretrained('roberta-base', num_labels=len(dataset[\"train\"].features[\"ner_tags\"].feature.names))\n",
    "\n",
    "# peft model\n",
    "config = LoraConfig(\n",
    "    # GUIDE   => https://huggingface.co/docs/peft/main/en/conceptual_guides/lora#common-lora-parameters-in-peft\n",
    "    # https://huggingface.co/docs/peft/main/en/conceptual_guides/lora#common-lora-parameters-in-peft:~:text=use_rslora%3A%20When%20set%20to%20True%2C%20uses%20Rank%2DStabilized%20LoRA%20which%20sets%20the%20adapter%20scaling%20factor\n",
    "    # https://arxiv.org/abs/2312.03732, \n",
    "    r = 2,\n",
    "    target_modules=[\"query\", \"key\", \"value\"],\n",
    "    bias=\"all\",\n",
    "    use_rslora=True,\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, config)\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7527f99c-16c3-40fd-ba40-95923861040c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lr_scheduler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 22\u001b[0m \u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m loss_values\u001b[38;5;241m.\u001b[39mappend(accelerator\u001b[38;5;241m.\u001b[39mgather(loss\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lr_scheduler' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup DataLoader\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=1, collate_fn=data_collator)\n",
    "\n",
    "# Prepare for training with the Accelerate library\n",
    "accelerator = Accelerator()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)\n",
    "\n",
    "# Initialize the list to store loss values\n",
    "loss_values = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**{k: v.to(accelerator.device) for k, v in batch.items()})\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_values.append(accelerator.gather(loss.repeat(1)).mean().item())\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Epoch: {epoch + 1}, Step: {step}, Loss: {loss.item()}\")\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(loss_values, label='Training Loss')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f214ff1e-517f-4a52-acdc-3f72d6a0cf9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
