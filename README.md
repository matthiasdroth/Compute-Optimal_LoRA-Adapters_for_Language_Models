# Compute-Optimal_LoRA_Adapters_for_Causal-LM

Chinchilla paper but with low-rank adaptation and for causal language modelling

## To do

- run training with llama (using LoRA)
- run inference with llama
- replace llama with blenderbot (using LoRA)
- run inference with blenderbot
- configure sweep training with blenderbot using LoRA hyperparameters (Which ones? â€“ See docs and chatgpt!)
- make sure all results are available on wandb
-

## Done
