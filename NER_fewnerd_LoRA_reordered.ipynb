{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f55bab0",
   "metadata": {},
   "source": [
    "- *It works!*\n",
    "- DONE: use `max_batchsize` from `utils`\n",
    "- DONE: make **dedicated notebook** to (i) compare `conll2003` to `fewnerd` and (ii) to bring `fewnerd` into the same format.\n",
    "- DONE: use fewnerd\n",
    "- DONE: get total flops count with **einops**\n",
    "- DONE: make custom splits\n",
    "- DONE: reorder the notebook cells:\n",
    "  - DONE: model (LoRA) and tokenizer (save peftconfig if necessary)\n",
    "  - DONE: dataset etc\n",
    "  - DONE: training, metrics and saving (tokenizer and model)\n",
    "  - DONE: inference via loading saved items (probably tokenizer, model and peftconfig – OR follow this [guide](https://huggingface.co/docs/peft/v0.9.0/en/package_reference/lora#peft.LoraModel))\n",
    "- DONE: In **Using the fine-tuned model**, [merge and unload](https://huggingface.co/docs/peft/v0.6.2/en/package_reference/tuners#peft.LoraModel.merge_and_unload) or [reinstantiate](https://huggingface.co/docs/peft/v0.6.2/en/task_guides/token-classification-lora#inference) the LoRA model!\n",
    "- DONE: adjust batch size – and if necessary epochs – for 3000 training steps `num_steps = train_instances*epochs/batch_size` $\\geq$ 3000 $\\Rightarrow$ `batch_size` $\\leq$ `train_instances*epochs/3000 = train_instances/1000` $\\Rightarrow$ `batch_size` $\\leq$ `train_instances / 1000` for `epochs = 3`<br>DONE: But do it like this:\n",
    "  - DONE: get max batch size for model (= max_batchsize_by_model)\n",
    "  - DONE: specify trainig split\n",
    "  - DONE: get max batch size for training split length (=max_batchsize_by_trainsplit)\n",
    "  - DONE: impose max batch size of 32\n",
    "  - DONE: the batch size is the minimum of these three numbers\n",
    "- DONE: build `results.json` (consider pandas series) via dict. It holds: splits, specified loraconfig details, flops, metrics (per epoch)\n",
    "- DONE: use the uuid library to save `results.json` under `results_{uuid}.json`\n",
    "- DONE: declare variable `split` and use it to select splits as well as for logging it in `results_{uuid}.json`.\n",
    "- DONE: use split `dev` and determine the learning rate for LoRA models. It seems that with `accelerate`, the maximum accepted learning rate is `5e-4` since training fails for higher learning rates.\n",
    "- DONE: try a different checkpoint for the model (larger!) and tokenizer. Outcomes:\n",
    "  - mistral etc. cannot perform token classification (loading these model with AutoModelForTokenClassification will error out)\n",
    "  - stick with `\"FacebookAI/roberta-large\"`\n",
    "- DONE: [Set random seeds](https://wandb.ai/sauravmaheshkar/RSNA-MICCAI/reports/How-to-Set-Random-Seeds-in-PyTorch-and-Tensorflow--VmlldzoxMDA2MDQy) for reproducibility.\n",
    "- DONE: Drop `uuid` to remove ambiguity about which checkpoint to use. And tidy up the according commented out sections!\n",
    "- build dedicated inference notebook for final evaluation on test set, inspired by accelerator validation loop that (i) loads the model, tokenizer and dataset, (ii) runs inference on the according test set.\n",
    "- add comments\n",
    "- Check once more the wandb [ablation study](https://wandb.ai/ayush-thakur/dl-question-bank/reports/What-s-the-Optimal-Batch-Size-to-Train-a-Neural-Network---VmlldzoyMDkyNDU).\n",
    "- make new notebook for wandb sweeps (outsource lots of code snippets into dedicated `ner_utils.py` file.\n",
    "\n",
    "# NER with `fewnerd` and LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4b5296-cb60-4875-a7f3-96e2c0c03fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-10 03:05:33,290] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/matthias/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from peft import LoraConfig, TaskType, PeftModel, PeftConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from utils import get_max_instance, get_max_batchsize\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "from evaluate import load\n",
    "from tqdm.auto import tqdm\n",
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler, pipeline, AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from huggingface_hub import login\n",
    "from torch.utils.data import DataLoader\n",
    "from deepspeed.profiling.flops_profiler import FlopsProfiler\n",
    "\n",
    "load_dotenv()\n",
    "login(token=os.getenv(\"HUGGINGFACE_API_KEY\"))\n",
    "logs_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1392fbd6-8971-43cd-8783-482097f1f889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seeds set as 42.\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed=42):\n",
    "    # https://wandb.ai/sauravmaheshkar/RSNA-MICCAI/reports/How-to-Set-Random-Seeds-in-PyTorch-and-Tensorflow--VmlldzoxMDA2MDQy\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # when running the cudnn backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seeds set as {seed}.\")\n",
    "    pass\n",
    "    \n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9864ca-1c11-44bd-8bf5-b1db6cb243d5",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b5fe577-f14f-4f86-a4a8-c2b85662178c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'0': 'O',\n",
       "  '1': 'art',\n",
       "  '2': 'building',\n",
       "  '3': 'event',\n",
       "  '4': 'location',\n",
       "  '5': 'organization',\n",
       "  '6': 'other',\n",
       "  '7': 'person',\n",
       "  '8': 'product'},\n",
       " {'O': '0',\n",
       "  'art': '1',\n",
       "  'building': '2',\n",
       "  'event': '3',\n",
       "  'location': '4',\n",
       "  'organization': '5',\n",
       "  'other': '6',\n",
       "  'person': '7',\n",
       "  'product': '8'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"DFKI-SLT/few-nerd\", \"supervised\")\n",
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "label_names = ner_feature.feature.names\n",
    "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "id2label, label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "091ef51a-7595-4d88-b57d-5d5a636f4ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA config:\n",
      "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.TOKEN_CLS: 'TOKEN_CLS'>, inference_mode=False, r=64, target_modules={'query', 'value_proj', 'value', 'key', 'key_proj', 'query_proj'}, lora_alpha=8, lora_dropout=0.2, fan_in_fan_out=False, bias='lora_only', use_rslora=True, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={})\n",
      "\n",
      "base_model type:\n",
      "<class 'transformers.models.roberta.modeling_roberta.RobertaForTokenClassification'>\n",
      "\n",
      "adapter_model type:\n",
      "<class 'peft.peft_model.PeftModelForTokenClassification'>\n",
      "\n",
      "trainable parameters:\n",
      "9446409\n",
      "\n",
      "nall parameters:\n",
      "363765778\n",
      "\n",
      "trainable fraction:\n",
      "0.02597\n"
     ]
    }
   ],
   "source": [
    "base_model_id = \"FacebookAI/roberta-large\"\n",
    "logs_dict[\"base_model_id\"] = base_model_id\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    base_model_id,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True\n",
    ")\n",
    "# LoRA model\n",
    "# datasets:      3 values [1%, 10%, 100%]\n",
    "# lora_rank:    10 values [1, ..., 512]\n",
    "# lora_dropout:  5 values [0, 0.1, 0.2, 0.3, 0.4]\n",
    "# lora_bias:     3 values [\"all\", \"none\", \"lora_only\"]\n",
    "# => 3 x 10 x 5 x 3 = 450 sweeps per notebook\n",
    "# => start with 2 dataset values (10%, 100%) and 3 rank values (2, 8, 32) => 6 sweeps\n",
    "LoRA_params_dict = {\n",
    "    \"r\": 64,\n",
    "    \"target_modules\": [\"query\", \"key\", \"value\", \"query_proj\", \"key_proj\", \"value_proj\"],\n",
    "    \"bias\": \"lora_only\",\n",
    "    \"use_rslora\": True,\n",
    "    \"task_type\": TaskType.TOKEN_CLS,\n",
    "    \"lora_dropout\": 0.2\n",
    "}\n",
    "logs_dict[\"LoRA_params_dict\"] = LoRA_params_dict\n",
    "# r =   1 => (  156681, 354476050, 0.00044)\n",
    "# r =   2 => (  304137, 354623506, 0.00086)\n",
    "# r =   4 => (  599049, 354918418, 0.00169)\n",
    "# r =   8 => ( 1188873, 355508242, 0.00334)\n",
    "# r =  16 => ( 2368521, 356687890, 0.00664)\n",
    "# r =  32 => ( 4727817, 359047186, 0.01317)\n",
    "# r =  64 => ( 9446409, 363765778, 0.02597)\n",
    "# r = 128 => (18883593, 373202962, 0.0506)\n",
    "# r = 256 => (37757961, 392077330, 0.0963)\n",
    "# r = 512 => (75506697, 429826066, 0.17567)\n",
    "config = LoraConfig(\n",
    "    # GUIDE   => https://huggingface.co/docs/peft/main/en/conceptual_guides/lora#common-lora-parameters-in-peft\n",
    "    # https://huggingface.co/docs/peft/main/en/conceptual_guides/lora#common-lora-parameters-in-peft:~:text=use_rslora%3A%20When%20set%20to%20True%2C%20uses%20Rank%2DStabilized%20LoRA%20which%20sets%20the%20adapter%20scaling%20factor\n",
    "    # https://arxiv.org/abs/2312.03732, \n",
    "    r = LoRA_params_dict[\"r\"],\n",
    "    target_modules=LoRA_params_dict[\"target_modules\"],\n",
    "    bias=LoRA_params_dict[\"bias\"],\n",
    "    use_rslora=LoRA_params_dict[\"use_rslora\"],\n",
    "    task_type=LoRA_params_dict[\"task_type\"],\n",
    "    lora_dropout=LoRA_params_dict[\"lora_dropout\"]\n",
    ")\n",
    "logs_dict[\"LoraConfig\"] = str(config)\n",
    "print(f\"LoRA config:\\n{config}\\n\")\n",
    "adapter_model = prepare_model_for_kbit_training(base_model)\n",
    "adapter_model = get_peft_model(adapter_model, config)\n",
    "print(f\"base_model type:\\n{type(base_model)}\\n\\nadapter_model type:\\n{type(adapter_model)}\")\n",
    "trainable_params, all_params = adapter_model.get_nb_trainable_parameters()\n",
    "trainable_fraction = round(trainable_params/all_params, 5)\n",
    "logs_dict[\"LoRA_model_trainable_params\"] = trainable_params\n",
    "logs_dict[\"LoRA_model_all_params\"] = all_params\n",
    "logs_dict[\"LoRA_model_trainable_fraction\"] = trainable_fraction\n",
    "print(f\"\\ntrainable parameters:\\n{trainable_params}\")\n",
    "print(f\"\\nnall parameters:\\n{all_params}\")\n",
    "print(f\"\\ntrainable fraction:\\n{trainable_fraction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "330cc610-6a56-4631-b226-df0419ac07f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer is fast: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='FacebookAI/roberta-large', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_prefix_space=True)\n",
    "logs_dict[\"tokenizer\"] = base_model_id\n",
    "print(f\"tokenizer is fast: {tokenizer.is_fast}\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8461807-8ab1-4253-ac37-c335c9423fa7",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ad64867-2e55-45ce-b7eb-4cc521a02074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
       "        num_rows: 131767\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
       "        num_rows: 18824\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
       "        num_rows: 37648\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"DFKI-SLT/few-nerd\", \"supervised\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9221b549-f8f4-4ed6-9ac2-98d9429614d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 188239\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "fewnerd_all_processed = concatenate_datasets([raw_datasets[\"train\"], raw_datasets[\"validation\"], raw_datasets[\"test\"]]).map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "fewnerd_all_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cecc64b1-a3bf-4e22-9ac6-8718e78998d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_100: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 136002\n",
       "    })\n",
       "    train_10: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 13601\n",
       "    })\n",
       "    train_1: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1361\n",
       "    })\n",
       "    valid_100: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 24001\n",
       "    })\n",
       "    valid_10: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2401\n",
       "    })\n",
       "    valid_1: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 241\n",
       "    })\n",
       "    test_100: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 28236\n",
       "    })\n",
       "    test_10: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2824\n",
       "    })\n",
       "    test_1: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 283\n",
       "    })\n",
       "    train_dev: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 120\n",
       "    })\n",
       "    valid_dev: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 32\n",
       "    })\n",
       "    test_dev: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter dataset by length if necessary\n",
    "trainvalid_test_splits = fewnerd_all_processed.train_test_split(test_size=0.15)\n",
    "test_split_100 = trainvalid_test_splits[\"test\"]\n",
    "test_split_10 = test_split_100.train_test_split(test_size = 0.1)[\"test\"]\n",
    "test_split_1 = test_split_100.train_test_split(test_size = 0.01)[\"test\"]\n",
    "trainvalid_split = trainvalid_test_splits[\"train\"]\n",
    "train_valid_split = trainvalid_split.train_test_split(test_size=0.15)\n",
    "valid_split_100 = train_valid_split[\"test\"]\n",
    "valid_split_10 = valid_split_100.train_test_split(test_size = 0.1)[\"test\"]\n",
    "valid_split_1 = valid_split_100.train_test_split(test_size = 0.01)[\"test\"]\n",
    "train_split_100 = train_valid_split[\"train\"]\n",
    "train_split_10 = train_split_100.train_test_split(test_size = 0.1)[\"test\"]\n",
    "train_split_1 = train_split_100.train_test_split(test_size = 0.01)[\"test\"]\n",
    "dev_train_split = train_split_100.train_test_split(test_size = 120)[\"test\"]\n",
    "dev_valid_split = valid_split_100.train_test_split(test_size = 32)[\"test\"]\n",
    "dev_test_split = test_split_100.train_test_split(test_size = 8)[\"test\"]\n",
    "fewnerd_dsd = DatasetDict({\n",
    "    \"train_100\": train_split_100,\n",
    "    \"train_10\": train_split_10,\n",
    "    \"train_1\": train_split_1,\n",
    "    \"valid_100\": valid_split_100,\n",
    "    \"valid_10\": valid_split_10,\n",
    "    \"valid_1\": valid_split_1,\n",
    "    \"test_100\": test_split_100,\n",
    "    \"test_10\": test_split_10,\n",
    "    \"test_1\": test_split_1,\n",
    "    \"train_dev\": dev_train_split,\n",
    "    \"valid_dev\": dev_valid_split,\n",
    "    \"test_dev\": dev_test_split\n",
    "})\n",
    "fewnerd_dsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "847fc32c-5cb0-44d3-b6d9-45f5f2202a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('train_dev', 'valid_dev', 'test_dev')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = \"dev\" # \"100\", \"10\", \"1\", \"dev\"\n",
    "assert split in (\"100\", \"10\", \"1\", \"dev\"), f\"Split '{split}' is not a valid choice.\"\n",
    "train_split = f\"train_{split}\"\n",
    "valid_split = f\"valid_{split}\"\n",
    "test_split = f\"test_{split}\"\n",
    "train_split, valid_split, test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ce12c9-fad5-4c94-af66-d77aade82d78",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "608c18ab-8a49-468d-9975-04bc6f0630a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForTokenClassification(tokenizer=RobertaTokenizerFast(name_or_path='FacebookAI/roberta-large', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}, padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd71e709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    0,    0,    0,    0,    0,    0,    5,    6,    0,    0,    4,\n",
       "            4,    4,    0,    4,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100],\n",
       "        [-100,    0,    5,    6,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    4,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0, -100]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([fewnerd_dsd[train_split][i] for i in [2, 3]])\n",
    "batch[\"labels\"] # As we can see, the second set of labels has been padded to the length of the first one using -100s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b954012-9bee-4710-85ae-4c81582f5644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/matthias/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size\t1\tworks!\n",
      "Batch size\t2\tworks!\n",
      "Batch size\t4\tworks!\n",
      "Batch size\t8\tworks!\n",
      "Batch size\t16\tworks!\n",
      "Batch size\t32\tworks!\n",
      "Batch size\t64\tworks!\n",
      "Batch size\t128\tworks!\n",
      "Batch size\t256\tworks!\n",
      "max_batchsize_by_trainsplit:\t1\n",
      "max_batchsize_by_model:\t\t256\n",
      "max_batchsize_by_speedup:\t32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(94, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# largest batch size that still leads to 1000 or more steps per training epoch\n",
    "trainsplit_len = len(fewnerd_dsd[train_split])\n",
    "max_batchsize_by_trainsplit = 1\n",
    "for i in range(20):\n",
    "    bs = 2**i\n",
    "    if trainsplit_len/bs >= 1000:\n",
    "        max_batchsize_by_trainsplit = bs\n",
    "# longest input instance\n",
    "max_instance, len_max_instance = get_max_instance(fewnerd_dsd[train_split])\n",
    "# max_batchsize (for current LoRA model)\n",
    "max_batchsize_by_model = get_max_batchsize(adapter_model, max_instance, data_collator)\n",
    "max_batchsize_by_speedup = 32 # diminishing speedup beyond batch_size=32 AND fewer loss minimization steps\n",
    "print(f\"max_batchsize_by_trainsplit:\\t{max_batchsize_by_trainsplit}\")\n",
    "print(f\"max_batchsize_by_model:\\t\\t{max_batchsize_by_model}\")\n",
    "print(f\"max_batchsize_by_speedup:\\t{max_batchsize_by_speedup}\")\n",
    "batch_size = min(max_batchsize_by_trainsplit, max_batchsize_by_model, max_batchsize_by_speedup)\n",
    "logs_dict[\"batch_size\"] = batch_size\n",
    "len_max_instance, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b560aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7310e00ffc10>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7310e00fe590>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    fewnerd_dsd[train_split],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "valid_dataloader = DataLoader(fewnerd_dsd[valid_split], collate_fn=data_collator, batch_size=batch_size)\n",
    "train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1911f061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ner_logs/FacebookAI-roberta-large'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = AdamW(adapter_model.parameters(), lr=5e-4) # 5e-4 works\n",
    "accelerator = Accelerator()\n",
    "acc_model, optimizer, train_dataloader, valid_dataloader = accelerator.prepare(\n",
    "    adapter_model,\n",
    "    optimizer,\n",
    "    train_dataloader,\n",
    "    valid_dataloader\n",
    ")\n",
    "model_folder = re.sub(\"/\", \"-\", base_model_id)\n",
    "output_dir = f\"ner_logs/{model_folder}\"\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "897170de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_steps (all epochs):\t360\n",
      "num_warmup_steps (first epoch):\t18\n"
     ]
    }
   ],
   "source": [
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "logs_dict[\"num_training_steps\"] = num_training_steps\n",
    "num_warmup_steps = min(500, round(0.15 * num_update_steps_per_epoch)) # 500 or 15% of one epoch, whichever is less\n",
    "logs_dict[\"num_warmup_steps\"] = num_warmup_steps\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "print(f\"training_steps (all epochs):\\t{num_training_steps}\\nnum_warmup_steps (first epoch):\\t{num_warmup_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe65a0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1825853247a44472a855dbd0be843540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-10 03:06:01,464] [INFO] [profiler.py:80:start_profile] Flops profiler started\n",
      "\n",
      "training\n",
      "['input_ids shape: [1, 30]', 'attention_mask shape: [1, 30]', 'labels shape: [1, 30]']\n",
      "logits shape: [1, 30, 9], loss: 0.887360692024231\n",
      "\n",
      "validation\n",
      "['input_ids shape: [1, 50]', 'attention_mask shape: [1, 50]', 'labels shape: [1, 50]']\n",
      "logits shape: [1, 50, 9], loss: 0.15421250462532043\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/.local/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: location seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/matthias/.local/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: product seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/matthias/.local/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: organization seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/matthias/.local/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: other seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/matthias/.local/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: person seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/matthias/.local/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: building seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/matthias/.local/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: event seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/matthias/.local/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: art seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/matthias/.local/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'precision': 0.175, 'recall': 0.2413793103448276, 'f1': 0.2028985507246377, 'accuracy': 0.7461629279811098}\n",
      "[2024-03-10 03:06:56,324] [INFO] [profiler.py:80:start_profile] Flops profiler started\n",
      "epoch 1: {'precision': 0.425, 'recall': 0.38636363636363635, 'f1': 0.40476190476190477, 'accuracy': 0.8240850059031877}\n",
      "[2024-03-10 03:07:51,066] [INFO] [profiler.py:80:start_profile] Flops profiler started\n",
      "epoch 2: {'precision': 0.425, 'recall': 0.37777777777777777, 'f1': 0.3999999999999999, 'accuracy': 0.820543093270366}\n",
      "training loop time: 0:02:45\n",
      "[2024-03-10 03:08:46,708] [INFO] [profiler.py:226:end_profile] Flops profiler finished\n",
      "[199381538048, 198746969344, 199153235200]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(597281742592, 199093914197.33334)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions\n",
    "\n",
    "prof = FlopsProfiler(acc_model) # deepspeed profiler\n",
    "flops_list = []\n",
    "training_start = True\n",
    "validation_start = True\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "start_time = time.time() # start time\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    acc_model.train()\n",
    "    prof.start_profile() # start profiling\n",
    "    for batch in train_dataloader:\n",
    "        outputs = acc_model(**batch)\n",
    "        if training_start:\n",
    "            print(\"\\ntraining\")\n",
    "            print([f\"{key} shape: {list(batch[key].shape)}\" for key in list(batch.keys())])\n",
    "            print(f\"logits shape: {list(outputs['logits'].shape)}, loss: {float(outputs['loss'])}\\n\")\n",
    "            training_start = False\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    prof.stop_profile() # stop profiling\n",
    "    total_flops = prof.get_total_flops()\n",
    "    flops_list.append(total_flops)\n",
    "    # Validation\n",
    "    acc_model.eval()\n",
    "    for batch in valid_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = acc_model(**batch)\n",
    "        if validation_start:\n",
    "            print(\"validation\")\n",
    "            print([f\"{key} shape: {list(batch[key].shape)}\" for key in list(batch.keys())])\n",
    "            print(f\"logits shape: {list(outputs['logits'].shape)}, loss: {float(outputs['loss'])}\\n\")\n",
    "            validation_start = False\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "    results = metric.compute()\n",
    "    results_dict = {key: results[f\"overall_{key}\"] for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]}\n",
    "    print(f\"epoch {epoch}:\", results_dict)\n",
    "    logs_dict[f\"epoch_{epoch}_results\"] = results_dict\n",
    "    # save acc_model\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(acc_model)\n",
    "    unwrapped_model.save_pretrained(output_dir)\n",
    "    # save tokenizer\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "#\n",
    "stop_time = time.time()\n",
    "training_loop_time = str(datetime.timedelta(seconds = round(stop_time-start_time)))\n",
    "print(f\"training loop time: {training_loop_time}\")\n",
    "logs_dict[\"training_loop_time\"] = training_loop_time\n",
    "prof.end_profile() # end profiling\n",
    "logs_dict[\"flops_list\"] = flops_list\n",
    "print(flops_list)\n",
    "flops_array = np.array(flops_list)\n",
    "np.sum(flops_array), np.mean(flops_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5269d6e-e4cb-49b5-93e3-0896a0f9ba4d",
   "metadata": {},
   "source": [
    "## Save logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d36a6f5-abd3-4cc5-abf6-311ed3250556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r': 64,\n",
       " 'target_modules': ['query',\n",
       "  'key',\n",
       "  'value',\n",
       "  'query_proj',\n",
       "  'key_proj',\n",
       "  'value_proj'],\n",
       " 'bias': 'lora_only',\n",
       " 'use_rslora': True,\n",
       " 'task_type': <TaskType.TOKEN_CLS: 'TOKEN_CLS'>,\n",
       " 'lora_dropout': 0.2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dev, epoch 0:\n",
    "# epoch 0: {'precision': 0.175, 'recall': 0.2413793103448276, 'f1': 0.2028985507246377, 'accuracy': 0.7461629279811098}\n",
    "LoRA_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d82dce9-d795-4212-ae87-a4e4e2d0075f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logsdict__split=dev__r=64__bias=lora_only__loradroput=0point2'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"logsdict\"\n",
    "filename += f\"__split={split}\"\n",
    "filename += f\"__r={LoRA_params_dict['r']}\"\n",
    "filename += f\"__bias={LoRA_params_dict['bias']}\"\n",
    "filename += f\"__loradroput=0point{str(LoRA_params_dict['lora_dropout'])[2:]}\"\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f58157b-a936-4221-9271-cf8a37eedd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_model_id': 'FacebookAI/roberta-large',\n",
       " 'LoRA_params_dict': {'r': 64,\n",
       "  'target_modules': ['query',\n",
       "   'key',\n",
       "   'value',\n",
       "   'query_proj',\n",
       "   'key_proj',\n",
       "   'value_proj'],\n",
       "  'bias': 'lora_only',\n",
       "  'use_rslora': True,\n",
       "  'task_type': <TaskType.TOKEN_CLS: 'TOKEN_CLS'>,\n",
       "  'lora_dropout': 0.2},\n",
       " 'LoraConfig': \"LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.TOKEN_CLS: 'TOKEN_CLS'>, inference_mode=False, r=64, target_modules={'query', 'value_proj', 'value', 'key', 'key_proj', 'query_proj'}, lora_alpha=8, lora_dropout=0.2, fan_in_fan_out=False, bias='lora_only', use_rslora=True, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={})\",\n",
       " 'LoRA_model_trainable_params': 9446409,\n",
       " 'LoRA_model_all_params': 363765778,\n",
       " 'LoRA_model_trainable_fraction': 0.02597,\n",
       " 'tokenizer': 'FacebookAI/roberta-large',\n",
       " 'batch_size': 1,\n",
       " 'num_training_steps': 360,\n",
       " 'num_warmup_steps': 18,\n",
       " 'epoch_0_results': {'precision': 0.175,\n",
       "  'recall': 0.2413793103448276,\n",
       "  'f1': 0.2028985507246377,\n",
       "  'accuracy': 0.7461629279811098},\n",
       " 'epoch_1_results': {'precision': 0.425,\n",
       "  'recall': 0.38636363636363635,\n",
       "  'f1': 0.40476190476190477,\n",
       "  'accuracy': 0.8240850059031877},\n",
       " 'epoch_2_results': {'precision': 0.425,\n",
       "  'recall': 0.37777777777777777,\n",
       "  'f1': 0.3999999999999999,\n",
       "  'accuracy': 0.820543093270366},\n",
       " 'training_loop_time': '0:02:45',\n",
       " 'flops_list': [199381538048, 198746969344, 199153235200]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"{output_dir}/{filename}.json\", \"w\") as outfile: \n",
    "\tjson.dump(logs_dict, outfile, indent=2)\n",
    "logs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afba571-3efb-4191-b61d-ea2690e61e17",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50068c2d-f2b5-44f9-9e7e-a258292e4332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/.local/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:71: UserWarning: Merge lora module to 8-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(inference_model):\n",
      "<class 'transformers.models.roberta.modeling_roberta.RobertaForTokenClassification'>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 11272, 22398,   329,    21,     5, 32357,     9, 12440, 28855,\n",
       "          5689, 10915, 38042,     6, 47073,     9,     5,  3395, 10915, 38042,\n",
       "           935, 31404,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load inference model\n",
    "config = PeftConfig.from_pretrained(output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "inference_model = PeftModel.from_pretrained(base_model, output_dir).merge_and_unload()\n",
    "print(f\"type(inference_model):\\n{type(inference_model)}\\n\")\n",
    "# get inputs from text (source: https://en.wikipedia.org/wiki/Konstanz#History)\n",
    "text = \"Konstanz was the birthplace of Count Ferdinand von Zeppelin, constructor of the famous Zeppelin airships.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f987d0b-c576-4487-ab74-03c86d9ddf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token (prediction)\n",
      "\n",
      "<s> (O)\n",
      "ĠKon (location)\n",
      "stan (location)\n",
      "z (location)\n",
      "Ġwas (O)\n",
      "Ġthe (O)\n",
      "Ġbirthplace (O)\n",
      "Ġof (O)\n",
      "ĠCount (person)\n",
      "ĠFerdinand (person)\n",
      "Ġvon (person)\n",
      "ĠZe (product)\n",
      "ppelin (product)\n",
      ", (O)\n",
      "Ġconstructor (O)\n",
      "Ġof (O)\n",
      "Ġthe (O)\n",
      "Ġfamous (O)\n",
      "ĠZe (product)\n",
      "ppelin (product)\n",
      "Ġair (product)\n",
      "ships (product)\n",
      ". (O)\n",
      "</s> (O)\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "with torch.no_grad():\n",
    "    logits = inference_model(**inputs).logits\n",
    "tokens = inputs.tokens()\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "print(f\"token (prediction)\\n\")\n",
    "for token, prediction in zip(tokens, predictions[0].numpy()):\n",
    "    print(f\"{token} ({id2label[str(prediction)]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f39b0ad-1be7-4b81-9d3e-933dab612028",
   "metadata": {},
   "source": [
    "$\\checkmark$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
